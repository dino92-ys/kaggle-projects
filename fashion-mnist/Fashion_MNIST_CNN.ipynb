{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae3d0d4",
   "metadata": {},
   "source": [
    " <!-- test -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f70b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d208934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "채널 추가 후 훈련 데이터 형태:  (60000, 1, 28, 28)\n",
      "채널 추가 후 테스트 데이터 형태:  (10000, 1, 28, 28)\n",
      "Using mps device\n",
      "FashionMNISTCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/lmw_kzfn5hq4h_p45q6m9_6r0000gn/T/ipykernel_91737/2242808179.py:189: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1741738245208/work/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  train_labels_tensor = torch.from_numpy(train_labels).long()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "Epoch [1/20], Batch [10/938], Loss: 1.8738\n",
      "Epoch [1/20], Batch [20/938], Loss: 1.1287\n",
      "Epoch [1/20], Batch [30/938], Loss: 0.8862\n",
      "Epoch [1/20], Batch [40/938], Loss: 0.6679\n",
      "Epoch [1/20], Batch [50/938], Loss: 0.8354\n",
      "Epoch [1/20], Batch [60/938], Loss: 0.5579\n",
      "Epoch [1/20], Batch [70/938], Loss: 0.5671\n",
      "Epoch [1/20], Batch [80/938], Loss: 0.6265\n",
      "Epoch [1/20], Batch [90/938], Loss: 0.7799\n",
      "Epoch [1/20], Batch [100/938], Loss: 0.5203\n",
      "Epoch [1/20], Batch [110/938], Loss: 0.5353\n",
      "Epoch [1/20], Batch [120/938], Loss: 0.6046\n",
      "Epoch [1/20], Batch [130/938], Loss: 0.7132\n",
      "Epoch [1/20], Batch [140/938], Loss: 0.5656\n",
      "Epoch [1/20], Batch [150/938], Loss: 0.3697\n",
      "Epoch [1/20], Batch [160/938], Loss: 0.4565\n",
      "Epoch [1/20], Batch [170/938], Loss: 0.4197\n",
      "Epoch [1/20], Batch [180/938], Loss: 0.6325\n",
      "Epoch [1/20], Batch [190/938], Loss: 0.5894\n",
      "Epoch [1/20], Batch [200/938], Loss: 0.6254\n",
      "Epoch [1/20], Batch [210/938], Loss: 0.6358\n",
      "Epoch [1/20], Batch [220/938], Loss: 0.4768\n",
      "Epoch [1/20], Batch [230/938], Loss: 0.3698\n",
      "Epoch [1/20], Batch [240/938], Loss: 0.5472\n",
      "Epoch [1/20], Batch [250/938], Loss: 0.6907\n",
      "Epoch [1/20], Batch [260/938], Loss: 0.5229\n",
      "Epoch [1/20], Batch [270/938], Loss: 0.3501\n",
      "Epoch [1/20], Batch [280/938], Loss: 0.5888\n",
      "Epoch [1/20], Batch [290/938], Loss: 0.5355\n",
      "Epoch [1/20], Batch [300/938], Loss: 0.4709\n",
      "Epoch [1/20], Batch [310/938], Loss: 0.3366\n",
      "Epoch [1/20], Batch [320/938], Loss: 0.6072\n",
      "Epoch [1/20], Batch [330/938], Loss: 0.4684\n",
      "Epoch [1/20], Batch [340/938], Loss: 0.3952\n",
      "Epoch [1/20], Batch [350/938], Loss: 0.3425\n",
      "Epoch [1/20], Batch [360/938], Loss: 0.7188\n",
      "Epoch [1/20], Batch [370/938], Loss: 0.4497\n",
      "Epoch [1/20], Batch [380/938], Loss: 0.4506\n",
      "Epoch [1/20], Batch [390/938], Loss: 0.3967\n",
      "Epoch [1/20], Batch [400/938], Loss: 0.6122\n",
      "Epoch [1/20], Batch [410/938], Loss: 0.2854\n",
      "Epoch [1/20], Batch [420/938], Loss: 0.4637\n",
      "Epoch [1/20], Batch [430/938], Loss: 0.3400\n",
      "Epoch [1/20], Batch [440/938], Loss: 0.3988\n",
      "Epoch [1/20], Batch [450/938], Loss: 0.4909\n",
      "Epoch [1/20], Batch [460/938], Loss: 0.3816\n",
      "Epoch [1/20], Batch [470/938], Loss: 0.2463\n",
      "Epoch [1/20], Batch [480/938], Loss: 0.3057\n",
      "Epoch [1/20], Batch [490/938], Loss: 0.4913\n",
      "Epoch [1/20], Batch [500/938], Loss: 0.4719\n",
      "Epoch [1/20], Batch [510/938], Loss: 0.3188\n",
      "Epoch [1/20], Batch [520/938], Loss: 0.5099\n",
      "Epoch [1/20], Batch [530/938], Loss: 0.1608\n",
      "Epoch [1/20], Batch [540/938], Loss: 0.3415\n",
      "Epoch [1/20], Batch [550/938], Loss: 0.4292\n",
      "Epoch [1/20], Batch [560/938], Loss: 0.1730\n",
      "Epoch [1/20], Batch [570/938], Loss: 0.5037\n",
      "Epoch [1/20], Batch [580/938], Loss: 0.3761\n",
      "Epoch [1/20], Batch [590/938], Loss: 0.3156\n",
      "Epoch [1/20], Batch [600/938], Loss: 0.4551\n",
      "Epoch [1/20], Batch [610/938], Loss: 0.3702\n",
      "Epoch [1/20], Batch [620/938], Loss: 0.2939\n",
      "Epoch [1/20], Batch [630/938], Loss: 0.4888\n",
      "Epoch [1/20], Batch [640/938], Loss: 0.3977\n",
      "Epoch [1/20], Batch [650/938], Loss: 0.3142\n",
      "Epoch [1/20], Batch [660/938], Loss: 0.5169\n",
      "Epoch [1/20], Batch [670/938], Loss: 0.3774\n",
      "Epoch [1/20], Batch [680/938], Loss: 0.2851\n",
      "Epoch [1/20], Batch [690/938], Loss: 0.2874\n",
      "Epoch [1/20], Batch [700/938], Loss: 0.3801\n",
      "Epoch [1/20], Batch [710/938], Loss: 0.3462\n",
      "Epoch [1/20], Batch [720/938], Loss: 0.3743\n",
      "Epoch [1/20], Batch [730/938], Loss: 0.4355\n",
      "Epoch [1/20], Batch [740/938], Loss: 0.3977\n",
      "Epoch [1/20], Batch [750/938], Loss: 0.2126\n",
      "Epoch [1/20], Batch [760/938], Loss: 0.4356\n",
      "Epoch [1/20], Batch [770/938], Loss: 0.2664\n",
      "Epoch [1/20], Batch [780/938], Loss: 0.2923\n",
      "Epoch [1/20], Batch [790/938], Loss: 0.3051\n",
      "Epoch [1/20], Batch [800/938], Loss: 0.2688\n",
      "Epoch [1/20], Batch [810/938], Loss: 0.4100\n",
      "Epoch [1/20], Batch [820/938], Loss: 0.3617\n",
      "Epoch [1/20], Batch [830/938], Loss: 0.2383\n",
      "Epoch [1/20], Batch [840/938], Loss: 0.4394\n",
      "Epoch [1/20], Batch [850/938], Loss: 0.2710\n",
      "Epoch [1/20], Batch [860/938], Loss: 0.4123\n",
      "Epoch [1/20], Batch [870/938], Loss: 0.3202\n",
      "Epoch [1/20], Batch [880/938], Loss: 0.3557\n",
      "Epoch [1/20], Batch [890/938], Loss: 0.4447\n",
      "Epoch [1/20], Batch [900/938], Loss: 0.4026\n",
      "Epoch [1/20], Batch [910/938], Loss: 0.5511\n",
      "Epoch [1/20], Batch [920/938], Loss: 0.4546\n",
      "Epoch [1/20], Batch [930/938], Loss: 0.3463\n",
      "Epoch [1/20], Average Loss: 0.4710\n",
      "------------------------------------------------------------\n",
      "Epoch [2/20], Batch [10/938], Loss: 0.3285\n",
      "Epoch [2/20], Batch [20/938], Loss: 0.3935\n",
      "Epoch [2/20], Batch [30/938], Loss: 0.4778\n",
      "Epoch [2/20], Batch [40/938], Loss: 0.3297\n",
      "Epoch [2/20], Batch [50/938], Loss: 0.4889\n",
      "Epoch [2/20], Batch [60/938], Loss: 0.2183\n",
      "Epoch [2/20], Batch [70/938], Loss: 0.4279\n",
      "Epoch [2/20], Batch [80/938], Loss: 0.3287\n",
      "Epoch [2/20], Batch [90/938], Loss: 0.4092\n",
      "Epoch [2/20], Batch [100/938], Loss: 0.4095\n",
      "Epoch [2/20], Batch [110/938], Loss: 0.2854\n",
      "Epoch [2/20], Batch [120/938], Loss: 0.3601\n",
      "Epoch [2/20], Batch [130/938], Loss: 0.2889\n",
      "Epoch [2/20], Batch [140/938], Loss: 0.3797\n",
      "Epoch [2/20], Batch [150/938], Loss: 0.2542\n",
      "Epoch [2/20], Batch [160/938], Loss: 0.5470\n",
      "Epoch [2/20], Batch [170/938], Loss: 0.1927\n",
      "Epoch [2/20], Batch [180/938], Loss: 0.1276\n",
      "Epoch [2/20], Batch [190/938], Loss: 0.2747\n",
      "Epoch [2/20], Batch [200/938], Loss: 0.6188\n",
      "Epoch [2/20], Batch [210/938], Loss: 0.2476\n",
      "Epoch [2/20], Batch [220/938], Loss: 0.3801\n",
      "Epoch [2/20], Batch [230/938], Loss: 0.3643\n",
      "Epoch [2/20], Batch [240/938], Loss: 0.3267\n",
      "Epoch [2/20], Batch [250/938], Loss: 0.2801\n",
      "Epoch [2/20], Batch [260/938], Loss: 0.3661\n",
      "Epoch [2/20], Batch [270/938], Loss: 0.2867\n",
      "Epoch [2/20], Batch [280/938], Loss: 0.3660\n",
      "Epoch [2/20], Batch [290/938], Loss: 0.3708\n",
      "Epoch [2/20], Batch [300/938], Loss: 0.4588\n",
      "Epoch [2/20], Batch [310/938], Loss: 0.2890\n",
      "Epoch [2/20], Batch [320/938], Loss: 0.2025\n",
      "Epoch [2/20], Batch [330/938], Loss: 0.2054\n",
      "Epoch [2/20], Batch [340/938], Loss: 0.3338\n",
      "Epoch [2/20], Batch [350/938], Loss: 0.1933\n",
      "Epoch [2/20], Batch [360/938], Loss: 0.4132\n",
      "Epoch [2/20], Batch [370/938], Loss: 0.2099\n",
      "Epoch [2/20], Batch [380/938], Loss: 0.4161\n",
      "Epoch [2/20], Batch [390/938], Loss: 0.2861\n",
      "Epoch [2/20], Batch [400/938], Loss: 0.2735\n",
      "Epoch [2/20], Batch [410/938], Loss: 0.2270\n",
      "Epoch [2/20], Batch [420/938], Loss: 0.2082\n",
      "Epoch [2/20], Batch [430/938], Loss: 0.3477\n",
      "Epoch [2/20], Batch [440/938], Loss: 0.4296\n",
      "Epoch [2/20], Batch [450/938], Loss: 0.3187\n",
      "Epoch [2/20], Batch [460/938], Loss: 0.2865\n",
      "Epoch [2/20], Batch [470/938], Loss: 0.1362\n",
      "Epoch [2/20], Batch [480/938], Loss: 0.2679\n",
      "Epoch [2/20], Batch [490/938], Loss: 0.3323\n",
      "Epoch [2/20], Batch [500/938], Loss: 0.3077\n",
      "Epoch [2/20], Batch [510/938], Loss: 0.4141\n",
      "Epoch [2/20], Batch [520/938], Loss: 0.2831\n",
      "Epoch [2/20], Batch [530/938], Loss: 0.4534\n",
      "Epoch [2/20], Batch [540/938], Loss: 0.3055\n",
      "Epoch [2/20], Batch [550/938], Loss: 0.2982\n",
      "Epoch [2/20], Batch [560/938], Loss: 0.1714\n",
      "Epoch [2/20], Batch [570/938], Loss: 0.3092\n",
      "Epoch [2/20], Batch [580/938], Loss: 0.3822\n",
      "Epoch [2/20], Batch [590/938], Loss: 0.3671\n",
      "Epoch [2/20], Batch [600/938], Loss: 0.1855\n",
      "Epoch [2/20], Batch [610/938], Loss: 0.2551\n",
      "Epoch [2/20], Batch [620/938], Loss: 0.2249\n",
      "Epoch [2/20], Batch [630/938], Loss: 0.2598\n",
      "Epoch [2/20], Batch [640/938], Loss: 0.2178\n",
      "Epoch [2/20], Batch [650/938], Loss: 0.2060\n",
      "Epoch [2/20], Batch [660/938], Loss: 0.3972\n",
      "Epoch [2/20], Batch [670/938], Loss: 0.2484\n",
      "Epoch [2/20], Batch [680/938], Loss: 0.1966\n",
      "Epoch [2/20], Batch [690/938], Loss: 0.2428\n",
      "Epoch [2/20], Batch [700/938], Loss: 0.2831\n",
      "Epoch [2/20], Batch [710/938], Loss: 0.2732\n",
      "Epoch [2/20], Batch [720/938], Loss: 0.2815\n",
      "Epoch [2/20], Batch [730/938], Loss: 0.2178\n",
      "Epoch [2/20], Batch [740/938], Loss: 0.2705\n",
      "Epoch [2/20], Batch [750/938], Loss: 0.4524\n",
      "Epoch [2/20], Batch [760/938], Loss: 0.1888\n",
      "Epoch [2/20], Batch [770/938], Loss: 0.3569\n",
      "Epoch [2/20], Batch [780/938], Loss: 0.4648\n",
      "Epoch [2/20], Batch [790/938], Loss: 0.4111\n",
      "Epoch [2/20], Batch [800/938], Loss: 0.1620\n",
      "Epoch [2/20], Batch [810/938], Loss: 0.3307\n",
      "Epoch [2/20], Batch [820/938], Loss: 0.3284\n",
      "Epoch [2/20], Batch [830/938], Loss: 0.1385\n",
      "Epoch [2/20], Batch [840/938], Loss: 0.3167\n",
      "Epoch [2/20], Batch [850/938], Loss: 0.5063\n",
      "Epoch [2/20], Batch [860/938], Loss: 0.1755\n",
      "Epoch [2/20], Batch [870/938], Loss: 0.3004\n",
      "Epoch [2/20], Batch [880/938], Loss: 0.2905\n",
      "Epoch [2/20], Batch [890/938], Loss: 0.3464\n",
      "Epoch [2/20], Batch [900/938], Loss: 0.3358\n",
      "Epoch [2/20], Batch [910/938], Loss: 0.1396\n",
      "Epoch [2/20], Batch [920/938], Loss: 0.2576\n",
      "Epoch [2/20], Batch [930/938], Loss: 0.2006\n",
      "Epoch [2/20], Average Loss: 0.3058\n",
      "------------------------------------------------------------\n",
      "Epoch [3/20], Batch [10/938], Loss: 0.1728\n",
      "Epoch [3/20], Batch [20/938], Loss: 0.2250\n",
      "Epoch [3/20], Batch [30/938], Loss: 0.1918\n",
      "Epoch [3/20], Batch [40/938], Loss: 0.2170\n",
      "Epoch [3/20], Batch [50/938], Loss: 0.3549\n",
      "Epoch [3/20], Batch [60/938], Loss: 0.3511\n",
      "Epoch [3/20], Batch [70/938], Loss: 0.2540\n",
      "Epoch [3/20], Batch [80/938], Loss: 0.1850\n",
      "Epoch [3/20], Batch [90/938], Loss: 0.2453\n",
      "Epoch [3/20], Batch [100/938], Loss: 0.1572\n",
      "Epoch [3/20], Batch [110/938], Loss: 0.2457\n",
      "Epoch [3/20], Batch [120/938], Loss: 0.1819\n",
      "Epoch [3/20], Batch [130/938], Loss: 0.1806\n",
      "Epoch [3/20], Batch [140/938], Loss: 0.1614\n",
      "Epoch [3/20], Batch [150/938], Loss: 0.2846\n",
      "Epoch [3/20], Batch [160/938], Loss: 0.2645\n",
      "Epoch [3/20], Batch [170/938], Loss: 0.2570\n",
      "Epoch [3/20], Batch [180/938], Loss: 0.5196\n",
      "Epoch [3/20], Batch [190/938], Loss: 0.3962\n",
      "Epoch [3/20], Batch [200/938], Loss: 0.1524\n",
      "Epoch [3/20], Batch [210/938], Loss: 0.2277\n",
      "Epoch [3/20], Batch [220/938], Loss: 0.2561\n",
      "Epoch [3/20], Batch [230/938], Loss: 0.2256\n",
      "Epoch [3/20], Batch [240/938], Loss: 0.2705\n",
      "Epoch [3/20], Batch [250/938], Loss: 0.4728\n",
      "Epoch [3/20], Batch [260/938], Loss: 0.2412\n",
      "Epoch [3/20], Batch [270/938], Loss: 0.3368\n",
      "Epoch [3/20], Batch [280/938], Loss: 0.2725\n",
      "Epoch [3/20], Batch [290/938], Loss: 0.3531\n",
      "Epoch [3/20], Batch [300/938], Loss: 0.3062\n",
      "Epoch [3/20], Batch [310/938], Loss: 0.3189\n",
      "Epoch [3/20], Batch [320/938], Loss: 0.2816\n",
      "Epoch [3/20], Batch [330/938], Loss: 0.2454\n",
      "Epoch [3/20], Batch [340/938], Loss: 0.1614\n",
      "Epoch [3/20], Batch [350/938], Loss: 0.2781\n",
      "Epoch [3/20], Batch [360/938], Loss: 0.1141\n",
      "Epoch [3/20], Batch [370/938], Loss: 0.1460\n",
      "Epoch [3/20], Batch [380/938], Loss: 0.1912\n",
      "Epoch [3/20], Batch [390/938], Loss: 0.2263\n",
      "Epoch [3/20], Batch [400/938], Loss: 0.3543\n",
      "Epoch [3/20], Batch [410/938], Loss: 0.1738\n",
      "Epoch [3/20], Batch [420/938], Loss: 0.2849\n",
      "Epoch [3/20], Batch [430/938], Loss: 0.3600\n",
      "Epoch [3/20], Batch [440/938], Loss: 0.2213\n",
      "Epoch [3/20], Batch [450/938], Loss: 0.2373\n",
      "Epoch [3/20], Batch [460/938], Loss: 0.2797\n",
      "Epoch [3/20], Batch [470/938], Loss: 0.2373\n",
      "Epoch [3/20], Batch [480/938], Loss: 0.2434\n",
      "Epoch [3/20], Batch [490/938], Loss: 0.2804\n",
      "Epoch [3/20], Batch [500/938], Loss: 0.2676\n",
      "Epoch [3/20], Batch [510/938], Loss: 0.4750\n",
      "Epoch [3/20], Batch [520/938], Loss: 0.2666\n",
      "Epoch [3/20], Batch [530/938], Loss: 0.2838\n",
      "Epoch [3/20], Batch [540/938], Loss: 0.4190\n",
      "Epoch [3/20], Batch [550/938], Loss: 0.2826\n",
      "Epoch [3/20], Batch [560/938], Loss: 0.2928\n",
      "Epoch [3/20], Batch [570/938], Loss: 0.1326\n",
      "Epoch [3/20], Batch [580/938], Loss: 0.2753\n",
      "Epoch [3/20], Batch [590/938], Loss: 0.2539\n",
      "Epoch [3/20], Batch [600/938], Loss: 0.3580\n",
      "Epoch [3/20], Batch [610/938], Loss: 0.2096\n",
      "Epoch [3/20], Batch [620/938], Loss: 0.3027\n",
      "Epoch [3/20], Batch [630/938], Loss: 0.3291\n",
      "Epoch [3/20], Batch [640/938], Loss: 0.3058\n",
      "Epoch [3/20], Batch [650/938], Loss: 0.1557\n",
      "Epoch [3/20], Batch [660/938], Loss: 0.2949\n",
      "Epoch [3/20], Batch [670/938], Loss: 0.3104\n",
      "Epoch [3/20], Batch [680/938], Loss: 0.2694\n",
      "Epoch [3/20], Batch [690/938], Loss: 0.2753\n",
      "Epoch [3/20], Batch [700/938], Loss: 0.2789\n",
      "Epoch [3/20], Batch [710/938], Loss: 0.2662\n",
      "Epoch [3/20], Batch [720/938], Loss: 0.3303\n",
      "Epoch [3/20], Batch [730/938], Loss: 0.2240\n",
      "Epoch [3/20], Batch [740/938], Loss: 0.1600\n",
      "Epoch [3/20], Batch [750/938], Loss: 0.1563\n",
      "Epoch [3/20], Batch [760/938], Loss: 0.2276\n",
      "Epoch [3/20], Batch [770/938], Loss: 0.2972\n",
      "Epoch [3/20], Batch [780/938], Loss: 0.2857\n",
      "Epoch [3/20], Batch [790/938], Loss: 0.2326\n",
      "Epoch [3/20], Batch [800/938], Loss: 0.2126\n",
      "Epoch [3/20], Batch [810/938], Loss: 0.5828\n",
      "Epoch [3/20], Batch [820/938], Loss: 0.1524\n",
      "Epoch [3/20], Batch [830/938], Loss: 0.3281\n",
      "Epoch [3/20], Batch [840/938], Loss: 0.3279\n",
      "Epoch [3/20], Batch [850/938], Loss: 0.1578\n",
      "Epoch [3/20], Batch [860/938], Loss: 0.1485\n",
      "Epoch [3/20], Batch [870/938], Loss: 0.3414\n",
      "Epoch [3/20], Batch [880/938], Loss: 0.1119\n",
      "Epoch [3/20], Batch [890/938], Loss: 0.2883\n",
      "Epoch [3/20], Batch [900/938], Loss: 0.2410\n",
      "Epoch [3/20], Batch [910/938], Loss: 0.2498\n",
      "Epoch [3/20], Batch [920/938], Loss: 0.2229\n",
      "Epoch [3/20], Batch [930/938], Loss: 0.2934\n",
      "Epoch [3/20], Average Loss: 0.2617\n",
      "------------------------------------------------------------\n",
      "Epoch [4/20], Batch [10/938], Loss: 0.2419\n",
      "Epoch [4/20], Batch [20/938], Loss: 0.3470\n",
      "Epoch [4/20], Batch [30/938], Loss: 0.2656\n",
      "Epoch [4/20], Batch [40/938], Loss: 0.2555\n",
      "Epoch [4/20], Batch [50/938], Loss: 0.2902\n",
      "Epoch [4/20], Batch [60/938], Loss: 0.3581\n",
      "Epoch [4/20], Batch [70/938], Loss: 0.2690\n",
      "Epoch [4/20], Batch [80/938], Loss: 0.1547\n",
      "Epoch [4/20], Batch [90/938], Loss: 0.3403\n",
      "Epoch [4/20], Batch [100/938], Loss: 0.2043\n",
      "Epoch [4/20], Batch [110/938], Loss: 0.3237\n",
      "Epoch [4/20], Batch [120/938], Loss: 0.2052\n",
      "Epoch [4/20], Batch [130/938], Loss: 0.3149\n",
      "Epoch [4/20], Batch [140/938], Loss: 0.1478\n",
      "Epoch [4/20], Batch [150/938], Loss: 0.2161\n",
      "Epoch [4/20], Batch [160/938], Loss: 0.0897\n",
      "Epoch [4/20], Batch [170/938], Loss: 0.3838\n",
      "Epoch [4/20], Batch [180/938], Loss: 0.3410\n",
      "Epoch [4/20], Batch [190/938], Loss: 0.2034\n",
      "Epoch [4/20], Batch [200/938], Loss: 0.1800\n",
      "Epoch [4/20], Batch [210/938], Loss: 0.1828\n",
      "Epoch [4/20], Batch [220/938], Loss: 0.3339\n",
      "Epoch [4/20], Batch [230/938], Loss: 0.2524\n",
      "Epoch [4/20], Batch [240/938], Loss: 0.3073\n",
      "Epoch [4/20], Batch [250/938], Loss: 0.2595\n",
      "Epoch [4/20], Batch [260/938], Loss: 0.1134\n",
      "Epoch [4/20], Batch [270/938], Loss: 0.1385\n",
      "Epoch [4/20], Batch [280/938], Loss: 0.2252\n",
      "Epoch [4/20], Batch [290/938], Loss: 0.3418\n",
      "Epoch [4/20], Batch [300/938], Loss: 0.2777\n",
      "Epoch [4/20], Batch [310/938], Loss: 0.3176\n",
      "Epoch [4/20], Batch [320/938], Loss: 0.2217\n",
      "Epoch [4/20], Batch [330/938], Loss: 0.3262\n",
      "Epoch [4/20], Batch [340/938], Loss: 0.1562\n",
      "Epoch [4/20], Batch [350/938], Loss: 0.2468\n",
      "Epoch [4/20], Batch [360/938], Loss: 0.2365\n",
      "Epoch [4/20], Batch [370/938], Loss: 0.3642\n",
      "Epoch [4/20], Batch [380/938], Loss: 0.2228\n",
      "Epoch [4/20], Batch [390/938], Loss: 0.1420\n",
      "Epoch [4/20], Batch [400/938], Loss: 0.3167\n",
      "Epoch [4/20], Batch [410/938], Loss: 0.1844\n",
      "Epoch [4/20], Batch [420/938], Loss: 0.2461\n",
      "Epoch [4/20], Batch [430/938], Loss: 0.2327\n",
      "Epoch [4/20], Batch [440/938], Loss: 0.1500\n",
      "Epoch [4/20], Batch [450/938], Loss: 0.1724\n",
      "Epoch [4/20], Batch [460/938], Loss: 0.2251\n",
      "Epoch [4/20], Batch [470/938], Loss: 0.2147\n",
      "Epoch [4/20], Batch [480/938], Loss: 0.1492\n",
      "Epoch [4/20], Batch [490/938], Loss: 0.3132\n",
      "Epoch [4/20], Batch [500/938], Loss: 0.2094\n",
      "Epoch [4/20], Batch [510/938], Loss: 0.1518\n",
      "Epoch [4/20], Batch [520/938], Loss: 0.2351\n",
      "Epoch [4/20], Batch [530/938], Loss: 0.2196\n",
      "Epoch [4/20], Batch [540/938], Loss: 0.1340\n",
      "Epoch [4/20], Batch [550/938], Loss: 0.2509\n",
      "Epoch [4/20], Batch [560/938], Loss: 0.2987\n",
      "Epoch [4/20], Batch [570/938], Loss: 0.2443\n",
      "Epoch [4/20], Batch [580/938], Loss: 0.4643\n",
      "Epoch [4/20], Batch [590/938], Loss: 0.2272\n",
      "Epoch [4/20], Batch [600/938], Loss: 0.2261\n",
      "Epoch [4/20], Batch [610/938], Loss: 0.2380\n",
      "Epoch [4/20], Batch [620/938], Loss: 0.1583\n",
      "Epoch [4/20], Batch [630/938], Loss: 0.2133\n",
      "Epoch [4/20], Batch [640/938], Loss: 0.1514\n",
      "Epoch [4/20], Batch [650/938], Loss: 0.1707\n",
      "Epoch [4/20], Batch [660/938], Loss: 0.1088\n",
      "Epoch [4/20], Batch [670/938], Loss: 0.1558\n",
      "Epoch [4/20], Batch [680/938], Loss: 0.1665\n",
      "Epoch [4/20], Batch [690/938], Loss: 0.2767\n",
      "Epoch [4/20], Batch [700/938], Loss: 0.2006\n",
      "Epoch [4/20], Batch [710/938], Loss: 0.1705\n",
      "Epoch [4/20], Batch [720/938], Loss: 0.1377\n",
      "Epoch [4/20], Batch [730/938], Loss: 0.3043\n",
      "Epoch [4/20], Batch [740/938], Loss: 0.1609\n",
      "Epoch [4/20], Batch [750/938], Loss: 0.2669\n",
      "Epoch [4/20], Batch [760/938], Loss: 0.1188\n",
      "Epoch [4/20], Batch [770/938], Loss: 0.2409\n",
      "Epoch [4/20], Batch [780/938], Loss: 0.1512\n",
      "Epoch [4/20], Batch [790/938], Loss: 0.2172\n",
      "Epoch [4/20], Batch [800/938], Loss: 0.0991\n",
      "Epoch [4/20], Batch [810/938], Loss: 0.2982\n",
      "Epoch [4/20], Batch [820/938], Loss: 0.1386\n",
      "Epoch [4/20], Batch [830/938], Loss: 0.1660\n",
      "Epoch [4/20], Batch [840/938], Loss: 0.2402\n",
      "Epoch [4/20], Batch [850/938], Loss: 0.1751\n",
      "Epoch [4/20], Batch [860/938], Loss: 0.2297\n",
      "Epoch [4/20], Batch [870/938], Loss: 0.1715\n",
      "Epoch [4/20], Batch [880/938], Loss: 0.2980\n",
      "Epoch [4/20], Batch [890/938], Loss: 0.3464\n",
      "Epoch [4/20], Batch [900/938], Loss: 0.1137\n",
      "Epoch [4/20], Batch [910/938], Loss: 0.3287\n",
      "Epoch [4/20], Batch [920/938], Loss: 0.2501\n",
      "Epoch [4/20], Batch [930/938], Loss: 0.2172\n",
      "Epoch [4/20], Average Loss: 0.2325\n",
      "------------------------------------------------------------\n",
      "Epoch [5/20], Batch [10/938], Loss: 0.1335\n",
      "Epoch [5/20], Batch [20/938], Loss: 0.0761\n",
      "Epoch [5/20], Batch [30/938], Loss: 0.0961\n",
      "Epoch [5/20], Batch [40/938], Loss: 0.1468\n",
      "Epoch [5/20], Batch [50/938], Loss: 0.2942\n",
      "Epoch [5/20], Batch [60/938], Loss: 0.1930\n",
      "Epoch [5/20], Batch [70/938], Loss: 0.2167\n",
      "Epoch [5/20], Batch [80/938], Loss: 0.3544\n",
      "Epoch [5/20], Batch [90/938], Loss: 0.3049\n",
      "Epoch [5/20], Batch [100/938], Loss: 0.1980\n",
      "Epoch [5/20], Batch [110/938], Loss: 0.1840\n",
      "Epoch [5/20], Batch [120/938], Loss: 0.0861\n",
      "Epoch [5/20], Batch [130/938], Loss: 0.2249\n",
      "Epoch [5/20], Batch [140/938], Loss: 0.2015\n",
      "Epoch [5/20], Batch [150/938], Loss: 0.1597\n",
      "Epoch [5/20], Batch [160/938], Loss: 0.3164\n",
      "Epoch [5/20], Batch [170/938], Loss: 0.1554\n",
      "Epoch [5/20], Batch [180/938], Loss: 0.2269\n",
      "Epoch [5/20], Batch [190/938], Loss: 0.2719\n",
      "Epoch [5/20], Batch [200/938], Loss: 0.2049\n",
      "Epoch [5/20], Batch [210/938], Loss: 0.2268\n",
      "Epoch [5/20], Batch [220/938], Loss: 0.1880\n",
      "Epoch [5/20], Batch [230/938], Loss: 0.1654\n",
      "Epoch [5/20], Batch [240/938], Loss: 0.2333\n",
      "Epoch [5/20], Batch [250/938], Loss: 0.1856\n",
      "Epoch [5/20], Batch [260/938], Loss: 0.2253\n",
      "Epoch [5/20], Batch [270/938], Loss: 0.2726\n",
      "Epoch [5/20], Batch [280/938], Loss: 0.1970\n",
      "Epoch [5/20], Batch [290/938], Loss: 0.1309\n",
      "Epoch [5/20], Batch [300/938], Loss: 0.1534\n",
      "Epoch [5/20], Batch [310/938], Loss: 0.2968\n",
      "Epoch [5/20], Batch [320/938], Loss: 0.2080\n",
      "Epoch [5/20], Batch [330/938], Loss: 0.1594\n",
      "Epoch [5/20], Batch [340/938], Loss: 0.1771\n",
      "Epoch [5/20], Batch [350/938], Loss: 0.2656\n",
      "Epoch [5/20], Batch [360/938], Loss: 0.3363\n",
      "Epoch [5/20], Batch [370/938], Loss: 0.1769\n",
      "Epoch [5/20], Batch [380/938], Loss: 0.2073\n",
      "Epoch [5/20], Batch [390/938], Loss: 0.0829\n",
      "Epoch [5/20], Batch [400/938], Loss: 0.1817\n",
      "Epoch [5/20], Batch [410/938], Loss: 0.1708\n",
      "Epoch [5/20], Batch [420/938], Loss: 0.1441\n",
      "Epoch [5/20], Batch [430/938], Loss: 0.1217\n",
      "Epoch [5/20], Batch [440/938], Loss: 0.3215\n",
      "Epoch [5/20], Batch [450/938], Loss: 0.1755\n",
      "Epoch [5/20], Batch [460/938], Loss: 0.1673\n",
      "Epoch [5/20], Batch [470/938], Loss: 0.2452\n",
      "Epoch [5/20], Batch [480/938], Loss: 0.1056\n",
      "Epoch [5/20], Batch [490/938], Loss: 0.2080\n",
      "Epoch [5/20], Batch [500/938], Loss: 0.1743\n",
      "Epoch [5/20], Batch [510/938], Loss: 0.1547\n",
      "Epoch [5/20], Batch [520/938], Loss: 0.1485\n",
      "Epoch [5/20], Batch [530/938], Loss: 0.1595\n",
      "Epoch [5/20], Batch [540/938], Loss: 0.1814\n",
      "Epoch [5/20], Batch [550/938], Loss: 0.3035\n",
      "Epoch [5/20], Batch [560/938], Loss: 0.1181\n",
      "Epoch [5/20], Batch [570/938], Loss: 0.1848\n",
      "Epoch [5/20], Batch [580/938], Loss: 0.2582\n",
      "Epoch [5/20], Batch [590/938], Loss: 0.2083\n",
      "Epoch [5/20], Batch [600/938], Loss: 0.2980\n",
      "Epoch [5/20], Batch [610/938], Loss: 0.1903\n",
      "Epoch [5/20], Batch [620/938], Loss: 0.1701\n",
      "Epoch [5/20], Batch [630/938], Loss: 0.3440\n",
      "Epoch [5/20], Batch [640/938], Loss: 0.1745\n",
      "Epoch [5/20], Batch [650/938], Loss: 0.1298\n",
      "Epoch [5/20], Batch [660/938], Loss: 0.2491\n",
      "Epoch [5/20], Batch [670/938], Loss: 0.1199\n",
      "Epoch [5/20], Batch [680/938], Loss: 0.1979\n",
      "Epoch [5/20], Batch [690/938], Loss: 0.0914\n",
      "Epoch [5/20], Batch [700/938], Loss: 0.2410\n",
      "Epoch [5/20], Batch [710/938], Loss: 0.1739\n",
      "Epoch [5/20], Batch [720/938], Loss: 0.4546\n",
      "Epoch [5/20], Batch [730/938], Loss: 0.2018\n",
      "Epoch [5/20], Batch [740/938], Loss: 0.1114\n",
      "Epoch [5/20], Batch [750/938], Loss: 0.3382\n",
      "Epoch [5/20], Batch [760/938], Loss: 0.2034\n",
      "Epoch [5/20], Batch [770/938], Loss: 0.2013\n",
      "Epoch [5/20], Batch [780/938], Loss: 0.1424\n",
      "Epoch [5/20], Batch [790/938], Loss: 0.3102\n",
      "Epoch [5/20], Batch [800/938], Loss: 0.1936\n",
      "Epoch [5/20], Batch [810/938], Loss: 0.2123\n",
      "Epoch [5/20], Batch [820/938], Loss: 0.2620\n",
      "Epoch [5/20], Batch [830/938], Loss: 0.3441\n",
      "Epoch [5/20], Batch [840/938], Loss: 0.2403\n",
      "Epoch [5/20], Batch [850/938], Loss: 0.1824\n",
      "Epoch [5/20], Batch [860/938], Loss: 0.1003\n",
      "Epoch [5/20], Batch [870/938], Loss: 0.1944\n",
      "Epoch [5/20], Batch [880/938], Loss: 0.1196\n",
      "Epoch [5/20], Batch [890/938], Loss: 0.5859\n",
      "Epoch [5/20], Batch [900/938], Loss: 0.1658\n",
      "Epoch [5/20], Batch [910/938], Loss: 0.1034\n",
      "Epoch [5/20], Batch [920/938], Loss: 0.1174\n",
      "Epoch [5/20], Batch [930/938], Loss: 0.1227\n",
      "Epoch [5/20], Average Loss: 0.2065\n",
      "------------------------------------------------------------\n",
      "Epoch [6/20], Batch [10/938], Loss: 0.1226\n",
      "Epoch [6/20], Batch [20/938], Loss: 0.2096\n",
      "Epoch [6/20], Batch [30/938], Loss: 0.2388\n",
      "Epoch [6/20], Batch [40/938], Loss: 0.2267\n",
      "Epoch [6/20], Batch [50/938], Loss: 0.2231\n",
      "Epoch [6/20], Batch [60/938], Loss: 0.1059\n",
      "Epoch [6/20], Batch [70/938], Loss: 0.1557\n",
      "Epoch [6/20], Batch [80/938], Loss: 0.3669\n",
      "Epoch [6/20], Batch [90/938], Loss: 0.0793\n",
      "Epoch [6/20], Batch [100/938], Loss: 0.1884\n",
      "Epoch [6/20], Batch [110/938], Loss: 0.2379\n",
      "Epoch [6/20], Batch [120/938], Loss: 0.2489\n",
      "Epoch [6/20], Batch [130/938], Loss: 0.1743\n",
      "Epoch [6/20], Batch [140/938], Loss: 0.1121\n",
      "Epoch [6/20], Batch [150/938], Loss: 0.1358\n",
      "Epoch [6/20], Batch [160/938], Loss: 0.1726\n",
      "Epoch [6/20], Batch [170/938], Loss: 0.1597\n",
      "Epoch [6/20], Batch [180/938], Loss: 0.1324\n",
      "Epoch [6/20], Batch [190/938], Loss: 0.2907\n",
      "Epoch [6/20], Batch [200/938], Loss: 0.0599\n",
      "Epoch [6/20], Batch [210/938], Loss: 0.1742\n",
      "Epoch [6/20], Batch [220/938], Loss: 0.0881\n",
      "Epoch [6/20], Batch [230/938], Loss: 0.1166\n",
      "Epoch [6/20], Batch [240/938], Loss: 0.1402\n",
      "Epoch [6/20], Batch [250/938], Loss: 0.2590\n",
      "Epoch [6/20], Batch [260/938], Loss: 0.1767\n",
      "Epoch [6/20], Batch [270/938], Loss: 0.0922\n",
      "Epoch [6/20], Batch [280/938], Loss: 0.2269\n",
      "Epoch [6/20], Batch [290/938], Loss: 0.1098\n",
      "Epoch [6/20], Batch [300/938], Loss: 0.1818\n",
      "Epoch [6/20], Batch [310/938], Loss: 0.0822\n",
      "Epoch [6/20], Batch [320/938], Loss: 0.1753\n",
      "Epoch [6/20], Batch [330/938], Loss: 0.1197\n",
      "Epoch [6/20], Batch [340/938], Loss: 0.1053\n",
      "Epoch [6/20], Batch [350/938], Loss: 0.2907\n",
      "Epoch [6/20], Batch [360/938], Loss: 0.1155\n",
      "Epoch [6/20], Batch [370/938], Loss: 0.2809\n",
      "Epoch [6/20], Batch [380/938], Loss: 0.1477\n",
      "Epoch [6/20], Batch [390/938], Loss: 0.0981\n",
      "Epoch [6/20], Batch [400/938], Loss: 0.1448\n",
      "Epoch [6/20], Batch [410/938], Loss: 0.2233\n",
      "Epoch [6/20], Batch [420/938], Loss: 0.2841\n",
      "Epoch [6/20], Batch [430/938], Loss: 0.1155\n",
      "Epoch [6/20], Batch [440/938], Loss: 0.2407\n",
      "Epoch [6/20], Batch [450/938], Loss: 0.1313\n",
      "Epoch [6/20], Batch [460/938], Loss: 0.1895\n",
      "Epoch [6/20], Batch [470/938], Loss: 0.1269\n",
      "Epoch [6/20], Batch [480/938], Loss: 0.1421\n",
      "Epoch [6/20], Batch [490/938], Loss: 0.2169\n",
      "Epoch [6/20], Batch [500/938], Loss: 0.3629\n",
      "Epoch [6/20], Batch [510/938], Loss: 0.1870\n",
      "Epoch [6/20], Batch [520/938], Loss: 0.0989\n",
      "Epoch [6/20], Batch [530/938], Loss: 0.2763\n",
      "Epoch [6/20], Batch [540/938], Loss: 0.1709\n",
      "Epoch [6/20], Batch [550/938], Loss: 0.0820\n",
      "Epoch [6/20], Batch [560/938], Loss: 0.2460\n",
      "Epoch [6/20], Batch [570/938], Loss: 0.3335\n",
      "Epoch [6/20], Batch [580/938], Loss: 0.1248\n",
      "Epoch [6/20], Batch [590/938], Loss: 0.1636\n",
      "Epoch [6/20], Batch [600/938], Loss: 0.1728\n",
      "Epoch [6/20], Batch [610/938], Loss: 0.1734\n",
      "Epoch [6/20], Batch [620/938], Loss: 0.1485\n",
      "Epoch [6/20], Batch [630/938], Loss: 0.1781\n",
      "Epoch [6/20], Batch [640/938], Loss: 0.1432\n",
      "Epoch [6/20], Batch [650/938], Loss: 0.2126\n",
      "Epoch [6/20], Batch [660/938], Loss: 0.1987\n",
      "Epoch [6/20], Batch [670/938], Loss: 0.1951\n",
      "Epoch [6/20], Batch [680/938], Loss: 0.2593\n",
      "Epoch [6/20], Batch [690/938], Loss: 0.1405\n",
      "Epoch [6/20], Batch [700/938], Loss: 0.1742\n",
      "Epoch [6/20], Batch [710/938], Loss: 0.1986\n",
      "Epoch [6/20], Batch [720/938], Loss: 0.1664\n",
      "Epoch [6/20], Batch [730/938], Loss: 0.0742\n",
      "Epoch [6/20], Batch [740/938], Loss: 0.1149\n",
      "Epoch [6/20], Batch [750/938], Loss: 0.0904\n",
      "Epoch [6/20], Batch [760/938], Loss: 0.1580\n",
      "Epoch [6/20], Batch [770/938], Loss: 0.3569\n",
      "Epoch [6/20], Batch [780/938], Loss: 0.2568\n",
      "Epoch [6/20], Batch [790/938], Loss: 0.1640\n",
      "Epoch [6/20], Batch [800/938], Loss: 0.0990\n",
      "Epoch [6/20], Batch [810/938], Loss: 0.2283\n",
      "Epoch [6/20], Batch [820/938], Loss: 0.1562\n",
      "Epoch [6/20], Batch [830/938], Loss: 0.2157\n",
      "Epoch [6/20], Batch [840/938], Loss: 0.1702\n",
      "Epoch [6/20], Batch [850/938], Loss: 0.2278\n",
      "Epoch [6/20], Batch [860/938], Loss: 0.1547\n",
      "Epoch [6/20], Batch [870/938], Loss: 0.1460\n",
      "Epoch [6/20], Batch [880/938], Loss: 0.1173\n",
      "Epoch [6/20], Batch [890/938], Loss: 0.1664\n",
      "Epoch [6/20], Batch [900/938], Loss: 0.1967\n",
      "Epoch [6/20], Batch [910/938], Loss: 0.1290\n",
      "Epoch [6/20], Batch [920/938], Loss: 0.1914\n",
      "Epoch [6/20], Batch [930/938], Loss: 0.1843\n",
      "Epoch [6/20], Average Loss: 0.1878\n",
      "------------------------------------------------------------\n",
      "Epoch [7/20], Batch [10/938], Loss: 0.0943\n",
      "Epoch [7/20], Batch [20/938], Loss: 0.0751\n",
      "Epoch [7/20], Batch [30/938], Loss: 0.2230\n",
      "Epoch [7/20], Batch [40/938], Loss: 0.1515\n",
      "Epoch [7/20], Batch [50/938], Loss: 0.2061\n",
      "Epoch [7/20], Batch [60/938], Loss: 0.1616\n",
      "Epoch [7/20], Batch [70/938], Loss: 0.0916\n",
      "Epoch [7/20], Batch [80/938], Loss: 0.1500\n",
      "Epoch [7/20], Batch [90/938], Loss: 0.2355\n",
      "Epoch [7/20], Batch [100/938], Loss: 0.1176\n",
      "Epoch [7/20], Batch [110/938], Loss: 0.2724\n",
      "Epoch [7/20], Batch [120/938], Loss: 0.1692\n",
      "Epoch [7/20], Batch [130/938], Loss: 0.1734\n",
      "Epoch [7/20], Batch [140/938], Loss: 0.1115\n",
      "Epoch [7/20], Batch [150/938], Loss: 0.0884\n",
      "Epoch [7/20], Batch [160/938], Loss: 0.1489\n",
      "Epoch [7/20], Batch [170/938], Loss: 0.1862\n",
      "Epoch [7/20], Batch [180/938], Loss: 0.0393\n",
      "Epoch [7/20], Batch [190/938], Loss: 0.1640\n",
      "Epoch [7/20], Batch [200/938], Loss: 0.2804\n",
      "Epoch [7/20], Batch [210/938], Loss: 0.2871\n",
      "Epoch [7/20], Batch [220/938], Loss: 0.0984\n",
      "Epoch [7/20], Batch [230/938], Loss: 0.2151\n",
      "Epoch [7/20], Batch [240/938], Loss: 0.1479\n",
      "Epoch [7/20], Batch [250/938], Loss: 0.1366\n",
      "Epoch [7/20], Batch [260/938], Loss: 0.1491\n",
      "Epoch [7/20], Batch [270/938], Loss: 0.1171\n",
      "Epoch [7/20], Batch [280/938], Loss: 0.2282\n",
      "Epoch [7/20], Batch [290/938], Loss: 0.2625\n",
      "Epoch [7/20], Batch [300/938], Loss: 0.0674\n",
      "Epoch [7/20], Batch [310/938], Loss: 0.1461\n",
      "Epoch [7/20], Batch [320/938], Loss: 0.1623\n",
      "Epoch [7/20], Batch [330/938], Loss: 0.1198\n",
      "Epoch [7/20], Batch [340/938], Loss: 0.2471\n",
      "Epoch [7/20], Batch [350/938], Loss: 0.1280\n",
      "Epoch [7/20], Batch [360/938], Loss: 0.1807\n",
      "Epoch [7/20], Batch [370/938], Loss: 0.0916\n",
      "Epoch [7/20], Batch [380/938], Loss: 0.1482\n",
      "Epoch [7/20], Batch [390/938], Loss: 0.1607\n",
      "Epoch [7/20], Batch [400/938], Loss: 0.1035\n",
      "Epoch [7/20], Batch [410/938], Loss: 0.1307\n",
      "Epoch [7/20], Batch [420/938], Loss: 0.1907\n",
      "Epoch [7/20], Batch [430/938], Loss: 0.0684\n",
      "Epoch [7/20], Batch [440/938], Loss: 0.1613\n",
      "Epoch [7/20], Batch [450/938], Loss: 0.1329\n",
      "Epoch [7/20], Batch [460/938], Loss: 0.2490\n",
      "Epoch [7/20], Batch [470/938], Loss: 0.1628\n",
      "Epoch [7/20], Batch [480/938], Loss: 0.0606\n",
      "Epoch [7/20], Batch [490/938], Loss: 0.1784\n",
      "Epoch [7/20], Batch [500/938], Loss: 0.1848\n",
      "Epoch [7/20], Batch [510/938], Loss: 0.0765\n",
      "Epoch [7/20], Batch [520/938], Loss: 0.1068\n",
      "Epoch [7/20], Batch [530/938], Loss: 0.1661\n",
      "Epoch [7/20], Batch [540/938], Loss: 0.1227\n",
      "Epoch [7/20], Batch [550/938], Loss: 0.1485\n",
      "Epoch [7/20], Batch [560/938], Loss: 0.1817\n",
      "Epoch [7/20], Batch [570/938], Loss: 0.1817\n",
      "Epoch [7/20], Batch [580/938], Loss: 0.0919\n",
      "Epoch [7/20], Batch [590/938], Loss: 0.1342\n",
      "Epoch [7/20], Batch [600/938], Loss: 0.1845\n",
      "Epoch [7/20], Batch [610/938], Loss: 0.1244\n",
      "Epoch [7/20], Batch [620/938], Loss: 0.2207\n",
      "Epoch [7/20], Batch [630/938], Loss: 0.1450\n",
      "Epoch [7/20], Batch [640/938], Loss: 0.1205\n",
      "Epoch [7/20], Batch [650/938], Loss: 0.0910\n",
      "Epoch [7/20], Batch [660/938], Loss: 0.1081\n",
      "Epoch [7/20], Batch [670/938], Loss: 0.2653\n",
      "Epoch [7/20], Batch [680/938], Loss: 0.1274\n",
      "Epoch [7/20], Batch [690/938], Loss: 0.1656\n",
      "Epoch [7/20], Batch [700/938], Loss: 0.1947\n",
      "Epoch [7/20], Batch [710/938], Loss: 0.1586\n",
      "Epoch [7/20], Batch [720/938], Loss: 0.1983\n",
      "Epoch [7/20], Batch [730/938], Loss: 0.2013\n",
      "Epoch [7/20], Batch [740/938], Loss: 0.1160\n",
      "Epoch [7/20], Batch [750/938], Loss: 0.2756\n",
      "Epoch [7/20], Batch [760/938], Loss: 0.1149\n",
      "Epoch [7/20], Batch [770/938], Loss: 0.1048\n",
      "Epoch [7/20], Batch [780/938], Loss: 0.1034\n",
      "Epoch [7/20], Batch [790/938], Loss: 0.0750\n",
      "Epoch [7/20], Batch [800/938], Loss: 0.1526\n",
      "Epoch [7/20], Batch [810/938], Loss: 0.1500\n",
      "Epoch [7/20], Batch [820/938], Loss: 0.1945\n",
      "Epoch [7/20], Batch [830/938], Loss: 0.2690\n",
      "Epoch [7/20], Batch [840/938], Loss: 0.1804\n",
      "Epoch [7/20], Batch [850/938], Loss: 0.0998\n",
      "Epoch [7/20], Batch [860/938], Loss: 0.1805\n",
      "Epoch [7/20], Batch [870/938], Loss: 0.1149\n",
      "Epoch [7/20], Batch [880/938], Loss: 0.3311\n",
      "Epoch [7/20], Batch [890/938], Loss: 0.1809\n",
      "Epoch [7/20], Batch [900/938], Loss: 0.3565\n",
      "Epoch [7/20], Batch [910/938], Loss: 0.1821\n",
      "Epoch [7/20], Batch [920/938], Loss: 0.2614\n",
      "Epoch [7/20], Batch [930/938], Loss: 0.1599\n",
      "Epoch [7/20], Average Loss: 0.1670\n",
      "------------------------------------------------------------\n",
      "Epoch [8/20], Batch [10/938], Loss: 0.2142\n",
      "Epoch [8/20], Batch [20/938], Loss: 0.2515\n",
      "Epoch [8/20], Batch [30/938], Loss: 0.1932\n",
      "Epoch [8/20], Batch [40/938], Loss: 0.0632\n",
      "Epoch [8/20], Batch [50/938], Loss: 0.3005\n",
      "Epoch [8/20], Batch [60/938], Loss: 0.1026\n",
      "Epoch [8/20], Batch [70/938], Loss: 0.0494\n",
      "Epoch [8/20], Batch [80/938], Loss: 0.1399\n",
      "Epoch [8/20], Batch [90/938], Loss: 0.1637\n",
      "Epoch [8/20], Batch [100/938], Loss: 0.2189\n",
      "Epoch [8/20], Batch [110/938], Loss: 0.1401\n",
      "Epoch [8/20], Batch [120/938], Loss: 0.1359\n",
      "Epoch [8/20], Batch [130/938], Loss: 0.1286\n",
      "Epoch [8/20], Batch [140/938], Loss: 0.1295\n",
      "Epoch [8/20], Batch [150/938], Loss: 0.0412\n",
      "Epoch [8/20], Batch [160/938], Loss: 0.1501\n",
      "Epoch [8/20], Batch [170/938], Loss: 0.1302\n",
      "Epoch [8/20], Batch [180/938], Loss: 0.2084\n",
      "Epoch [8/20], Batch [190/938], Loss: 0.1177\n",
      "Epoch [8/20], Batch [200/938], Loss: 0.0799\n",
      "Epoch [8/20], Batch [210/938], Loss: 0.1077\n",
      "Epoch [8/20], Batch [220/938], Loss: 0.0828\n",
      "Epoch [8/20], Batch [230/938], Loss: 0.2615\n",
      "Epoch [8/20], Batch [240/938], Loss: 0.1426\n",
      "Epoch [8/20], Batch [250/938], Loss: 0.1453\n",
      "Epoch [8/20], Batch [260/938], Loss: 0.2068\n",
      "Epoch [8/20], Batch [270/938], Loss: 0.1774\n",
      "Epoch [8/20], Batch [280/938], Loss: 0.1652\n",
      "Epoch [8/20], Batch [290/938], Loss: 0.2008\n",
      "Epoch [8/20], Batch [300/938], Loss: 0.1868\n",
      "Epoch [8/20], Batch [310/938], Loss: 0.1458\n",
      "Epoch [8/20], Batch [320/938], Loss: 0.1926\n",
      "Epoch [8/20], Batch [330/938], Loss: 0.0937\n",
      "Epoch [8/20], Batch [340/938], Loss: 0.0993\n",
      "Epoch [8/20], Batch [350/938], Loss: 0.1711\n",
      "Epoch [8/20], Batch [360/938], Loss: 0.2272\n",
      "Epoch [8/20], Batch [370/938], Loss: 0.1535\n",
      "Epoch [8/20], Batch [380/938], Loss: 0.1852\n",
      "Epoch [8/20], Batch [390/938], Loss: 0.1014\n",
      "Epoch [8/20], Batch [400/938], Loss: 0.1743\n",
      "Epoch [8/20], Batch [410/938], Loss: 0.1402\n",
      "Epoch [8/20], Batch [420/938], Loss: 0.0434\n",
      "Epoch [8/20], Batch [430/938], Loss: 0.0813\n",
      "Epoch [8/20], Batch [440/938], Loss: 0.2237\n",
      "Epoch [8/20], Batch [450/938], Loss: 0.0811\n",
      "Epoch [8/20], Batch [460/938], Loss: 0.1201\n",
      "Epoch [8/20], Batch [470/938], Loss: 0.2701\n",
      "Epoch [8/20], Batch [480/938], Loss: 0.0991\n",
      "Epoch [8/20], Batch [490/938], Loss: 0.1299\n",
      "Epoch [8/20], Batch [500/938], Loss: 0.2108\n",
      "Epoch [8/20], Batch [510/938], Loss: 0.1643\n",
      "Epoch [8/20], Batch [520/938], Loss: 0.0663\n",
      "Epoch [8/20], Batch [530/938], Loss: 0.1227\n",
      "Epoch [8/20], Batch [540/938], Loss: 0.1107\n",
      "Epoch [8/20], Batch [550/938], Loss: 0.1690\n",
      "Epoch [8/20], Batch [560/938], Loss: 0.0902\n",
      "Epoch [8/20], Batch [570/938], Loss: 0.1192\n",
      "Epoch [8/20], Batch [580/938], Loss: 0.2007\n",
      "Epoch [8/20], Batch [590/938], Loss: 0.1831\n",
      "Epoch [8/20], Batch [600/938], Loss: 0.0828\n",
      "Epoch [8/20], Batch [610/938], Loss: 0.1469\n",
      "Epoch [8/20], Batch [620/938], Loss: 0.1413\n",
      "Epoch [8/20], Batch [630/938], Loss: 0.1322\n",
      "Epoch [8/20], Batch [640/938], Loss: 0.0865\n",
      "Epoch [8/20], Batch [650/938], Loss: 0.0644\n",
      "Epoch [8/20], Batch [660/938], Loss: 0.0574\n",
      "Epoch [8/20], Batch [670/938], Loss: 0.0856\n",
      "Epoch [8/20], Batch [680/938], Loss: 0.2034\n",
      "Epoch [8/20], Batch [690/938], Loss: 0.1682\n",
      "Epoch [8/20], Batch [700/938], Loss: 0.1822\n",
      "Epoch [8/20], Batch [710/938], Loss: 0.1311\n",
      "Epoch [8/20], Batch [720/938], Loss: 0.0749\n",
      "Epoch [8/20], Batch [730/938], Loss: 0.0320\n",
      "Epoch [8/20], Batch [740/938], Loss: 0.0944\n",
      "Epoch [8/20], Batch [750/938], Loss: 0.2115\n",
      "Epoch [8/20], Batch [760/938], Loss: 0.0604\n",
      "Epoch [8/20], Batch [770/938], Loss: 0.1971\n",
      "Epoch [8/20], Batch [780/938], Loss: 0.0757\n",
      "Epoch [8/20], Batch [790/938], Loss: 0.1715\n",
      "Epoch [8/20], Batch [800/938], Loss: 0.1114\n",
      "Epoch [8/20], Batch [810/938], Loss: 0.1107\n",
      "Epoch [8/20], Batch [820/938], Loss: 0.2952\n",
      "Epoch [8/20], Batch [830/938], Loss: 0.0559\n",
      "Epoch [8/20], Batch [840/938], Loss: 0.1934\n",
      "Epoch [8/20], Batch [850/938], Loss: 0.0971\n",
      "Epoch [8/20], Batch [860/938], Loss: 0.2730\n",
      "Epoch [8/20], Batch [870/938], Loss: 0.1315\n",
      "Epoch [8/20], Batch [880/938], Loss: 0.0860\n",
      "Epoch [8/20], Batch [890/938], Loss: 0.1290\n",
      "Epoch [8/20], Batch [900/938], Loss: 0.0825\n",
      "Epoch [8/20], Batch [910/938], Loss: 0.2652\n",
      "Epoch [8/20], Batch [920/938], Loss: 0.1996\n",
      "Epoch [8/20], Batch [930/938], Loss: 0.1622\n",
      "Epoch [8/20], Average Loss: 0.1522\n",
      "------------------------------------------------------------\n",
      "Epoch [9/20], Batch [10/938], Loss: 0.1289\n",
      "Epoch [9/20], Batch [20/938], Loss: 0.1381\n",
      "Epoch [9/20], Batch [30/938], Loss: 0.0552\n",
      "Epoch [9/20], Batch [40/938], Loss: 0.1232\n",
      "Epoch [9/20], Batch [50/938], Loss: 0.1467\n",
      "Epoch [9/20], Batch [60/938], Loss: 0.2018\n",
      "Epoch [9/20], Batch [70/938], Loss: 0.0709\n",
      "Epoch [9/20], Batch [80/938], Loss: 0.0704\n",
      "Epoch [9/20], Batch [90/938], Loss: 0.1362\n",
      "Epoch [9/20], Batch [100/938], Loss: 0.1653\n",
      "Epoch [9/20], Batch [110/938], Loss: 0.1792\n",
      "Epoch [9/20], Batch [120/938], Loss: 0.0948\n",
      "Epoch [9/20], Batch [130/938], Loss: 0.2758\n",
      "Epoch [9/20], Batch [140/938], Loss: 0.1040\n",
      "Epoch [9/20], Batch [150/938], Loss: 0.1082\n",
      "Epoch [9/20], Batch [160/938], Loss: 0.1564\n",
      "Epoch [9/20], Batch [170/938], Loss: 0.2836\n",
      "Epoch [9/20], Batch [180/938], Loss: 0.0939\n",
      "Epoch [9/20], Batch [190/938], Loss: 0.0916\n",
      "Epoch [9/20], Batch [200/938], Loss: 0.1320\n",
      "Epoch [9/20], Batch [210/938], Loss: 0.1502\n",
      "Epoch [9/20], Batch [220/938], Loss: 0.2686\n",
      "Epoch [9/20], Batch [230/938], Loss: 0.1550\n",
      "Epoch [9/20], Batch [240/938], Loss: 0.2117\n",
      "Epoch [9/20], Batch [250/938], Loss: 0.1301\n",
      "Epoch [9/20], Batch [260/938], Loss: 0.1780\n",
      "Epoch [9/20], Batch [270/938], Loss: 0.1191\n",
      "Epoch [9/20], Batch [280/938], Loss: 0.1148\n",
      "Epoch [9/20], Batch [290/938], Loss: 0.0651\n",
      "Epoch [9/20], Batch [300/938], Loss: 0.1426\n",
      "Epoch [9/20], Batch [310/938], Loss: 0.1968\n",
      "Epoch [9/20], Batch [320/938], Loss: 0.1418\n",
      "Epoch [9/20], Batch [330/938], Loss: 0.2160\n",
      "Epoch [9/20], Batch [340/938], Loss: 0.1360\n",
      "Epoch [9/20], Batch [350/938], Loss: 0.0767\n",
      "Epoch [9/20], Batch [360/938], Loss: 0.1371\n",
      "Epoch [9/20], Batch [370/938], Loss: 0.1884\n",
      "Epoch [9/20], Batch [380/938], Loss: 0.1366\n",
      "Epoch [9/20], Batch [390/938], Loss: 0.2149\n",
      "Epoch [9/20], Batch [400/938], Loss: 0.1172\n",
      "Epoch [9/20], Batch [410/938], Loss: 0.2668\n",
      "Epoch [9/20], Batch [420/938], Loss: 0.0735\n",
      "Epoch [9/20], Batch [430/938], Loss: 0.1197\n",
      "Epoch [9/20], Batch [440/938], Loss: 0.1742\n",
      "Epoch [9/20], Batch [450/938], Loss: 0.1312\n",
      "Epoch [9/20], Batch [460/938], Loss: 0.1796\n",
      "Epoch [9/20], Batch [470/938], Loss: 0.0604\n",
      "Epoch [9/20], Batch [480/938], Loss: 0.1979\n",
      "Epoch [9/20], Batch [490/938], Loss: 0.1748\n",
      "Epoch [9/20], Batch [500/938], Loss: 0.1155\n",
      "Epoch [9/20], Batch [510/938], Loss: 0.1584\n",
      "Epoch [9/20], Batch [520/938], Loss: 0.1338\n",
      "Epoch [9/20], Batch [530/938], Loss: 0.0712\n",
      "Epoch [9/20], Batch [540/938], Loss: 0.0943\n",
      "Epoch [9/20], Batch [550/938], Loss: 0.1044\n",
      "Epoch [9/20], Batch [560/938], Loss: 0.1525\n",
      "Epoch [9/20], Batch [570/938], Loss: 0.1315\n",
      "Epoch [9/20], Batch [580/938], Loss: 0.0741\n",
      "Epoch [9/20], Batch [590/938], Loss: 0.1176\n",
      "Epoch [9/20], Batch [600/938], Loss: 0.2966\n",
      "Epoch [9/20], Batch [610/938], Loss: 0.1859\n",
      "Epoch [9/20], Batch [620/938], Loss: 0.1183\n",
      "Epoch [9/20], Batch [630/938], Loss: 0.1425\n",
      "Epoch [9/20], Batch [640/938], Loss: 0.1488\n",
      "Epoch [9/20], Batch [650/938], Loss: 0.1641\n",
      "Epoch [9/20], Batch [660/938], Loss: 0.2005\n",
      "Epoch [9/20], Batch [670/938], Loss: 0.1092\n",
      "Epoch [9/20], Batch [680/938], Loss: 0.0273\n",
      "Epoch [9/20], Batch [690/938], Loss: 0.2077\n",
      "Epoch [9/20], Batch [700/938], Loss: 0.0915\n",
      "Epoch [9/20], Batch [710/938], Loss: 0.1761\n",
      "Epoch [9/20], Batch [720/938], Loss: 0.1386\n",
      "Epoch [9/20], Batch [730/938], Loss: 0.1771\n",
      "Epoch [9/20], Batch [740/938], Loss: 0.0665\n",
      "Epoch [9/20], Batch [750/938], Loss: 0.1549\n",
      "Epoch [9/20], Batch [760/938], Loss: 0.2033\n",
      "Epoch [9/20], Batch [770/938], Loss: 0.1762\n",
      "Epoch [9/20], Batch [780/938], Loss: 0.1522\n",
      "Epoch [9/20], Batch [790/938], Loss: 0.0506\n",
      "Epoch [9/20], Batch [800/938], Loss: 0.1576\n",
      "Epoch [9/20], Batch [810/938], Loss: 0.1121\n",
      "Epoch [9/20], Batch [820/938], Loss: 0.1462\n",
      "Epoch [9/20], Batch [830/938], Loss: 0.2007\n",
      "Epoch [9/20], Batch [840/938], Loss: 0.0725\n",
      "Epoch [9/20], Batch [850/938], Loss: 0.1663\n",
      "Epoch [9/20], Batch [860/938], Loss: 0.1032\n",
      "Epoch [9/20], Batch [870/938], Loss: 0.0605\n",
      "Epoch [9/20], Batch [880/938], Loss: 0.0869\n",
      "Epoch [9/20], Batch [890/938], Loss: 0.0723\n",
      "Epoch [9/20], Batch [900/938], Loss: 0.1936\n",
      "Epoch [9/20], Batch [910/938], Loss: 0.1462\n",
      "Epoch [9/20], Batch [920/938], Loss: 0.1022\n",
      "Epoch [9/20], Batch [930/938], Loss: 0.1137\n",
      "Epoch [9/20], Average Loss: 0.1352\n",
      "------------------------------------------------------------\n",
      "Epoch [10/20], Batch [10/938], Loss: 0.1126\n",
      "Epoch [10/20], Batch [20/938], Loss: 0.0923\n",
      "Epoch [10/20], Batch [30/938], Loss: 0.1351\n",
      "Epoch [10/20], Batch [40/938], Loss: 0.0753\n",
      "Epoch [10/20], Batch [50/938], Loss: 0.2357\n",
      "Epoch [10/20], Batch [60/938], Loss: 0.0450\n",
      "Epoch [10/20], Batch [70/938], Loss: 0.1026\n",
      "Epoch [10/20], Batch [80/938], Loss: 0.1836\n",
      "Epoch [10/20], Batch [90/938], Loss: 0.1145\n",
      "Epoch [10/20], Batch [100/938], Loss: 0.0882\n",
      "Epoch [10/20], Batch [110/938], Loss: 0.0539\n",
      "Epoch [10/20], Batch [120/938], Loss: 0.1664\n",
      "Epoch [10/20], Batch [130/938], Loss: 0.0673\n",
      "Epoch [10/20], Batch [140/938], Loss: 0.1198\n",
      "Epoch [10/20], Batch [150/938], Loss: 0.0834\n",
      "Epoch [10/20], Batch [160/938], Loss: 0.0668\n",
      "Epoch [10/20], Batch [170/938], Loss: 0.0723\n",
      "Epoch [10/20], Batch [180/938], Loss: 0.0319\n",
      "Epoch [10/20], Batch [190/938], Loss: 0.0874\n",
      "Epoch [10/20], Batch [200/938], Loss: 0.1334\n",
      "Epoch [10/20], Batch [210/938], Loss: 0.2689\n",
      "Epoch [10/20], Batch [220/938], Loss: 0.1701\n",
      "Epoch [10/20], Batch [230/938], Loss: 0.0824\n",
      "Epoch [10/20], Batch [240/938], Loss: 0.1606\n",
      "Epoch [10/20], Batch [250/938], Loss: 0.0883\n",
      "Epoch [10/20], Batch [260/938], Loss: 0.1353\n",
      "Epoch [10/20], Batch [270/938], Loss: 0.1143\n",
      "Epoch [10/20], Batch [280/938], Loss: 0.1263\n",
      "Epoch [10/20], Batch [290/938], Loss: 0.0757\n",
      "Epoch [10/20], Batch [300/938], Loss: 0.1222\n",
      "Epoch [10/20], Batch [310/938], Loss: 0.0421\n",
      "Epoch [10/20], Batch [320/938], Loss: 0.0590\n",
      "Epoch [10/20], Batch [330/938], Loss: 0.0416\n",
      "Epoch [10/20], Batch [340/938], Loss: 0.1810\n",
      "Epoch [10/20], Batch [350/938], Loss: 0.0596\n",
      "Epoch [10/20], Batch [360/938], Loss: 0.0697\n",
      "Epoch [10/20], Batch [370/938], Loss: 0.1421\n",
      "Epoch [10/20], Batch [380/938], Loss: 0.1085\n",
      "Epoch [10/20], Batch [390/938], Loss: 0.0306\n",
      "Epoch [10/20], Batch [400/938], Loss: 0.1174\n",
      "Epoch [10/20], Batch [410/938], Loss: 0.1437\n",
      "Epoch [10/20], Batch [420/938], Loss: 0.0473\n",
      "Epoch [10/20], Batch [430/938], Loss: 0.1356\n",
      "Epoch [10/20], Batch [440/938], Loss: 0.0966\n",
      "Epoch [10/20], Batch [450/938], Loss: 0.0513\n",
      "Epoch [10/20], Batch [460/938], Loss: 0.0668\n",
      "Epoch [10/20], Batch [470/938], Loss: 0.0764\n",
      "Epoch [10/20], Batch [480/938], Loss: 0.2401\n",
      "Epoch [10/20], Batch [490/938], Loss: 0.1215\n",
      "Epoch [10/20], Batch [500/938], Loss: 0.0442\n",
      "Epoch [10/20], Batch [510/938], Loss: 0.1095\n",
      "Epoch [10/20], Batch [520/938], Loss: 0.1385\n",
      "Epoch [10/20], Batch [530/938], Loss: 0.1383\n",
      "Epoch [10/20], Batch [540/938], Loss: 0.1027\n",
      "Epoch [10/20], Batch [550/938], Loss: 0.0899\n",
      "Epoch [10/20], Batch [560/938], Loss: 0.0906\n",
      "Epoch [10/20], Batch [570/938], Loss: 0.1079\n",
      "Epoch [10/20], Batch [580/938], Loss: 0.1209\n",
      "Epoch [10/20], Batch [590/938], Loss: 0.1239\n",
      "Epoch [10/20], Batch [600/938], Loss: 0.0839\n",
      "Epoch [10/20], Batch [610/938], Loss: 0.0964\n",
      "Epoch [10/20], Batch [620/938], Loss: 0.1579\n",
      "Epoch [10/20], Batch [630/938], Loss: 0.1714\n",
      "Epoch [10/20], Batch [640/938], Loss: 0.1765\n",
      "Epoch [10/20], Batch [650/938], Loss: 0.0417\n",
      "Epoch [10/20], Batch [660/938], Loss: 0.1512\n",
      "Epoch [10/20], Batch [670/938], Loss: 0.1458\n",
      "Epoch [10/20], Batch [680/938], Loss: 0.0678\n",
      "Epoch [10/20], Batch [690/938], Loss: 0.0641\n",
      "Epoch [10/20], Batch [700/938], Loss: 0.1167\n",
      "Epoch [10/20], Batch [710/938], Loss: 0.0513\n",
      "Epoch [10/20], Batch [720/938], Loss: 0.1423\n",
      "Epoch [10/20], Batch [730/938], Loss: 0.1879\n",
      "Epoch [10/20], Batch [740/938], Loss: 0.1151\n",
      "Epoch [10/20], Batch [750/938], Loss: 0.1124\n",
      "Epoch [10/20], Batch [760/938], Loss: 0.0770\n",
      "Epoch [10/20], Batch [770/938], Loss: 0.3321\n",
      "Epoch [10/20], Batch [780/938], Loss: 0.0971\n",
      "Epoch [10/20], Batch [790/938], Loss: 0.1563\n",
      "Epoch [10/20], Batch [800/938], Loss: 0.0673\n",
      "Epoch [10/20], Batch [810/938], Loss: 0.2161\n",
      "Epoch [10/20], Batch [820/938], Loss: 0.1634\n",
      "Epoch [10/20], Batch [830/938], Loss: 0.1951\n",
      "Epoch [10/20], Batch [840/938], Loss: 0.2594\n",
      "Epoch [10/20], Batch [850/938], Loss: 0.0244\n",
      "Epoch [10/20], Batch [860/938], Loss: 0.1141\n",
      "Epoch [10/20], Batch [870/938], Loss: 0.2404\n",
      "Epoch [10/20], Batch [880/938], Loss: 0.0157\n",
      "Epoch [10/20], Batch [890/938], Loss: 0.1646\n",
      "Epoch [10/20], Batch [900/938], Loss: 0.0645\n",
      "Epoch [10/20], Batch [910/938], Loss: 0.2303\n",
      "Epoch [10/20], Batch [920/938], Loss: 0.1370\n",
      "Epoch [10/20], Batch [930/938], Loss: 0.2899\n",
      "Epoch [10/20], Average Loss: 0.1201\n",
      "------------------------------------------------------------\n",
      "Epoch [11/20], Batch [10/938], Loss: 0.1142\n",
      "Epoch [11/20], Batch [20/938], Loss: 0.1047\n",
      "Epoch [11/20], Batch [30/938], Loss: 0.0446\n",
      "Epoch [11/20], Batch [40/938], Loss: 0.1475\n",
      "Epoch [11/20], Batch [50/938], Loss: 0.0370\n",
      "Epoch [11/20], Batch [60/938], Loss: 0.0877\n",
      "Epoch [11/20], Batch [70/938], Loss: 0.1246\n",
      "Epoch [11/20], Batch [80/938], Loss: 0.0849\n",
      "Epoch [11/20], Batch [90/938], Loss: 0.0449\n",
      "Epoch [11/20], Batch [100/938], Loss: 0.1135\n",
      "Epoch [11/20], Batch [110/938], Loss: 0.3548\n",
      "Epoch [11/20], Batch [120/938], Loss: 0.1370\n",
      "Epoch [11/20], Batch [130/938], Loss: 0.2400\n",
      "Epoch [11/20], Batch [140/938], Loss: 0.1227\n",
      "Epoch [11/20], Batch [150/938], Loss: 0.0962\n",
      "Epoch [11/20], Batch [160/938], Loss: 0.0182\n",
      "Epoch [11/20], Batch [170/938], Loss: 0.0905\n",
      "Epoch [11/20], Batch [180/938], Loss: 0.2090\n",
      "Epoch [11/20], Batch [190/938], Loss: 0.0987\n",
      "Epoch [11/20], Batch [200/938], Loss: 0.0543\n",
      "Epoch [11/20], Batch [210/938], Loss: 0.2173\n",
      "Epoch [11/20], Batch [220/938], Loss: 0.1306\n",
      "Epoch [11/20], Batch [230/938], Loss: 0.0970\n",
      "Epoch [11/20], Batch [240/938], Loss: 0.1389\n",
      "Epoch [11/20], Batch [250/938], Loss: 0.1130\n",
      "Epoch [11/20], Batch [260/938], Loss: 0.0752\n",
      "Epoch [11/20], Batch [270/938], Loss: 0.1368\n",
      "Epoch [11/20], Batch [280/938], Loss: 0.1238\n",
      "Epoch [11/20], Batch [290/938], Loss: 0.1079\n",
      "Epoch [11/20], Batch [300/938], Loss: 0.0866\n",
      "Epoch [11/20], Batch [310/938], Loss: 0.2003\n",
      "Epoch [11/20], Batch [320/938], Loss: 0.0612\n",
      "Epoch [11/20], Batch [330/938], Loss: 0.0452\n",
      "Epoch [11/20], Batch [340/938], Loss: 0.0751\n",
      "Epoch [11/20], Batch [350/938], Loss: 0.0861\n",
      "Epoch [11/20], Batch [360/938], Loss: 0.1365\n",
      "Epoch [11/20], Batch [370/938], Loss: 0.0665\n",
      "Epoch [11/20], Batch [380/938], Loss: 0.1324\n",
      "Epoch [11/20], Batch [390/938], Loss: 0.0667\n",
      "Epoch [11/20], Batch [400/938], Loss: 0.0641\n",
      "Epoch [11/20], Batch [410/938], Loss: 0.1286\n",
      "Epoch [11/20], Batch [420/938], Loss: 0.1032\n",
      "Epoch [11/20], Batch [430/938], Loss: 0.1244\n",
      "Epoch [11/20], Batch [440/938], Loss: 0.1117\n",
      "Epoch [11/20], Batch [450/938], Loss: 0.1095\n",
      "Epoch [11/20], Batch [460/938], Loss: 0.0749\n",
      "Epoch [11/20], Batch [470/938], Loss: 0.1708\n",
      "Epoch [11/20], Batch [480/938], Loss: 0.1447\n",
      "Epoch [11/20], Batch [490/938], Loss: 0.0646\n",
      "Epoch [11/20], Batch [500/938], Loss: 0.0359\n",
      "Epoch [11/20], Batch [510/938], Loss: 0.1082\n",
      "Epoch [11/20], Batch [520/938], Loss: 0.1299\n",
      "Epoch [11/20], Batch [530/938], Loss: 0.1573\n",
      "Epoch [11/20], Batch [540/938], Loss: 0.2044\n",
      "Epoch [11/20], Batch [550/938], Loss: 0.0349\n",
      "Epoch [11/20], Batch [560/938], Loss: 0.0588\n",
      "Epoch [11/20], Batch [570/938], Loss: 0.0996\n",
      "Epoch [11/20], Batch [580/938], Loss: 0.1606\n",
      "Epoch [11/20], Batch [590/938], Loss: 0.0447\n",
      "Epoch [11/20], Batch [600/938], Loss: 0.0620\n",
      "Epoch [11/20], Batch [610/938], Loss: 0.0244\n",
      "Epoch [11/20], Batch [620/938], Loss: 0.0236\n",
      "Epoch [11/20], Batch [630/938], Loss: 0.0771\n",
      "Epoch [11/20], Batch [640/938], Loss: 0.0565\n",
      "Epoch [11/20], Batch [650/938], Loss: 0.1875\n",
      "Epoch [11/20], Batch [660/938], Loss: 0.0494\n",
      "Epoch [11/20], Batch [670/938], Loss: 0.0790\n",
      "Epoch [11/20], Batch [680/938], Loss: 0.0846\n",
      "Epoch [11/20], Batch [690/938], Loss: 0.0954\n",
      "Epoch [11/20], Batch [700/938], Loss: 0.1500\n",
      "Epoch [11/20], Batch [710/938], Loss: 0.0512\n",
      "Epoch [11/20], Batch [720/938], Loss: 0.0619\n",
      "Epoch [11/20], Batch [730/938], Loss: 0.0828\n",
      "Epoch [11/20], Batch [740/938], Loss: 0.1649\n",
      "Epoch [11/20], Batch [750/938], Loss: 0.1071\n",
      "Epoch [11/20], Batch [760/938], Loss: 0.1751\n",
      "Epoch [11/20], Batch [770/938], Loss: 0.0966\n",
      "Epoch [11/20], Batch [780/938], Loss: 0.1430\n",
      "Epoch [11/20], Batch [790/938], Loss: 0.1109\n",
      "Epoch [11/20], Batch [800/938], Loss: 0.0414\n",
      "Epoch [11/20], Batch [810/938], Loss: 0.0639\n",
      "Epoch [11/20], Batch [820/938], Loss: 0.1251\n",
      "Epoch [11/20], Batch [830/938], Loss: 0.0460\n",
      "Epoch [11/20], Batch [840/938], Loss: 0.1475\n",
      "Epoch [11/20], Batch [850/938], Loss: 0.1157\n",
      "Epoch [11/20], Batch [860/938], Loss: 0.1409\n",
      "Epoch [11/20], Batch [870/938], Loss: 0.0902\n",
      "Epoch [11/20], Batch [880/938], Loss: 0.0642\n",
      "Epoch [11/20], Batch [890/938], Loss: 0.0179\n",
      "Epoch [11/20], Batch [900/938], Loss: 0.0305\n",
      "Epoch [11/20], Batch [910/938], Loss: 0.1316\n",
      "Epoch [11/20], Batch [920/938], Loss: 0.0639\n",
      "Epoch [11/20], Batch [930/938], Loss: 0.0705\n",
      "Epoch [11/20], Average Loss: 0.1073\n",
      "------------------------------------------------------------\n",
      "Epoch [12/20], Batch [10/938], Loss: 0.1776\n",
      "Epoch [12/20], Batch [20/938], Loss: 0.0998\n",
      "Epoch [12/20], Batch [30/938], Loss: 0.0632\n",
      "Epoch [12/20], Batch [40/938], Loss: 0.0844\n",
      "Epoch [12/20], Batch [50/938], Loss: 0.1517\n",
      "Epoch [12/20], Batch [60/938], Loss: 0.1417\n",
      "Epoch [12/20], Batch [70/938], Loss: 0.0849\n",
      "Epoch [12/20], Batch [80/938], Loss: 0.0882\n",
      "Epoch [12/20], Batch [90/938], Loss: 0.1525\n",
      "Epoch [12/20], Batch [100/938], Loss: 0.0927\n",
      "Epoch [12/20], Batch [110/938], Loss: 0.1377\n",
      "Epoch [12/20], Batch [120/938], Loss: 0.0378\n",
      "Epoch [12/20], Batch [130/938], Loss: 0.1522\n",
      "Epoch [12/20], Batch [140/938], Loss: 0.0753\n",
      "Epoch [12/20], Batch [150/938], Loss: 0.0897\n",
      "Epoch [12/20], Batch [160/938], Loss: 0.0857\n",
      "Epoch [12/20], Batch [170/938], Loss: 0.0754\n",
      "Epoch [12/20], Batch [180/938], Loss: 0.1371\n",
      "Epoch [12/20], Batch [190/938], Loss: 0.0235\n",
      "Epoch [12/20], Batch [200/938], Loss: 0.1001\n",
      "Epoch [12/20], Batch [210/938], Loss: 0.1193\n",
      "Epoch [12/20], Batch [220/938], Loss: 0.0846\n",
      "Epoch [12/20], Batch [230/938], Loss: 0.0468\n",
      "Epoch [12/20], Batch [240/938], Loss: 0.1403\n",
      "Epoch [12/20], Batch [250/938], Loss: 0.0443\n",
      "Epoch [12/20], Batch [260/938], Loss: 0.0559\n",
      "Epoch [12/20], Batch [270/938], Loss: 0.0436\n",
      "Epoch [12/20], Batch [280/938], Loss: 0.1043\n",
      "Epoch [12/20], Batch [290/938], Loss: 0.2411\n",
      "Epoch [12/20], Batch [300/938], Loss: 0.1704\n",
      "Epoch [12/20], Batch [310/938], Loss: 0.1022\n",
      "Epoch [12/20], Batch [320/938], Loss: 0.1189\n",
      "Epoch [12/20], Batch [330/938], Loss: 0.1609\n",
      "Epoch [12/20], Batch [340/938], Loss: 0.0676\n",
      "Epoch [12/20], Batch [350/938], Loss: 0.0952\n",
      "Epoch [12/20], Batch [360/938], Loss: 0.1176\n",
      "Epoch [12/20], Batch [370/938], Loss: 0.1745\n",
      "Epoch [12/20], Batch [380/938], Loss: 0.1179\n",
      "Epoch [12/20], Batch [390/938], Loss: 0.1387\n",
      "Epoch [12/20], Batch [400/938], Loss: 0.0528\n",
      "Epoch [12/20], Batch [410/938], Loss: 0.1408\n",
      "Epoch [12/20], Batch [420/938], Loss: 0.1311\n",
      "Epoch [12/20], Batch [430/938], Loss: 0.0327\n",
      "Epoch [12/20], Batch [440/938], Loss: 0.1829\n",
      "Epoch [12/20], Batch [450/938], Loss: 0.0440\n",
      "Epoch [12/20], Batch [460/938], Loss: 0.0659\n",
      "Epoch [12/20], Batch [470/938], Loss: 0.0494\n",
      "Epoch [12/20], Batch [480/938], Loss: 0.0597\n",
      "Epoch [12/20], Batch [490/938], Loss: 0.0509\n",
      "Epoch [12/20], Batch [500/938], Loss: 0.1615\n",
      "Epoch [12/20], Batch [510/938], Loss: 0.0303\n",
      "Epoch [12/20], Batch [520/938], Loss: 0.1151\n",
      "Epoch [12/20], Batch [530/938], Loss: 0.0932\n",
      "Epoch [12/20], Batch [540/938], Loss: 0.1664\n",
      "Epoch [12/20], Batch [550/938], Loss: 0.1059\n",
      "Epoch [12/20], Batch [560/938], Loss: 0.0884\n",
      "Epoch [12/20], Batch [570/938], Loss: 0.1314\n",
      "Epoch [12/20], Batch [580/938], Loss: 0.1818\n",
      "Epoch [12/20], Batch [590/938], Loss: 0.0810\n",
      "Epoch [12/20], Batch [600/938], Loss: 0.0867\n",
      "Epoch [12/20], Batch [610/938], Loss: 0.0583\n",
      "Epoch [12/20], Batch [620/938], Loss: 0.1059\n",
      "Epoch [12/20], Batch [630/938], Loss: 0.1143\n",
      "Epoch [12/20], Batch [640/938], Loss: 0.0657\n",
      "Epoch [12/20], Batch [650/938], Loss: 0.0974\n",
      "Epoch [12/20], Batch [660/938], Loss: 0.0515\n",
      "Epoch [12/20], Batch [670/938], Loss: 0.0438\n",
      "Epoch [12/20], Batch [680/938], Loss: 0.0645\n",
      "Epoch [12/20], Batch [690/938], Loss: 0.1064\n",
      "Epoch [12/20], Batch [700/938], Loss: 0.1500\n",
      "Epoch [12/20], Batch [710/938], Loss: 0.2539\n",
      "Epoch [12/20], Batch [720/938], Loss: 0.1188\n",
      "Epoch [12/20], Batch [730/938], Loss: 0.2393\n",
      "Epoch [12/20], Batch [740/938], Loss: 0.1386\n",
      "Epoch [12/20], Batch [750/938], Loss: 0.0958\n",
      "Epoch [12/20], Batch [760/938], Loss: 0.1159\n",
      "Epoch [12/20], Batch [770/938], Loss: 0.2254\n",
      "Epoch [12/20], Batch [780/938], Loss: 0.1025\n",
      "Epoch [12/20], Batch [790/938], Loss: 0.0842\n",
      "Epoch [12/20], Batch [800/938], Loss: 0.1310\n",
      "Epoch [12/20], Batch [810/938], Loss: 0.0802\n",
      "Epoch [12/20], Batch [820/938], Loss: 0.1344\n",
      "Epoch [12/20], Batch [830/938], Loss: 0.0928\n",
      "Epoch [12/20], Batch [840/938], Loss: 0.0902\n",
      "Epoch [12/20], Batch [850/938], Loss: 0.2127\n",
      "Epoch [12/20], Batch [860/938], Loss: 0.0714\n",
      "Epoch [12/20], Batch [870/938], Loss: 0.0441\n",
      "Epoch [12/20], Batch [880/938], Loss: 0.0611\n",
      "Epoch [12/20], Batch [890/938], Loss: 0.0489\n",
      "Epoch [12/20], Batch [900/938], Loss: 0.0470\n",
      "Epoch [12/20], Batch [910/938], Loss: 0.0773\n",
      "Epoch [12/20], Batch [920/938], Loss: 0.0747\n",
      "Epoch [12/20], Batch [930/938], Loss: 0.0574\n",
      "Epoch [12/20], Average Loss: 0.0931\n",
      "------------------------------------------------------------\n",
      "Epoch [13/20], Batch [10/938], Loss: 0.0763\n",
      "Epoch [13/20], Batch [20/938], Loss: 0.0448\n",
      "Epoch [13/20], Batch [30/938], Loss: 0.0952\n",
      "Epoch [13/20], Batch [40/938], Loss: 0.0212\n",
      "Epoch [13/20], Batch [50/938], Loss: 0.1012\n",
      "Epoch [13/20], Batch [60/938], Loss: 0.0669\n",
      "Epoch [13/20], Batch [70/938], Loss: 0.0228\n",
      "Epoch [13/20], Batch [80/938], Loss: 0.1737\n",
      "Epoch [13/20], Batch [90/938], Loss: 0.0987\n",
      "Epoch [13/20], Batch [100/938], Loss: 0.0706\n",
      "Epoch [13/20], Batch [110/938], Loss: 0.0338\n",
      "Epoch [13/20], Batch [120/938], Loss: 0.1791\n",
      "Epoch [13/20], Batch [130/938], Loss: 0.0933\n",
      "Epoch [13/20], Batch [140/938], Loss: 0.2111\n",
      "Epoch [13/20], Batch [150/938], Loss: 0.0744\n",
      "Epoch [13/20], Batch [160/938], Loss: 0.0504\n",
      "Epoch [13/20], Batch [170/938], Loss: 0.0493\n",
      "Epoch [13/20], Batch [180/938], Loss: 0.0782\n",
      "Epoch [13/20], Batch [190/938], Loss: 0.0424\n",
      "Epoch [13/20], Batch [200/938], Loss: 0.0435\n",
      "Epoch [13/20], Batch [210/938], Loss: 0.0586\n",
      "Epoch [13/20], Batch [220/938], Loss: 0.0939\n",
      "Epoch [13/20], Batch [230/938], Loss: 0.0925\n",
      "Epoch [13/20], Batch [240/938], Loss: 0.1344\n",
      "Epoch [13/20], Batch [250/938], Loss: 0.1339\n",
      "Epoch [13/20], Batch [260/938], Loss: 0.0694\n",
      "Epoch [13/20], Batch [270/938], Loss: 0.1466\n",
      "Epoch [13/20], Batch [280/938], Loss: 0.1501\n",
      "Epoch [13/20], Batch [290/938], Loss: 0.0748\n",
      "Epoch [13/20], Batch [300/938], Loss: 0.1551\n",
      "Epoch [13/20], Batch [310/938], Loss: 0.0921\n",
      "Epoch [13/20], Batch [320/938], Loss: 0.0230\n",
      "Epoch [13/20], Batch [330/938], Loss: 0.0286\n",
      "Epoch [13/20], Batch [340/938], Loss: 0.1162\n",
      "Epoch [13/20], Batch [350/938], Loss: 0.0862\n",
      "Epoch [13/20], Batch [360/938], Loss: 0.0793\n",
      "Epoch [13/20], Batch [370/938], Loss: 0.1020\n",
      "Epoch [13/20], Batch [380/938], Loss: 0.0912\n",
      "Epoch [13/20], Batch [390/938], Loss: 0.0151\n",
      "Epoch [13/20], Batch [400/938], Loss: 0.1420\n",
      "Epoch [13/20], Batch [410/938], Loss: 0.1234\n",
      "Epoch [13/20], Batch [420/938], Loss: 0.0748\n",
      "Epoch [13/20], Batch [430/938], Loss: 0.0602\n",
      "Epoch [13/20], Batch [440/938], Loss: 0.0876\n",
      "Epoch [13/20], Batch [450/938], Loss: 0.1062\n",
      "Epoch [13/20], Batch [460/938], Loss: 0.0946\n",
      "Epoch [13/20], Batch [470/938], Loss: 0.0367\n",
      "Epoch [13/20], Batch [480/938], Loss: 0.1074\n",
      "Epoch [13/20], Batch [490/938], Loss: 0.0510\n",
      "Epoch [13/20], Batch [500/938], Loss: 0.0730\n",
      "Epoch [13/20], Batch [510/938], Loss: 0.0901\n",
      "Epoch [13/20], Batch [520/938], Loss: 0.0676\n",
      "Epoch [13/20], Batch [530/938], Loss: 0.0724\n",
      "Epoch [13/20], Batch [540/938], Loss: 0.0501\n",
      "Epoch [13/20], Batch [550/938], Loss: 0.1086\n",
      "Epoch [13/20], Batch [560/938], Loss: 0.1764\n",
      "Epoch [13/20], Batch [570/938], Loss: 0.1387\n",
      "Epoch [13/20], Batch [580/938], Loss: 0.0711\n",
      "Epoch [13/20], Batch [590/938], Loss: 0.0578\n",
      "Epoch [13/20], Batch [600/938], Loss: 0.0625\n",
      "Epoch [13/20], Batch [610/938], Loss: 0.1246\n",
      "Epoch [13/20], Batch [620/938], Loss: 0.0922\n",
      "Epoch [13/20], Batch [630/938], Loss: 0.0425\n",
      "Epoch [13/20], Batch [640/938], Loss: 0.0548\n",
      "Epoch [13/20], Batch [650/938], Loss: 0.0521\n",
      "Epoch [13/20], Batch [660/938], Loss: 0.0145\n",
      "Epoch [13/20], Batch [670/938], Loss: 0.0529\n",
      "Epoch [13/20], Batch [680/938], Loss: 0.0572\n",
      "Epoch [13/20], Batch [690/938], Loss: 0.0751\n",
      "Epoch [13/20], Batch [700/938], Loss: 0.1612\n",
      "Epoch [13/20], Batch [710/938], Loss: 0.0991\n",
      "Epoch [13/20], Batch [720/938], Loss: 0.0507\n",
      "Epoch [13/20], Batch [730/938], Loss: 0.1800\n",
      "Epoch [13/20], Batch [740/938], Loss: 0.0583\n",
      "Epoch [13/20], Batch [750/938], Loss: 0.0211\n",
      "Epoch [13/20], Batch [760/938], Loss: 0.1271\n",
      "Epoch [13/20], Batch [770/938], Loss: 0.0304\n",
      "Epoch [13/20], Batch [780/938], Loss: 0.0564\n",
      "Epoch [13/20], Batch [790/938], Loss: 0.0676\n",
      "Epoch [13/20], Batch [800/938], Loss: 0.0304\n",
      "Epoch [13/20], Batch [810/938], Loss: 0.0734\n",
      "Epoch [13/20], Batch [820/938], Loss: 0.1565\n",
      "Epoch [13/20], Batch [830/938], Loss: 0.0376\n",
      "Epoch [13/20], Batch [840/938], Loss: 0.2148\n",
      "Epoch [13/20], Batch [850/938], Loss: 0.0962\n",
      "Epoch [13/20], Batch [860/938], Loss: 0.0412\n",
      "Epoch [13/20], Batch [870/938], Loss: 0.0360\n",
      "Epoch [13/20], Batch [880/938], Loss: 0.0949\n",
      "Epoch [13/20], Batch [890/938], Loss: 0.1009\n",
      "Epoch [13/20], Batch [900/938], Loss: 0.1106\n",
      "Epoch [13/20], Batch [910/938], Loss: 0.1177\n",
      "Epoch [13/20], Batch [920/938], Loss: 0.0701\n",
      "Epoch [13/20], Batch [930/938], Loss: 0.0887\n",
      "Epoch [13/20], Average Loss: 0.0801\n",
      "------------------------------------------------------------\n",
      "Epoch [14/20], Batch [10/938], Loss: 0.0505\n",
      "Epoch [14/20], Batch [20/938], Loss: 0.0696\n",
      "Epoch [14/20], Batch [30/938], Loss: 0.0658\n",
      "Epoch [14/20], Batch [40/938], Loss: 0.0459\n",
      "Epoch [14/20], Batch [50/938], Loss: 0.0558\n",
      "Epoch [14/20], Batch [60/938], Loss: 0.0380\n",
      "Epoch [14/20], Batch [70/938], Loss: 0.0517\n",
      "Epoch [14/20], Batch [80/938], Loss: 0.0322\n",
      "Epoch [14/20], Batch [90/938], Loss: 0.0164\n",
      "Epoch [14/20], Batch [100/938], Loss: 0.0955\n",
      "Epoch [14/20], Batch [110/938], Loss: 0.0237\n",
      "Epoch [14/20], Batch [120/938], Loss: 0.0218\n",
      "Epoch [14/20], Batch [130/938], Loss: 0.0447\n",
      "Epoch [14/20], Batch [140/938], Loss: 0.0620\n",
      "Epoch [14/20], Batch [150/938], Loss: 0.0397\n",
      "Epoch [14/20], Batch [160/938], Loss: 0.1249\n",
      "Epoch [14/20], Batch [170/938], Loss: 0.0494\n",
      "Epoch [14/20], Batch [180/938], Loss: 0.0340\n",
      "Epoch [14/20], Batch [190/938], Loss: 0.1018\n",
      "Epoch [14/20], Batch [200/938], Loss: 0.0622\n",
      "Epoch [14/20], Batch [210/938], Loss: 0.0352\n",
      "Epoch [14/20], Batch [220/938], Loss: 0.0453\n",
      "Epoch [14/20], Batch [230/938], Loss: 0.1222\n",
      "Epoch [14/20], Batch [240/938], Loss: 0.0763\n",
      "Epoch [14/20], Batch [250/938], Loss: 0.1307\n",
      "Epoch [14/20], Batch [260/938], Loss: 0.0637\n",
      "Epoch [14/20], Batch [270/938], Loss: 0.0090\n",
      "Epoch [14/20], Batch [280/938], Loss: 0.0438\n",
      "Epoch [14/20], Batch [290/938], Loss: 0.0571\n",
      "Epoch [14/20], Batch [300/938], Loss: 0.0417\n",
      "Epoch [14/20], Batch [310/938], Loss: 0.0395\n",
      "Epoch [14/20], Batch [320/938], Loss: 0.0993\n",
      "Epoch [14/20], Batch [330/938], Loss: 0.0978\n",
      "Epoch [14/20], Batch [340/938], Loss: 0.0348\n",
      "Epoch [14/20], Batch [350/938], Loss: 0.0838\n",
      "Epoch [14/20], Batch [360/938], Loss: 0.0281\n",
      "Epoch [14/20], Batch [370/938], Loss: 0.0611\n",
      "Epoch [14/20], Batch [380/938], Loss: 0.0452\n",
      "Epoch [14/20], Batch [390/938], Loss: 0.0586\n",
      "Epoch [14/20], Batch [400/938], Loss: 0.1211\n",
      "Epoch [14/20], Batch [410/938], Loss: 0.1098\n",
      "Epoch [14/20], Batch [420/938], Loss: 0.0616\n",
      "Epoch [14/20], Batch [430/938], Loss: 0.0646\n",
      "Epoch [14/20], Batch [440/938], Loss: 0.1033\n",
      "Epoch [14/20], Batch [450/938], Loss: 0.0244\n",
      "Epoch [14/20], Batch [460/938], Loss: 0.0647\n",
      "Epoch [14/20], Batch [470/938], Loss: 0.0185\n",
      "Epoch [14/20], Batch [480/938], Loss: 0.0468\n",
      "Epoch [14/20], Batch [490/938], Loss: 0.0935\n",
      "Epoch [14/20], Batch [500/938], Loss: 0.0269\n",
      "Epoch [14/20], Batch [510/938], Loss: 0.0111\n",
      "Epoch [14/20], Batch [520/938], Loss: 0.0217\n",
      "Epoch [14/20], Batch [530/938], Loss: 0.0445\n",
      "Epoch [14/20], Batch [540/938], Loss: 0.1236\n",
      "Epoch [14/20], Batch [550/938], Loss: 0.0261\n",
      "Epoch [14/20], Batch [560/938], Loss: 0.0791\n",
      "Epoch [14/20], Batch [570/938], Loss: 0.0331\n",
      "Epoch [14/20], Batch [580/938], Loss: 0.0846\n",
      "Epoch [14/20], Batch [590/938], Loss: 0.0787\n",
      "Epoch [14/20], Batch [600/938], Loss: 0.0193\n",
      "Epoch [14/20], Batch [610/938], Loss: 0.0731\n",
      "Epoch [14/20], Batch [620/938], Loss: 0.0198\n",
      "Epoch [14/20], Batch [630/938], Loss: 0.0860\n",
      "Epoch [14/20], Batch [640/938], Loss: 0.0301\n",
      "Epoch [14/20], Batch [650/938], Loss: 0.1654\n",
      "Epoch [14/20], Batch [660/938], Loss: 0.0980\n",
      "Epoch [14/20], Batch [670/938], Loss: 0.0626\n",
      "Epoch [14/20], Batch [680/938], Loss: 0.1674\n",
      "Epoch [14/20], Batch [690/938], Loss: 0.1231\n",
      "Epoch [14/20], Batch [700/938], Loss: 0.0626\n",
      "Epoch [14/20], Batch [710/938], Loss: 0.0791\n",
      "Epoch [14/20], Batch [720/938], Loss: 0.0756\n",
      "Epoch [14/20], Batch [730/938], Loss: 0.0698\n",
      "Epoch [14/20], Batch [740/938], Loss: 0.0708\n",
      "Epoch [14/20], Batch [750/938], Loss: 0.0341\n",
      "Epoch [14/20], Batch [760/938], Loss: 0.2726\n",
      "Epoch [14/20], Batch [770/938], Loss: 0.0757\n",
      "Epoch [14/20], Batch [780/938], Loss: 0.0466\n",
      "Epoch [14/20], Batch [790/938], Loss: 0.0314\n",
      "Epoch [14/20], Batch [800/938], Loss: 0.0186\n",
      "Epoch [14/20], Batch [810/938], Loss: 0.1185\n",
      "Epoch [14/20], Batch [820/938], Loss: 0.0389\n",
      "Epoch [14/20], Batch [830/938], Loss: 0.0334\n",
      "Epoch [14/20], Batch [840/938], Loss: 0.1351\n",
      "Epoch [14/20], Batch [850/938], Loss: 0.0632\n",
      "Epoch [14/20], Batch [860/938], Loss: 0.0294\n",
      "Epoch [14/20], Batch [870/938], Loss: 0.0906\n",
      "Epoch [14/20], Batch [880/938], Loss: 0.1215\n",
      "Epoch [14/20], Batch [890/938], Loss: 0.0520\n",
      "Epoch [14/20], Batch [900/938], Loss: 0.0407\n",
      "Epoch [14/20], Batch [910/938], Loss: 0.0493\n",
      "Epoch [14/20], Batch [920/938], Loss: 0.0244\n",
      "Epoch [14/20], Batch [930/938], Loss: 0.0741\n",
      "Epoch [14/20], Average Loss: 0.0715\n",
      "------------------------------------------------------------\n",
      "Epoch [15/20], Batch [10/938], Loss: 0.0162\n",
      "Epoch [15/20], Batch [20/938], Loss: 0.0860\n",
      "Epoch [15/20], Batch [30/938], Loss: 0.1171\n",
      "Epoch [15/20], Batch [40/938], Loss: 0.0453\n",
      "Epoch [15/20], Batch [50/938], Loss: 0.0300\n",
      "Epoch [15/20], Batch [60/938], Loss: 0.0125\n",
      "Epoch [15/20], Batch [70/938], Loss: 0.0264\n",
      "Epoch [15/20], Batch [80/938], Loss: 0.0177\n",
      "Epoch [15/20], Batch [90/938], Loss: 0.0552\n",
      "Epoch [15/20], Batch [100/938], Loss: 0.0229\n",
      "Epoch [15/20], Batch [110/938], Loss: 0.1018\n",
      "Epoch [15/20], Batch [120/938], Loss: 0.0150\n",
      "Epoch [15/20], Batch [130/938], Loss: 0.0214\n",
      "Epoch [15/20], Batch [140/938], Loss: 0.0419\n",
      "Epoch [15/20], Batch [150/938], Loss: 0.0444\n",
      "Epoch [15/20], Batch [160/938], Loss: 0.0686\n",
      "Epoch [15/20], Batch [170/938], Loss: 0.0210\n",
      "Epoch [15/20], Batch [180/938], Loss: 0.0363\n",
      "Epoch [15/20], Batch [190/938], Loss: 0.0900\n",
      "Epoch [15/20], Batch [200/938], Loss: 0.0557\n",
      "Epoch [15/20], Batch [210/938], Loss: 0.0275\n",
      "Epoch [15/20], Batch [220/938], Loss: 0.0952\n",
      "Epoch [15/20], Batch [230/938], Loss: 0.0313\n",
      "Epoch [15/20], Batch [240/938], Loss: 0.0301\n",
      "Epoch [15/20], Batch [250/938], Loss: 0.1015\n",
      "Epoch [15/20], Batch [260/938], Loss: 0.0759\n",
      "Epoch [15/20], Batch [270/938], Loss: 0.1270\n",
      "Epoch [15/20], Batch [280/938], Loss: 0.0401\n",
      "Epoch [15/20], Batch [290/938], Loss: 0.0251\n",
      "Epoch [15/20], Batch [300/938], Loss: 0.1198\n",
      "Epoch [15/20], Batch [310/938], Loss: 0.1159\n",
      "Epoch [15/20], Batch [320/938], Loss: 0.0961\n",
      "Epoch [15/20], Batch [330/938], Loss: 0.0349\n",
      "Epoch [15/20], Batch [340/938], Loss: 0.1231\n",
      "Epoch [15/20], Batch [350/938], Loss: 0.0926\n",
      "Epoch [15/20], Batch [360/938], Loss: 0.0680\n",
      "Epoch [15/20], Batch [370/938], Loss: 0.0485\n",
      "Epoch [15/20], Batch [380/938], Loss: 0.0576\n",
      "Epoch [15/20], Batch [390/938], Loss: 0.0750\n",
      "Epoch [15/20], Batch [400/938], Loss: 0.0363\n",
      "Epoch [15/20], Batch [410/938], Loss: 0.0166\n",
      "Epoch [15/20], Batch [420/938], Loss: 0.0673\n",
      "Epoch [15/20], Batch [430/938], Loss: 0.0027\n",
      "Epoch [15/20], Batch [440/938], Loss: 0.0135\n",
      "Epoch [15/20], Batch [450/938], Loss: 0.0185\n",
      "Epoch [15/20], Batch [460/938], Loss: 0.0196\n",
      "Epoch [15/20], Batch [470/938], Loss: 0.0930\n",
      "Epoch [15/20], Batch [480/938], Loss: 0.0407\n",
      "Epoch [15/20], Batch [490/938], Loss: 0.1090\n",
      "Epoch [15/20], Batch [500/938], Loss: 0.0372\n",
      "Epoch [15/20], Batch [510/938], Loss: 0.0561\n",
      "Epoch [15/20], Batch [520/938], Loss: 0.0678\n",
      "Epoch [15/20], Batch [530/938], Loss: 0.0313\n",
      "Epoch [15/20], Batch [540/938], Loss: 0.0882\n",
      "Epoch [15/20], Batch [550/938], Loss: 0.0242\n",
      "Epoch [15/20], Batch [560/938], Loss: 0.0595\n",
      "Epoch [15/20], Batch [570/938], Loss: 0.0389\n",
      "Epoch [15/20], Batch [580/938], Loss: 0.0738\n",
      "Epoch [15/20], Batch [590/938], Loss: 0.0816\n",
      "Epoch [15/20], Batch [600/938], Loss: 0.0087\n",
      "Epoch [15/20], Batch [610/938], Loss: 0.1129\n",
      "Epoch [15/20], Batch [620/938], Loss: 0.0829\n",
      "Epoch [15/20], Batch [630/938], Loss: 0.0550\n",
      "Epoch [15/20], Batch [640/938], Loss: 0.1001\n",
      "Epoch [15/20], Batch [650/938], Loss: 0.0565\n",
      "Epoch [15/20], Batch [660/938], Loss: 0.0289\n",
      "Epoch [15/20], Batch [670/938], Loss: 0.0471\n",
      "Epoch [15/20], Batch [680/938], Loss: 0.0357\n",
      "Epoch [15/20], Batch [690/938], Loss: 0.0469\n",
      "Epoch [15/20], Batch [700/938], Loss: 0.0769\n",
      "Epoch [15/20], Batch [710/938], Loss: 0.1403\n",
      "Epoch [15/20], Batch [720/938], Loss: 0.0472\n",
      "Epoch [15/20], Batch [730/938], Loss: 0.0384\n",
      "Epoch [15/20], Batch [740/938], Loss: 0.0406\n",
      "Epoch [15/20], Batch [750/938], Loss: 0.0972\n",
      "Epoch [15/20], Batch [760/938], Loss: 0.0363\n",
      "Epoch [15/20], Batch [770/938], Loss: 0.1222\n",
      "Epoch [15/20], Batch [780/938], Loss: 0.0343\n",
      "Epoch [15/20], Batch [790/938], Loss: 0.0555\n",
      "Epoch [15/20], Batch [800/938], Loss: 0.0150\n",
      "Epoch [15/20], Batch [810/938], Loss: 0.0518\n",
      "Epoch [15/20], Batch [820/938], Loss: 0.1109\n",
      "Epoch [15/20], Batch [830/938], Loss: 0.0648\n",
      "Epoch [15/20], Batch [840/938], Loss: 0.0711\n",
      "Epoch [15/20], Batch [850/938], Loss: 0.1097\n",
      "Epoch [15/20], Batch [860/938], Loss: 0.0886\n",
      "Epoch [15/20], Batch [870/938], Loss: 0.0517\n",
      "Epoch [15/20], Batch [880/938], Loss: 0.1192\n",
      "Epoch [15/20], Batch [890/938], Loss: 0.0340\n",
      "Epoch [15/20], Batch [900/938], Loss: 0.0644\n",
      "Epoch [15/20], Batch [910/938], Loss: 0.0656\n",
      "Epoch [15/20], Batch [920/938], Loss: 0.1323\n",
      "Epoch [15/20], Batch [930/938], Loss: 0.1342\n",
      "Epoch [15/20], Average Loss: 0.0637\n",
      "------------------------------------------------------------\n",
      "Epoch [16/20], Batch [10/938], Loss: 0.0907\n",
      "Epoch [16/20], Batch [20/938], Loss: 0.0484\n",
      "Epoch [16/20], Batch [30/938], Loss: 0.0371\n",
      "Epoch [16/20], Batch [40/938], Loss: 0.0490\n",
      "Epoch [16/20], Batch [50/938], Loss: 0.0347\n",
      "Epoch [16/20], Batch [60/938], Loss: 0.0380\n",
      "Epoch [16/20], Batch [70/938], Loss: 0.0255\n",
      "Epoch [16/20], Batch [80/938], Loss: 0.0153\n",
      "Epoch [16/20], Batch [90/938], Loss: 0.0396\n",
      "Epoch [16/20], Batch [100/938], Loss: 0.0268\n",
      "Epoch [16/20], Batch [110/938], Loss: 0.0132\n",
      "Epoch [16/20], Batch [120/938], Loss: 0.0309\n",
      "Epoch [16/20], Batch [130/938], Loss: 0.0581\n",
      "Epoch [16/20], Batch [140/938], Loss: 0.0327\n",
      "Epoch [16/20], Batch [150/938], Loss: 0.1023\n",
      "Epoch [16/20], Batch [160/938], Loss: 0.0386\n",
      "Epoch [16/20], Batch [170/938], Loss: 0.1898\n",
      "Epoch [16/20], Batch [180/938], Loss: 0.1037\n",
      "Epoch [16/20], Batch [190/938], Loss: 0.0613\n",
      "Epoch [16/20], Batch [200/938], Loss: 0.0320\n",
      "Epoch [16/20], Batch [210/938], Loss: 0.0379\n",
      "Epoch [16/20], Batch [220/938], Loss: 0.0229\n",
      "Epoch [16/20], Batch [230/938], Loss: 0.0543\n",
      "Epoch [16/20], Batch [240/938], Loss: 0.0333\n",
      "Epoch [16/20], Batch [250/938], Loss: 0.0632\n",
      "Epoch [16/20], Batch [260/938], Loss: 0.0877\n",
      "Epoch [16/20], Batch [270/938], Loss: 0.0550\n",
      "Epoch [16/20], Batch [280/938], Loss: 0.0575\n",
      "Epoch [16/20], Batch [290/938], Loss: 0.0854\n",
      "Epoch [16/20], Batch [300/938], Loss: 0.0361\n",
      "Epoch [16/20], Batch [310/938], Loss: 0.0292\n",
      "Epoch [16/20], Batch [320/938], Loss: 0.0530\n",
      "Epoch [16/20], Batch [330/938], Loss: 0.0313\n",
      "Epoch [16/20], Batch [340/938], Loss: 0.0108\n",
      "Epoch [16/20], Batch [350/938], Loss: 0.0250\n",
      "Epoch [16/20], Batch [360/938], Loss: 0.0965\n",
      "Epoch [16/20], Batch [370/938], Loss: 0.0261\n",
      "Epoch [16/20], Batch [380/938], Loss: 0.0865\n",
      "Epoch [16/20], Batch [390/938], Loss: 0.0502\n",
      "Epoch [16/20], Batch [400/938], Loss: 0.1031\n",
      "Epoch [16/20], Batch [410/938], Loss: 0.0116\n",
      "Epoch [16/20], Batch [420/938], Loss: 0.0226\n",
      "Epoch [16/20], Batch [430/938], Loss: 0.0075\n",
      "Epoch [16/20], Batch [440/938], Loss: 0.0412\n",
      "Epoch [16/20], Batch [450/938], Loss: 0.0953\n",
      "Epoch [16/20], Batch [460/938], Loss: 0.0426\n",
      "Epoch [16/20], Batch [470/938], Loss: 0.2015\n",
      "Epoch [16/20], Batch [480/938], Loss: 0.0662\n",
      "Epoch [16/20], Batch [490/938], Loss: 0.0508\n",
      "Epoch [16/20], Batch [500/938], Loss: 0.0370\n",
      "Epoch [16/20], Batch [510/938], Loss: 0.0109\n",
      "Epoch [16/20], Batch [520/938], Loss: 0.1533\n",
      "Epoch [16/20], Batch [530/938], Loss: 0.0752\n",
      "Epoch [16/20], Batch [540/938], Loss: 0.0662\n",
      "Epoch [16/20], Batch [550/938], Loss: 0.0168\n",
      "Epoch [16/20], Batch [560/938], Loss: 0.0266\n",
      "Epoch [16/20], Batch [570/938], Loss: 0.0148\n",
      "Epoch [16/20], Batch [580/938], Loss: 0.0813\n",
      "Epoch [16/20], Batch [590/938], Loss: 0.0483\n",
      "Epoch [16/20], Batch [600/938], Loss: 0.0638\n",
      "Epoch [16/20], Batch [610/938], Loss: 0.0579\n",
      "Epoch [16/20], Batch [620/938], Loss: 0.1059\n",
      "Epoch [16/20], Batch [630/938], Loss: 0.0226\n",
      "Epoch [16/20], Batch [640/938], Loss: 0.0631\n",
      "Epoch [16/20], Batch [650/938], Loss: 0.0278\n",
      "Epoch [16/20], Batch [660/938], Loss: 0.0637\n",
      "Epoch [16/20], Batch [670/938], Loss: 0.1021\n",
      "Epoch [16/20], Batch [680/938], Loss: 0.0117\n",
      "Epoch [16/20], Batch [690/938], Loss: 0.0210\n",
      "Epoch [16/20], Batch [700/938], Loss: 0.0405\n",
      "Epoch [16/20], Batch [710/938], Loss: 0.0963\n",
      "Epoch [16/20], Batch [720/938], Loss: 0.0352\n",
      "Epoch [16/20], Batch [730/938], Loss: 0.0215\n",
      "Epoch [16/20], Batch [740/938], Loss: 0.0430\n",
      "Epoch [16/20], Batch [750/938], Loss: 0.0315\n",
      "Epoch [16/20], Batch [760/938], Loss: 0.0501\n",
      "Epoch [16/20], Batch [770/938], Loss: 0.0526\n",
      "Epoch [16/20], Batch [780/938], Loss: 0.0542\n",
      "Epoch [16/20], Batch [790/938], Loss: 0.0790\n",
      "Epoch [16/20], Batch [800/938], Loss: 0.0292\n",
      "Epoch [16/20], Batch [810/938], Loss: 0.0465\n",
      "Epoch [16/20], Batch [820/938], Loss: 0.0324\n",
      "Epoch [16/20], Batch [830/938], Loss: 0.2076\n",
      "Epoch [16/20], Batch [840/938], Loss: 0.1467\n",
      "Epoch [16/20], Batch [850/938], Loss: 0.0464\n",
      "Epoch [16/20], Batch [860/938], Loss: 0.0526\n",
      "Epoch [16/20], Batch [870/938], Loss: 0.0164\n",
      "Epoch [16/20], Batch [880/938], Loss: 0.1064\n",
      "Epoch [16/20], Batch [890/938], Loss: 0.0450\n",
      "Epoch [16/20], Batch [900/938], Loss: 0.0564\n",
      "Epoch [16/20], Batch [910/938], Loss: 0.0096\n",
      "Epoch [16/20], Batch [920/938], Loss: 0.0236\n",
      "Epoch [16/20], Batch [930/938], Loss: 0.0612\n",
      "Epoch [16/20], Average Loss: 0.0541\n",
      "------------------------------------------------------------\n",
      "Epoch [17/20], Batch [10/938], Loss: 0.1463\n",
      "Epoch [17/20], Batch [20/938], Loss: 0.0573\n",
      "Epoch [17/20], Batch [30/938], Loss: 0.0201\n",
      "Epoch [17/20], Batch [40/938], Loss: 0.1299\n",
      "Epoch [17/20], Batch [50/938], Loss: 0.0209\n",
      "Epoch [17/20], Batch [60/938], Loss: 0.0078\n",
      "Epoch [17/20], Batch [70/938], Loss: 0.0139\n",
      "Epoch [17/20], Batch [80/938], Loss: 0.1013\n",
      "Epoch [17/20], Batch [90/938], Loss: 0.0412\n",
      "Epoch [17/20], Batch [100/938], Loss: 0.0449\n",
      "Epoch [17/20], Batch [110/938], Loss: 0.0796\n",
      "Epoch [17/20], Batch [120/938], Loss: 0.0457\n",
      "Epoch [17/20], Batch [130/938], Loss: 0.0397\n",
      "Epoch [17/20], Batch [140/938], Loss: 0.0278\n",
      "Epoch [17/20], Batch [150/938], Loss: 0.0701\n",
      "Epoch [17/20], Batch [160/938], Loss: 0.0475\n",
      "Epoch [17/20], Batch [170/938], Loss: 0.0222\n",
      "Epoch [17/20], Batch [180/938], Loss: 0.0334\n",
      "Epoch [17/20], Batch [190/938], Loss: 0.0386\n",
      "Epoch [17/20], Batch [200/938], Loss: 0.0378\n",
      "Epoch [17/20], Batch [210/938], Loss: 0.0057\n",
      "Epoch [17/20], Batch [220/938], Loss: 0.0760\n",
      "Epoch [17/20], Batch [230/938], Loss: 0.0095\n",
      "Epoch [17/20], Batch [240/938], Loss: 0.0350\n",
      "Epoch [17/20], Batch [250/938], Loss: 0.0187\n",
      "Epoch [17/20], Batch [260/938], Loss: 0.0276\n",
      "Epoch [17/20], Batch [270/938], Loss: 0.0175\n",
      "Epoch [17/20], Batch [280/938], Loss: 0.0343\n",
      "Epoch [17/20], Batch [290/938], Loss: 0.0221\n",
      "Epoch [17/20], Batch [300/938], Loss: 0.1442\n",
      "Epoch [17/20], Batch [310/938], Loss: 0.0372\n",
      "Epoch [17/20], Batch [320/938], Loss: 0.0326\n",
      "Epoch [17/20], Batch [330/938], Loss: 0.0351\n",
      "Epoch [17/20], Batch [340/938], Loss: 0.0468\n",
      "Epoch [17/20], Batch [350/938], Loss: 0.0267\n",
      "Epoch [17/20], Batch [360/938], Loss: 0.0688\n",
      "Epoch [17/20], Batch [370/938], Loss: 0.1092\n",
      "Epoch [17/20], Batch [380/938], Loss: 0.0307\n",
      "Epoch [17/20], Batch [390/938], Loss: 0.0259\n",
      "Epoch [17/20], Batch [400/938], Loss: 0.0427\n",
      "Epoch [17/20], Batch [410/938], Loss: 0.0933\n",
      "Epoch [17/20], Batch [420/938], Loss: 0.0729\n",
      "Epoch [17/20], Batch [430/938], Loss: 0.1176\n",
      "Epoch [17/20], Batch [440/938], Loss: 0.0435\n",
      "Epoch [17/20], Batch [450/938], Loss: 0.0653\n",
      "Epoch [17/20], Batch [460/938], Loss: 0.0268\n",
      "Epoch [17/20], Batch [470/938], Loss: 0.0818\n",
      "Epoch [17/20], Batch [480/938], Loss: 0.0888\n",
      "Epoch [17/20], Batch [490/938], Loss: 0.0082\n",
      "Epoch [17/20], Batch [500/938], Loss: 0.0535\n",
      "Epoch [17/20], Batch [510/938], Loss: 0.0617\n",
      "Epoch [17/20], Batch [520/938], Loss: 0.0617\n",
      "Epoch [17/20], Batch [530/938], Loss: 0.0154\n",
      "Epoch [17/20], Batch [540/938], Loss: 0.0476\n",
      "Epoch [17/20], Batch [550/938], Loss: 0.0856\n",
      "Epoch [17/20], Batch [560/938], Loss: 0.0036\n",
      "Epoch [17/20], Batch [570/938], Loss: 0.1324\n",
      "Epoch [17/20], Batch [580/938], Loss: 0.0048\n",
      "Epoch [17/20], Batch [590/938], Loss: 0.0690\n",
      "Epoch [17/20], Batch [600/938], Loss: 0.0909\n",
      "Epoch [17/20], Batch [610/938], Loss: 0.0337\n",
      "Epoch [17/20], Batch [620/938], Loss: 0.0433\n",
      "Epoch [17/20], Batch [630/938], Loss: 0.0777\n",
      "Epoch [17/20], Batch [640/938], Loss: 0.0816\n",
      "Epoch [17/20], Batch [650/938], Loss: 0.0323\n",
      "Epoch [17/20], Batch [660/938], Loss: 0.0396\n",
      "Epoch [17/20], Batch [670/938], Loss: 0.0186\n",
      "Epoch [17/20], Batch [680/938], Loss: 0.0353\n",
      "Epoch [17/20], Batch [690/938], Loss: 0.0325\n",
      "Epoch [17/20], Batch [700/938], Loss: 0.0207\n",
      "Epoch [17/20], Batch [710/938], Loss: 0.0573\n",
      "Epoch [17/20], Batch [720/938], Loss: 0.2647\n",
      "Epoch [17/20], Batch [730/938], Loss: 0.0408\n",
      "Epoch [17/20], Batch [740/938], Loss: 0.1238\n",
      "Epoch [17/20], Batch [750/938], Loss: 0.0405\n",
      "Epoch [17/20], Batch [760/938], Loss: 0.0763\n",
      "Epoch [17/20], Batch [770/938], Loss: 0.0459\n",
      "Epoch [17/20], Batch [780/938], Loss: 0.0135\n",
      "Epoch [17/20], Batch [790/938], Loss: 0.0774\n",
      "Epoch [17/20], Batch [800/938], Loss: 0.0941\n",
      "Epoch [17/20], Batch [810/938], Loss: 0.0418\n",
      "Epoch [17/20], Batch [820/938], Loss: 0.0424\n",
      "Epoch [17/20], Batch [830/938], Loss: 0.0532\n",
      "Epoch [17/20], Batch [840/938], Loss: 0.0688\n",
      "Epoch [17/20], Batch [850/938], Loss: 0.0408\n",
      "Epoch [17/20], Batch [860/938], Loss: 0.1280\n",
      "Epoch [17/20], Batch [870/938], Loss: 0.1011\n",
      "Epoch [17/20], Batch [880/938], Loss: 0.1600\n",
      "Epoch [17/20], Batch [890/938], Loss: 0.0632\n",
      "Epoch [17/20], Batch [900/938], Loss: 0.0108\n",
      "Epoch [17/20], Batch [910/938], Loss: 0.1012\n",
      "Epoch [17/20], Batch [920/938], Loss: 0.0170\n",
      "Epoch [17/20], Batch [930/938], Loss: 0.0162\n",
      "Epoch [17/20], Average Loss: 0.0486\n",
      "------------------------------------------------------------\n",
      "Epoch [18/20], Batch [10/938], Loss: 0.0423\n",
      "Epoch [18/20], Batch [20/938], Loss: 0.0434\n",
      "Epoch [18/20], Batch [30/938], Loss: 0.0287\n",
      "Epoch [18/20], Batch [40/938], Loss: 0.0176\n",
      "Epoch [18/20], Batch [50/938], Loss: 0.0820\n",
      "Epoch [18/20], Batch [60/938], Loss: 0.0446\n",
      "Epoch [18/20], Batch [70/938], Loss: 0.0353\n",
      "Epoch [18/20], Batch [80/938], Loss: 0.0320\n",
      "Epoch [18/20], Batch [90/938], Loss: 0.0259\n",
      "Epoch [18/20], Batch [100/938], Loss: 0.0271\n",
      "Epoch [18/20], Batch [110/938], Loss: 0.0844\n",
      "Epoch [18/20], Batch [120/938], Loss: 0.0381\n",
      "Epoch [18/20], Batch [130/938], Loss: 0.0470\n",
      "Epoch [18/20], Batch [140/938], Loss: 0.0334\n",
      "Epoch [18/20], Batch [150/938], Loss: 0.0107\n",
      "Epoch [18/20], Batch [160/938], Loss: 0.0308\n",
      "Epoch [18/20], Batch [170/938], Loss: 0.0051\n",
      "Epoch [18/20], Batch [180/938], Loss: 0.0282\n",
      "Epoch [18/20], Batch [190/938], Loss: 0.0174\n",
      "Epoch [18/20], Batch [200/938], Loss: 0.0563\n",
      "Epoch [18/20], Batch [210/938], Loss: 0.0469\n",
      "Epoch [18/20], Batch [220/938], Loss: 0.0116\n",
      "Epoch [18/20], Batch [230/938], Loss: 0.0163\n",
      "Epoch [18/20], Batch [240/938], Loss: 0.0506\n",
      "Epoch [18/20], Batch [250/938], Loss: 0.0356\n",
      "Epoch [18/20], Batch [260/938], Loss: 0.0718\n",
      "Epoch [18/20], Batch [270/938], Loss: 0.0514\n",
      "Epoch [18/20], Batch [280/938], Loss: 0.0359\n",
      "Epoch [18/20], Batch [290/938], Loss: 0.0273\n",
      "Epoch [18/20], Batch [300/938], Loss: 0.0282\n",
      "Epoch [18/20], Batch [310/938], Loss: 0.0489\n",
      "Epoch [18/20], Batch [320/938], Loss: 0.0155\n",
      "Epoch [18/20], Batch [330/938], Loss: 0.0290\n",
      "Epoch [18/20], Batch [340/938], Loss: 0.0072\n",
      "Epoch [18/20], Batch [350/938], Loss: 0.0880\n",
      "Epoch [18/20], Batch [360/938], Loss: 0.0563\n",
      "Epoch [18/20], Batch [370/938], Loss: 0.0454\n",
      "Epoch [18/20], Batch [380/938], Loss: 0.0606\n",
      "Epoch [18/20], Batch [390/938], Loss: 0.0105\n",
      "Epoch [18/20], Batch [400/938], Loss: 0.0277\n",
      "Epoch [18/20], Batch [410/938], Loss: 0.0627\n",
      "Epoch [18/20], Batch [420/938], Loss: 0.0191\n",
      "Epoch [18/20], Batch [430/938], Loss: 0.1054\n",
      "Epoch [18/20], Batch [440/938], Loss: 0.0224\n",
      "Epoch [18/20], Batch [450/938], Loss: 0.0253\n",
      "Epoch [18/20], Batch [460/938], Loss: 0.0152\n",
      "Epoch [18/20], Batch [470/938], Loss: 0.0079\n",
      "Epoch [18/20], Batch [480/938], Loss: 0.0581\n",
      "Epoch [18/20], Batch [490/938], Loss: 0.0480\n",
      "Epoch [18/20], Batch [500/938], Loss: 0.0199\n",
      "Epoch [18/20], Batch [510/938], Loss: 0.0594\n",
      "Epoch [18/20], Batch [520/938], Loss: 0.0870\n",
      "Epoch [18/20], Batch [530/938], Loss: 0.0126\n",
      "Epoch [18/20], Batch [540/938], Loss: 0.0379\n",
      "Epoch [18/20], Batch [550/938], Loss: 0.0363\n",
      "Epoch [18/20], Batch [560/938], Loss: 0.0206\n",
      "Epoch [18/20], Batch [570/938], Loss: 0.0329\n",
      "Epoch [18/20], Batch [580/938], Loss: 0.0704\n",
      "Epoch [18/20], Batch [590/938], Loss: 0.0267\n",
      "Epoch [18/20], Batch [600/938], Loss: 0.0849\n",
      "Epoch [18/20], Batch [610/938], Loss: 0.0512\n",
      "Epoch [18/20], Batch [620/938], Loss: 0.0263\n",
      "Epoch [18/20], Batch [630/938], Loss: 0.0651\n",
      "Epoch [18/20], Batch [640/938], Loss: 0.0324\n",
      "Epoch [18/20], Batch [650/938], Loss: 0.0261\n",
      "Epoch [18/20], Batch [660/938], Loss: 0.0422\n",
      "Epoch [18/20], Batch [670/938], Loss: 0.0991\n",
      "Epoch [18/20], Batch [680/938], Loss: 0.0227\n",
      "Epoch [18/20], Batch [690/938], Loss: 0.0117\n",
      "Epoch [18/20], Batch [700/938], Loss: 0.0280\n",
      "Epoch [18/20], Batch [710/938], Loss: 0.0289\n",
      "Epoch [18/20], Batch [720/938], Loss: 0.0372\n",
      "Epoch [18/20], Batch [730/938], Loss: 0.0088\n",
      "Epoch [18/20], Batch [740/938], Loss: 0.0250\n",
      "Epoch [18/20], Batch [750/938], Loss: 0.0102\n",
      "Epoch [18/20], Batch [760/938], Loss: 0.0651\n",
      "Epoch [18/20], Batch [770/938], Loss: 0.0826\n",
      "Epoch [18/20], Batch [780/938], Loss: 0.0239\n",
      "Epoch [18/20], Batch [790/938], Loss: 0.0339\n",
      "Epoch [18/20], Batch [800/938], Loss: 0.0123\n",
      "Epoch [18/20], Batch [810/938], Loss: 0.0264\n",
      "Epoch [18/20], Batch [820/938], Loss: 0.0132\n",
      "Epoch [18/20], Batch [830/938], Loss: 0.0053\n",
      "Epoch [18/20], Batch [840/938], Loss: 0.0971\n",
      "Epoch [18/20], Batch [850/938], Loss: 0.0337\n",
      "Epoch [18/20], Batch [860/938], Loss: 0.0490\n",
      "Epoch [18/20], Batch [870/938], Loss: 0.0189\n",
      "Epoch [18/20], Batch [880/938], Loss: 0.0370\n",
      "Epoch [18/20], Batch [890/938], Loss: 0.0432\n",
      "Epoch [18/20], Batch [900/938], Loss: 0.0717\n",
      "Epoch [18/20], Batch [910/938], Loss: 0.0544\n",
      "Epoch [18/20], Batch [920/938], Loss: 0.1897\n",
      "Epoch [18/20], Batch [930/938], Loss: 0.0184\n",
      "Epoch [18/20], Average Loss: 0.0439\n",
      "------------------------------------------------------------\n",
      "Epoch [19/20], Batch [10/938], Loss: 0.0120\n",
      "Epoch [19/20], Batch [20/938], Loss: 0.0236\n",
      "Epoch [19/20], Batch [30/938], Loss: 0.0837\n",
      "Epoch [19/20], Batch [40/938], Loss: 0.0104\n",
      "Epoch [19/20], Batch [50/938], Loss: 0.0278\n",
      "Epoch [19/20], Batch [60/938], Loss: 0.0945\n",
      "Epoch [19/20], Batch [70/938], Loss: 0.0042\n",
      "Epoch [19/20], Batch [80/938], Loss: 0.0296\n",
      "Epoch [19/20], Batch [90/938], Loss: 0.0429\n",
      "Epoch [19/20], Batch [100/938], Loss: 0.0313\n",
      "Epoch [19/20], Batch [110/938], Loss: 0.0149\n",
      "Epoch [19/20], Batch [120/938], Loss: 0.0363\n",
      "Epoch [19/20], Batch [130/938], Loss: 0.0212\n",
      "Epoch [19/20], Batch [140/938], Loss: 0.0220\n",
      "Epoch [19/20], Batch [150/938], Loss: 0.0423\n",
      "Epoch [19/20], Batch [160/938], Loss: 0.0486\n",
      "Epoch [19/20], Batch [170/938], Loss: 0.0357\n",
      "Epoch [19/20], Batch [180/938], Loss: 0.0450\n",
      "Epoch [19/20], Batch [190/938], Loss: 0.0180\n",
      "Epoch [19/20], Batch [200/938], Loss: 0.0051\n",
      "Epoch [19/20], Batch [210/938], Loss: 0.0352\n",
      "Epoch [19/20], Batch [220/938], Loss: 0.0211\n",
      "Epoch [19/20], Batch [230/938], Loss: 0.0151\n",
      "Epoch [19/20], Batch [240/938], Loss: 0.0359\n",
      "Epoch [19/20], Batch [250/938], Loss: 0.0121\n",
      "Epoch [19/20], Batch [260/938], Loss: 0.0013\n",
      "Epoch [19/20], Batch [270/938], Loss: 0.0082\n",
      "Epoch [19/20], Batch [280/938], Loss: 0.0865\n",
      "Epoch [19/20], Batch [290/938], Loss: 0.1095\n",
      "Epoch [19/20], Batch [300/938], Loss: 0.1211\n",
      "Epoch [19/20], Batch [310/938], Loss: 0.0744\n",
      "Epoch [19/20], Batch [320/938], Loss: 0.0503\n",
      "Epoch [19/20], Batch [330/938], Loss: 0.0583\n",
      "Epoch [19/20], Batch [340/938], Loss: 0.0346\n",
      "Epoch [19/20], Batch [350/938], Loss: 0.1161\n",
      "Epoch [19/20], Batch [360/938], Loss: 0.0226\n",
      "Epoch [19/20], Batch [370/938], Loss: 0.0173\n",
      "Epoch [19/20], Batch [380/938], Loss: 0.0191\n",
      "Epoch [19/20], Batch [390/938], Loss: 0.0360\n",
      "Epoch [19/20], Batch [400/938], Loss: 0.0192\n",
      "Epoch [19/20], Batch [410/938], Loss: 0.2941\n",
      "Epoch [19/20], Batch [420/938], Loss: 0.0211\n",
      "Epoch [19/20], Batch [430/938], Loss: 0.0445\n",
      "Epoch [19/20], Batch [440/938], Loss: 0.1180\n",
      "Epoch [19/20], Batch [450/938], Loss: 0.0693\n",
      "Epoch [19/20], Batch [460/938], Loss: 0.0477\n",
      "Epoch [19/20], Batch [470/938], Loss: 0.0039\n",
      "Epoch [19/20], Batch [480/938], Loss: 0.0175\n",
      "Epoch [19/20], Batch [490/938], Loss: 0.0272\n",
      "Epoch [19/20], Batch [500/938], Loss: 0.0174\n",
      "Epoch [19/20], Batch [510/938], Loss: 0.0148\n",
      "Epoch [19/20], Batch [520/938], Loss: 0.0121\n",
      "Epoch [19/20], Batch [530/938], Loss: 0.1025\n",
      "Epoch [19/20], Batch [540/938], Loss: 0.0196\n",
      "Epoch [19/20], Batch [550/938], Loss: 0.0128\n",
      "Epoch [19/20], Batch [560/938], Loss: 0.0241\n",
      "Epoch [19/20], Batch [570/938], Loss: 0.0377\n",
      "Epoch [19/20], Batch [580/938], Loss: 0.0501\n",
      "Epoch [19/20], Batch [590/938], Loss: 0.0116\n",
      "Epoch [19/20], Batch [600/938], Loss: 0.0162\n",
      "Epoch [19/20], Batch [610/938], Loss: 0.0163\n",
      "Epoch [19/20], Batch [620/938], Loss: 0.0254\n",
      "Epoch [19/20], Batch [630/938], Loss: 0.0100\n",
      "Epoch [19/20], Batch [640/938], Loss: 0.0022\n",
      "Epoch [19/20], Batch [650/938], Loss: 0.0345\n",
      "Epoch [19/20], Batch [660/938], Loss: 0.0120\n",
      "Epoch [19/20], Batch [670/938], Loss: 0.0344\n",
      "Epoch [19/20], Batch [680/938], Loss: 0.0731\n",
      "Epoch [19/20], Batch [690/938], Loss: 0.0329\n",
      "Epoch [19/20], Batch [700/938], Loss: 0.0188\n",
      "Epoch [19/20], Batch [710/938], Loss: 0.0186\n",
      "Epoch [19/20], Batch [720/938], Loss: 0.0298\n",
      "Epoch [19/20], Batch [730/938], Loss: 0.0403\n",
      "Epoch [19/20], Batch [740/938], Loss: 0.0441\n",
      "Epoch [19/20], Batch [750/938], Loss: 0.0722\n",
      "Epoch [19/20], Batch [760/938], Loss: 0.0930\n",
      "Epoch [19/20], Batch [770/938], Loss: 0.0320\n",
      "Epoch [19/20], Batch [780/938], Loss: 0.0087\n",
      "Epoch [19/20], Batch [790/938], Loss: 0.0547\n",
      "Epoch [19/20], Batch [800/938], Loss: 0.0229\n",
      "Epoch [19/20], Batch [810/938], Loss: 0.0359\n",
      "Epoch [19/20], Batch [820/938], Loss: 0.0239\n",
      "Epoch [19/20], Batch [830/938], Loss: 0.0296\n",
      "Epoch [19/20], Batch [840/938], Loss: 0.0379\n",
      "Epoch [19/20], Batch [850/938], Loss: 0.1871\n",
      "Epoch [19/20], Batch [860/938], Loss: 0.0378\n",
      "Epoch [19/20], Batch [870/938], Loss: 0.0135\n",
      "Epoch [19/20], Batch [880/938], Loss: 0.0583\n",
      "Epoch [19/20], Batch [890/938], Loss: 0.0390\n",
      "Epoch [19/20], Batch [900/938], Loss: 0.0805\n",
      "Epoch [19/20], Batch [910/938], Loss: 0.0131\n",
      "Epoch [19/20], Batch [920/938], Loss: 0.0102\n",
      "Epoch [19/20], Batch [930/938], Loss: 0.0234\n",
      "Epoch [19/20], Average Loss: 0.0388\n",
      "------------------------------------------------------------\n",
      "Epoch [20/20], Batch [10/938], Loss: 0.0625\n",
      "Epoch [20/20], Batch [20/938], Loss: 0.0310\n",
      "Epoch [20/20], Batch [30/938], Loss: 0.0228\n",
      "Epoch [20/20], Batch [40/938], Loss: 0.0116\n",
      "Epoch [20/20], Batch [50/938], Loss: 0.0299\n",
      "Epoch [20/20], Batch [60/938], Loss: 0.0097\n",
      "Epoch [20/20], Batch [70/938], Loss: 0.0261\n",
      "Epoch [20/20], Batch [80/938], Loss: 0.0066\n",
      "Epoch [20/20], Batch [90/938], Loss: 0.0084\n",
      "Epoch [20/20], Batch [100/938], Loss: 0.0726\n",
      "Epoch [20/20], Batch [110/938], Loss: 0.0226\n",
      "Epoch [20/20], Batch [120/938], Loss: 0.0097\n",
      "Epoch [20/20], Batch [130/938], Loss: 0.0042\n",
      "Epoch [20/20], Batch [140/938], Loss: 0.0051\n",
      "Epoch [20/20], Batch [150/938], Loss: 0.0394\n",
      "Epoch [20/20], Batch [160/938], Loss: 0.0713\n",
      "Epoch [20/20], Batch [170/938], Loss: 0.0275\n",
      "Epoch [20/20], Batch [180/938], Loss: 0.0598\n",
      "Epoch [20/20], Batch [190/938], Loss: 0.0367\n",
      "Epoch [20/20], Batch [200/938], Loss: 0.0280\n",
      "Epoch [20/20], Batch [210/938], Loss: 0.0833\n",
      "Epoch [20/20], Batch [220/938], Loss: 0.0167\n",
      "Epoch [20/20], Batch [230/938], Loss: 0.0266\n",
      "Epoch [20/20], Batch [240/938], Loss: 0.0646\n",
      "Epoch [20/20], Batch [250/938], Loss: 0.0427\n",
      "Epoch [20/20], Batch [260/938], Loss: 0.0503\n",
      "Epoch [20/20], Batch [270/938], Loss: 0.0289\n",
      "Epoch [20/20], Batch [280/938], Loss: 0.0270\n",
      "Epoch [20/20], Batch [290/938], Loss: 0.0037\n",
      "Epoch [20/20], Batch [300/938], Loss: 0.0579\n",
      "Epoch [20/20], Batch [310/938], Loss: 0.0118\n",
      "Epoch [20/20], Batch [320/938], Loss: 0.0173\n",
      "Epoch [20/20], Batch [330/938], Loss: 0.0610\n",
      "Epoch [20/20], Batch [340/938], Loss: 0.0336\n",
      "Epoch [20/20], Batch [350/938], Loss: 0.0086\n",
      "Epoch [20/20], Batch [360/938], Loss: 0.0570\n",
      "Epoch [20/20], Batch [370/938], Loss: 0.0124\n",
      "Epoch [20/20], Batch [380/938], Loss: 0.0040\n",
      "Epoch [20/20], Batch [390/938], Loss: 0.0230\n",
      "Epoch [20/20], Batch [400/938], Loss: 0.0120\n",
      "Epoch [20/20], Batch [410/938], Loss: 0.0105\n",
      "Epoch [20/20], Batch [420/938], Loss: 0.0877\n",
      "Epoch [20/20], Batch [430/938], Loss: 0.0325\n",
      "Epoch [20/20], Batch [440/938], Loss: 0.0254\n",
      "Epoch [20/20], Batch [450/938], Loss: 0.0743\n",
      "Epoch [20/20], Batch [460/938], Loss: 0.0019\n",
      "Epoch [20/20], Batch [470/938], Loss: 0.0358\n",
      "Epoch [20/20], Batch [480/938], Loss: 0.0125\n",
      "Epoch [20/20], Batch [490/938], Loss: 0.0239\n",
      "Epoch [20/20], Batch [500/938], Loss: 0.0578\n",
      "Epoch [20/20], Batch [510/938], Loss: 0.0032\n",
      "Epoch [20/20], Batch [520/938], Loss: 0.0625\n",
      "Epoch [20/20], Batch [530/938], Loss: 0.0179\n",
      "Epoch [20/20], Batch [540/938], Loss: 0.0309\n",
      "Epoch [20/20], Batch [550/938], Loss: 0.0322\n",
      "Epoch [20/20], Batch [560/938], Loss: 0.0583\n",
      "Epoch [20/20], Batch [570/938], Loss: 0.0904\n",
      "Epoch [20/20], Batch [580/938], Loss: 0.0589\n",
      "Epoch [20/20], Batch [590/938], Loss: 0.0377\n",
      "Epoch [20/20], Batch [600/938], Loss: 0.0280\n",
      "Epoch [20/20], Batch [610/938], Loss: 0.0324\n",
      "Epoch [20/20], Batch [620/938], Loss: 0.0228\n",
      "Epoch [20/20], Batch [630/938], Loss: 0.0360\n",
      "Epoch [20/20], Batch [640/938], Loss: 0.0093\n",
      "Epoch [20/20], Batch [650/938], Loss: 0.0398\n",
      "Epoch [20/20], Batch [660/938], Loss: 0.1180\n",
      "Epoch [20/20], Batch [670/938], Loss: 0.0393\n",
      "Epoch [20/20], Batch [680/938], Loss: 0.0551\n",
      "Epoch [20/20], Batch [690/938], Loss: 0.0337\n",
      "Epoch [20/20], Batch [700/938], Loss: 0.0319\n",
      "Epoch [20/20], Batch [710/938], Loss: 0.0564\n",
      "Epoch [20/20], Batch [720/938], Loss: 0.0197\n",
      "Epoch [20/20], Batch [730/938], Loss: 0.0220\n",
      "Epoch [20/20], Batch [740/938], Loss: 0.0293\n",
      "Epoch [20/20], Batch [750/938], Loss: 0.1535\n",
      "Epoch [20/20], Batch [760/938], Loss: 0.1353\n",
      "Epoch [20/20], Batch [770/938], Loss: 0.0508\n",
      "Epoch [20/20], Batch [780/938], Loss: 0.0622\n",
      "Epoch [20/20], Batch [790/938], Loss: 0.0290\n",
      "Epoch [20/20], Batch [800/938], Loss: 0.0219\n",
      "Epoch [20/20], Batch [810/938], Loss: 0.0265\n",
      "Epoch [20/20], Batch [820/938], Loss: 0.0371\n",
      "Epoch [20/20], Batch [830/938], Loss: 0.0434\n",
      "Epoch [20/20], Batch [840/938], Loss: 0.0093\n",
      "Epoch [20/20], Batch [850/938], Loss: 0.0846\n",
      "Epoch [20/20], Batch [860/938], Loss: 0.0114\n",
      "Epoch [20/20], Batch [870/938], Loss: 0.0099\n",
      "Epoch [20/20], Batch [880/938], Loss: 0.0229\n",
      "Epoch [20/20], Batch [890/938], Loss: 0.1337\n",
      "Epoch [20/20], Batch [900/938], Loss: 0.0164\n",
      "Epoch [20/20], Batch [910/938], Loss: 0.0062\n",
      "Epoch [20/20], Batch [920/938], Loss: 0.0143\n",
      "Epoch [20/20], Batch [930/938], Loss: 0.0312\n",
      "Epoch [20/20], Average Loss: 0.0330\n",
      "------------------------------------------------------------\n",
      "Training finished!\n",
      "Test Accuracy: 91.98%\n",
      "Evaluation finished!\n",
      "오차 행렬 (Confusion Matrix):\n",
      "[[848   1  14  11   7   2 112   0   5   0]\n",
      " [  2 980   3  10   2   0   2   0   1   0]\n",
      " [ 15   1 836   6  85   0  57   0   0   0]\n",
      " [ 13   4  12 922  19   0  29   0   1   0]\n",
      " [  0   0  24  19 914   1  41   0   1   0]\n",
      " [  0   0   0   0   0 987   0  11   0   2]\n",
      " [ 79   0  35  18  72   0 793   0   3   0]\n",
      " [  0   0   0   0   0  12   0 976   0  12]\n",
      " [  5   0   0   2   3   4   8   1 977   0]\n",
      " [  1   0   0   0   0   7   0  27   0 965]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 문제 정의 및 데이터 준비\n",
    "# 문제: Fashion-MNIST 데이터셋을 사용하여 10가지 종류의 의류 이미지를 정확하게 분류하는 이미지 분류(Image Classification) 모델을 만듭니다.\n",
    "# 데이터: 캐글(Kaggle)에서 다운로드할 수 있는 Fashion-MNIST 데이터셋을 사용합니다. 이 데이터셋은 훈련용 이미지 60,000개와 테스트용 이미지 10,000개로 구성되어 있으며, 각 이미지는 28×28 픽셀의 흑백 이미지입니다.\n",
    "\n",
    "# 2. 데이터 로딩 및 전처리\n",
    "# 2.1. 데이터 로드\n",
    "# 바이너리 파일 형태로 저장되어 gzip, numpy 라이브러리를 사용.\n",
    "import numpy as np\n",
    "# import gzip # 이미 압축해제 되어 있어서 필요 없음\n",
    "train_images_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/kaggle-projects/fashion-mnist/data/train-images-idx3-ubyte'\n",
    "train_labels_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/kaggle-projects/fashion-mnist/data/train-labels-idx1-ubyte'\n",
    "test_images_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/kaggle-projects/fashion-mnist/data/t10k-images-idx3-ubyte'\n",
    "test_labels_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/kaggle-projects/fashion-mnist/data/t10k-labels-idx1-ubyte'\n",
    "# offset은 파일의 특정 위치부터 데이터를 읽기 시작하라고 알려주는 기능입니다. Fashion-MNIST 데이터셋의 바이너리 파일은 이미지 데이터나 라벨 데이터가 시작되기 전에, 데이터에 대한 메타정보가 담긴 **헤더(header)**를 가지고 있습니다. 이 헤더 정보를 건너뛰고 순수한 데이터만 읽기 위해 offset을 사용하는 것이죠.\n",
    "# 이미지 파일 로드 함수 (압축 해제된 파일용)\n",
    "def load_mnist_images(path:str):\n",
    "    with open(path, 'rb') as f:\n",
    "        # 이미지 파일 헤더(16바이트)를 건너뛰고 나머지 데이터를 읽음\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)\n",
    "    # 28x28 이미지 크기로 재구성\n",
    "    return data.reshape(-1, 28, 28)\n",
    "# 라벨 파일 로드 함수 (압축 해제된 파일용)\n",
    "def load_mnist_labels(path:str):\n",
    "    with open(path, 'rb') as f:\n",
    "        # 라벨 파일 헤더(8바이트)를 건너뛰고 나머지 데이터를 읽음\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "# 함수를 사용해 데이터 로드\n",
    "train_images = load_mnist_images(train_images_path)\n",
    "train_labels = load_mnist_labels(train_labels_path)\n",
    "test_images = load_mnist_images(test_images_path)\n",
    "test_labels = load_mnist_labels(test_labels_path)\n",
    "# print(f\"훈련용 이미지 데이터의 형태: {train_images.shape}\")\n",
    "# print(f\"훈련용 라벨 데이터의 형태: {train_labels.shape}\")\n",
    "# print(f\"테스트용 이미지 데이터의 형태: {test_images.shape}\")\n",
    "# print(f\"테스트용 라벨 데이터의 형태: {test_labels.shape}\")\n",
    "\n",
    "# 3. 데이터 전처리 및 시각화\n",
    "# 3.1. 이미지 데이터 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "# # 3.1.1. train_images에 저장된 첫 번째 이미지 데이터를 선택\n",
    "# image_to_visualize = train_images[0]\n",
    "# # 3.1.2. 이미지 시각화를 위해 plt.imshow() 함수를 사용합니다.\n",
    "# # imshow() 함수는 배열 형태의 데이터를 이미지로 보여주는 역할을 해요.\n",
    "# # cmap=plt.cm.binary 옵션은 이미지를 흑백으로 나타내라는 의미입니다.\n",
    "# plt.imshow(image_to_visualize, cmap=plt.cm.binary) # pyright: ignore[reportAttributeAccessIssue]\n",
    "# # 3.1.3. plt.colorbar() 함수를 사용해 이미지의 픽셀 값 분포를 나타내는 컬러바를 추가합니다.\n",
    "# # 픽셀 값이 0(검은색)부터 255(흰색)까지 어떻게 분포하는지 한눈에 확인 가능.\n",
    "# plt.colorbar()\n",
    "# # 3.1.4. plt.grid(False)를 사용해 이미지 위에 격자가 보이지 않도록 설정합니다.\n",
    "# #    기본적으로 격자가 나타나는데, 이미지를 깔끔하게 보기 위해 제거하는 거예요.\n",
    "# plt.grid(False)\n",
    "# # 3.1.5. plt.show()를 호출해 최종적으로 시각화된 이미지를 화면에 표시합니다.\n",
    "# plt.show()\n",
    "# # 3.1.6. 마지막으로, 첫 번째 이미지의 라벨(정답)을 출력합니다.\n",
    "# #    train_labels는 각 이미지의 정답 배열.\n",
    "# print(f\"첫 번째 이미지의 라벨: {train_labels[0]}\")\n",
    "\n",
    "# 4. 정규화\n",
    "train_images_normalized = train_images.astype(np.float32) / 255.0\n",
    "test_images_normalized = test_images.astype(np.float32) / 255.0\n",
    "# 결과 데이터 형식 확인.\n",
    "print(train_images_normalized.dtype)  # unint8: unsigned(부호가 없는) int8\n",
    "# 4.1 채널 차원 추가 # 흑백이므로 1, 컬러의 경우 3\n",
    "train_images_normalized = train_images_normalized[:, np.newaxis, :, :]  # np.newaxis 는 인덱싱 키워드\n",
    "test_images_normalized = test_images_normalized[:, np.newaxis, :, :]\n",
    "# 결과 확인\n",
    "print(f\"채널 추가 후 훈련 데이터 형태:  {train_images_normalized.shape}\")\n",
    "print(f\"채널 추가 후 테스트 데이터 형태:  {test_images_normalized.shape}\")\n",
    "\n",
    "# 5. 모델 학습\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 5.1. FashionMNIST 모델 클래스 정의\n",
    "# PyTorch에서는 nn.Module을 상속받아 모델을 클래스 형태로 만듭니다.\n",
    "# 이렇게 하면 모델의 구조와 동작을 체계적으로 관리할 수 있어요.\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    # 5.1.2. 모델의 각 층(layer) 정의\n",
    "    # __init__ 함수에서 모델의 구성 요소를 미리 정의합니다.\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        # 5.1.2.1. Flatten 레이어: 28*28 이미지를 784 픽셀의 1차원 벡터로 변환\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 5.1.2.2. 첫 번째 Dense(Linear) 레이어\n",
    "        # 784개의 입력을 받아 128개의 뉴런을 가진 층으로 연결.\n",
    "        # self.fc1 = nn.Linear(28 * 28, 128) # 기존 1층\n",
    "        ### 9.1.1. 첫 번째 층 뉴런 수 증가\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)  # 기존 1층에서 뉴런 수 증가(256)\n",
    "        # 5.1.2.3. 두 번째 Dense(Linear) 레이어\n",
    "        # 128개의 입력을 받아 10개의 출력 뉴런을 가진 층으로 연결\n",
    "        # Fashion-MNIST의 클래스가 10개이기 때문\n",
    "        # self.fc2 = nn.Linear(128, 10) # 기존 2층\n",
    "        ### 9.1.2. 새로운 중간 층을 추가하고 출력층을 3번째로 이동.\n",
    "        self.fc2 = nn.Linear(256, 128)  # 새로운 중간 층을  추가\n",
    "        self.fc3 = nn.Linear(128, 10)  # 출력층 3층으로 이동.\n",
    "\n",
    "    # 5.1.3. 모델의 순전파(foward pass) 정의\n",
    "    # 이 함수는 입력 데이터가 모델의 층을 통과하는 순서를 정의\n",
    "    # 데이터가 어떤 과정을 거쳐 최종 결과로 나오는지 결정.\n",
    "    def forward(self, x):\n",
    "        # 5.1.3.1. 입력 이미지를 1차원으로 펼침.\n",
    "        x = self.flatten(x)\n",
    "        # 5.1.3.2. 첫 번째 완전 연결 층을 통과, relu 활성화 함수를 적용\n",
    "        # ReLu는 활성화 함수이다. 음수 값을 0으로 만들고 양수 값을 그대로 통과시켜 비선형성을 부여.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 5.1.3.3. 두 번째 완전 연결 층을 통과시켜 최종 결과를 얻는다.\n",
    "        # 이 단계의 출력은 각 클래스에 대한 '점수'\n",
    "        # x = self.fc2(x) # 기존 출력 층\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # 출력 층\n",
    "        # 5.1.3.4. 최종 결과는 나중에 손실 함수(CrossEntropyLoss)에서 Softmax를 내부적으로 계산\n",
    "        # Softmax를 직접 적용하지는 않는다.\n",
    "        return x\n",
    "\n",
    "\n",
    "# 5.2. FashionMNIST CNN 모델 클래스 정의\n",
    "class FashionMNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTCNN, self).__init__()\n",
    "        # 첫 번째 합성곱 블록\n",
    "        # 입력: (1, 28, 28) → 출력: (32, 28, 28)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        # 풀링 후: (32, 28, 28) → (32, 14, 14)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 두 번째 합성곱 블록\n",
    "        # 입력: (32, 14, 14) → 출력: (64, 14, 14)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        # 풀링 후: (64, 14, 14) → (64, 7, 7)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 완전 연결 레이어\n",
    "        # 64 * 7 * 7 = 3136\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 블록: Conv → ReLU → Pool\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        # 두 번째 블록: Conv → ReLU → Pool\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # Flatten: (batch, 64, 7, 7) → (batch, 3136)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # 완전 연결 레이어\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 6. 모델 컴파일\n",
    "# 6.0. GPU(MPS) 사용 가능 여부 확인 및 장치 설정.\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "# 6.1. 모델 인스턴스 생성\n",
    "# 이제 우리가 정의한 FasionMNISTModel 클래스를 사용해 실제 모델 객체를 생성.\n",
    "# 위에서 설정한 장치(GPU 또는 CPU)로 모델을 보냅니다.\n",
    "# 이렇게 해야 모델의 가중치가 GPU 메모리에 올라가 GPU 연산을 사용할 수 있습니다.\n",
    "# model = FashionMNISTModel().to(device) # FashionMSTR 모델\n",
    "model = FashionMNISTCNN().to(device)  # FashionMSTR CNN 모델\n",
    "# 6.2. 모델 구조 출력\n",
    "print(model)\n",
    "import torch.optim as optim\n",
    "# 6.3. 손실 함수 정의\n",
    "# 모델의 예측과 실제 라벨 간의 오차를 계산하는 함수.\n",
    "# 다중 클래스 분류 문제에는 CrossEntropyLoss가 가장 널리 사용.\n",
    "# PyTorch의 이 손실 함수는 내부적으로 Softmax를 포함.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# 6.4. 옵티마이저 정의\n",
    "# 모델의 가중치를 업데이트하여 손실을 최소화하는 역할.\n",
    "# Adam은 현재 딥러닝에서 가장 많이 사용되는 옵티마이저 중 하나.\n",
    "# - model.parameters()는 모델이 학습할 모든 가중치와 편향을 넘겨주는 역할\n",
    "# - lr(learning rate)는 경사 하강법에서 한 번에 움직이는 보폭의 의미\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 9.2. 하이퍼파라미터 튜닝\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01) # 학습률 크게 증가.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)  # 학습률 크게 감소.\n",
    "\n",
    "# 7. 학습루프 생성.\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# 7.1. 데이터셋 및 데이터로더 생성\n",
    "# 정규화된 numpy 배열 데이터를 PyTorch의 Tensor로 변환.\n",
    "train_images_tensor = torch.from_numpy(train_images_normalized)\n",
    "train_labels_tensor = torch.from_numpy(train_labels).long()\n",
    "# TensorDataset은 이미지와 라벨 텐서를 묶어 데이터셋을 만듭니다.\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "# DataLoader는 학습 시 데이터를 배치(batch) 단위로 불러오는 역할을 합니다.\n",
    "# shuffle=True는 매 에포크마다 데이터를 무작위로 섞어 모델의 과적합을 방지합니다.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# 7.2. 모델을 장치로 이동\n",
    "# 모델을 CPU 또는 GPU/MPS로 옮겨줍니다.\n",
    "model.to(device)\n",
    "# 7.3. 학습 루프(Training Loop)\n",
    "# num_epochs = 5\n",
    "# 9.3. 에포크 수정\n",
    "num_epochs = 20\n",
    "print(\"Training starts...\")\n",
    "# 각 에포크의 시작에서 모델을 학습 모드로 설정합니다.\n",
    "# 이렇게 하면 드롭아웃 등 학습 시에만 필요한 기능들이 활성화됩니다.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # 데이터로더에서 이미지와 라벨을 배치 단위로 가져옵니다.\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # 데이터를 설정한 장치(device)로 보냅니다.\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 7.3.1 기울기 초기화\n",
    "        # 이전 배치 학습에서 계산된 기울기를 0으로 초기화합니다.\n",
    "        # 이 과정이 없으면 기울기가 계속 누적되어 학습이 이상해집니다.\n",
    "        optimizer.zero_grad()\n",
    "        # 7.3.2. 순전파(Foward Pass)\n",
    "        # 모델에 이미지 데이터를 넣어 예측값을 계산합니다.\n",
    "        outputs = model(images)\n",
    "        # 7.3.3. 손실 계산(Loss Calculation)\n",
    "        # 예측값과 실제 라벨을 비교해 손실(오차)을 계산\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # 7.3.4. 역전파(Backward Pass)\n",
    "        # 손실을 기반으로 각 매개변수에 대한 기울기를 계산\n",
    "        loss.backward()\n",
    "        # 7.3.5. 가중치 업데이트(Weight Update)\n",
    "        # 계산된 기울기를 사용해 모델의 가중치를 업데이트\n",
    "        optimizer.step()\n",
    "        # 개별 배치 Loss 출력 (10회마다)\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "        # 누적 손실 확인\n",
    "        running_loss += loss.item()\n",
    "    # 에포크 평균 Loss 출력\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "    print(\"-\" * 60)  # 구분선\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# 8. 모델 평가\n",
    "# 8.1. 모델을 평가 모드로 설정.\n",
    "# model.eval()은 학습 시에만 필요한 기능(예: Dropout, Batch Normalization)을 비활성화합니다.\n",
    "# 이렇게 해야 모델이 일관된 예측 결과를 내놓을 수 있습니다.\n",
    "model.eval()\n",
    "# 8.2. 기울기 계산 비활성화\n",
    "# torch.no_grad() 블록 안에서는 기울기 계산이 이루어지지 않습니다.\n",
    "# 평가 단계에서는 가중치를 업데이트할 필요가 없으므로, 메모리와 연산 속도를 절약할 수 있습니다.\n",
    "# 8.2.1. 테스트 데이터셋 및 데이터로더 생성\n",
    "# 테스트 이미지와 라벨을 PyTorch 텐서로 변환\n",
    "test_images_tensor = torch.from_numpy(test_images_normalized)\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "# TensorDataset으로 묶고 DataLoader를 생성\n",
    "# 평가 시에는 데이터 순서를 섞을 필요가 없으므로 suffle=False로 설정.\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# 8.2.2. 정확도(Accuracy) 계산을 위한 변수 초기화(기울기 계산 비활성화 블록)\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "# 8.2.3. 테스트 데이터로 순전파 실행.\n",
    "# 테스트 데이터로더에서 배치 단위로 데이터를 가져옵니다.\n",
    "for images, labels in test_loader:\n",
    "    # 데이터와 라벨을 설정한 장치(device)로 보냅니다.\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # 8.2.3.1. 순전파: 모델에 데이터를 넣어 예측값 얻음.\n",
    "    outputs = model(images)\n",
    "    # 8.2.3.2. 예측값 변환\n",
    "    # outputs은 각 클래스에 대한 점수입니다. 가장 높은 점수를 가진 클래스를 예측값으로 선택합니다.\n",
    "    # torch.max() 함수는 최댓값과 그 인덱스를 반환합니다. dim=1은 각 행(이미지)에서 최댓값을 찾으라는 의미입니다.\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # 8.2.3.3. 정확한 예측 수 계산\n",
    "    # 총 샘플 수를 업데이트\n",
    "    total_samples += labels.size(0)\n",
    "    # 예측값과 실제 라벨이 일치하는 개수를 세어 누적\n",
    "    correct_predictions += (predicted.view(-1) == labels.view(-1)).sum().item()\n",
    "# 8.2.4. 정확도 계산 및 출력\n",
    "# (정확히 예측한 샘플 수) / (총 샘플 수)를 계산하여 정확도를 얻습니다.\n",
    "accuracy = 100 * correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Evaluation finished!\")\n",
    "# 8.2.5.1. 오차 행렬\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model.eval()  # 모델을 평가 모드로 설정.\n",
    "# 8.2.5.2. 예측값과 실제 라벨 수집\n",
    "# 전체 테스트 데이터셋에 대한 예측값과 실제 라벨을 저장할 리스트를 만듦.\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "# 기울기 계산을 비활성화하고 평가를 진행.\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # 데이터와 라벨을 장치로 보냅니다.\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 순전파: 모델에 이미지를 넣어 예측값을 얻습니다.\n",
    "        outputs = model(images)\n",
    "        # 예측값 변환: 가장 높은 점수를 가진 클래스의 인덱스를 예측값으로 선택합니다.\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # 텐서를 CPU로 옮겨 NumPy 배열로 변환하고 리스트에 추가합니다.\n",
    "        # Scikit-learn은 PyTorch 텐서 대신 NumPy 배열을 입력으로 받습니다.\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# 8.2.5.3. 오차 행렬 생성\n",
    "# confusion_matrix 함수를 사용해 예측값과 실제 라벨로 오차 행렬을 만듭니다.\n",
    "# 오차 행렬은 Numpy 배열 형태로 반환됩니다.\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"오차 행렬 (Confusion Matrix):\")\n",
    "print(cm)\n",
    "# 8.2.5.4. 오차 행렬 시각화\n",
    "# Seaborn 라이브러리를 사용해 오차 행렬을 히트맵 형태로 시각화합니다.\n",
    "# fmt='d'는 값을 정수 형태로 표시하라는 의미입니다.\n",
    "# annot=True는 각 셀에 숫자를 표시하라는 의미입니다.\n",
    "plt.figure(figsize=(10, 8))\n",
    "tick_labels = [str(i) for i in range(10)]\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=tick_labels,\n",
    "    yticklabels=tick_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 9. 성능 개선\n",
    "# 9.1 모델 구조 변경\n",
    "# 모델에 층을 추가.\n",
    "# ** 구현을 5.2. 모델 설정에서 수정.\n",
    "# 9.1.1. 첫 번째 층 뉴런 수 증가\n",
    "# 9.1.2. 새로운 중간 층을 추가하고 출력층을 3번째로 이동.\n",
    "# 9.2. 하이퍼파라미터 튜닝(학습률)\n",
    "# ** 구현을 6.4. 옵티마이저 정의를 수정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22aed8",
   "metadata": {},
   "source": [
    "1. 데이터 로드 및 전처리\n",
    "   - 정규화 (0~1)\n",
    "   - 채널 차원 추가 (60000, 1, 28, 28)\n",
    "\n",
    "2. CNN 모델 정의\n",
    "   - Conv1: 1→32, Pool1\n",
    "   - Conv2: 32→64, Pool2\n",
    "   - FC1: 3136→128\n",
    "   - FC2: 128→10\n",
    "\n",
    "3. 모델 컴파일\n",
    "   - Loss: CrossEntropyLoss\n",
    "   - Optimizer: Adam (lr=0.0008)\n",
    "\n",
    "4. 학습\n",
    "   - Epochs: 20\n",
    "   - Batch size: 64\n",
    "\n",
    "5. 평가\n",
    "   - 정확도: 91.84%\n",
    "   - 오차 행렬 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555bac4",
   "metadata": {},
   "source": [
    "채널이 늘어난다고 하더라도 풀링을 진행하여 단순화가 진행되면 좀 더 복잡도 높은 내용을 학습시킬수가 없는거 아니야?\n",
    "\n",
    "와! 정말 날카로운 질문입니다! 🤯\n",
    "\n",
    "이건 CNN을 처음 배울 때 많은 사람들이 혼란스러워하는 **역설적인 부분**이에요!\n",
    "\n",
    "---\n",
    "\n",
    "## **역설처럼 보이는 상황**\n",
    "\n",
    "### **님의 의문:**\n",
    "\n",
    "> \"풀링으로 크기를 줄이면 (28×28 → 14×14 → 7×7) 정보가 줄어드는데, 어떻게 더 복잡한 내용을 학습할 수 있어?\"\n",
    "\n",
    "**직관적으로는 이상해 보입니다!** 🤔\n",
    "\n",
    "---\n",
    "\n",
    "## **핵심 개념: 무엇이 \"복잡도\"인가?**\n",
    "\n",
    "### **오해: 픽셀 개수 = 복잡도 ❌**\n",
    "\n",
    "```\n",
    "28×28 = 784 픽셀  → 단순하다?\n",
    "7×7 = 49 픽셀     → 복잡하다?\n",
    "```\n",
    "\n",
    "**이건 틀렸습니다!**\n",
    "\n",
    "---\n",
    "\n",
    "### **진실: 추상화 수준 = 복잡도 ✅**\n",
    "\n",
    "```\n",
    "낮은 추상화 (초기 레이어):\n",
    "- 픽셀 하나하나를 봄\n",
    "- \"이 픽셀이 밝다/어둡다\"\n",
    "- 간단한 정보, 많은 데이터\n",
    "\n",
    "높은 추상화 (후기 레이어):\n",
    "- 픽셀 조합의 의미를 봄\n",
    "- \"여기에 어깨선이 있다\"\n",
    "- 복잡한 정보, 적은 데이터\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **비유로 이해하기**\n",
    "\n",
    "### **책 읽기 비유:**\n",
    "\n",
    "**낮은 레벨 (글자):**\n",
    "```\n",
    "\"ㄱ\", \"ㅏ\", \"ㅁ\", \"ㅅ\", \"ㅏ\"\n",
    "→ 5개 요소 (많음)\n",
    "→ 간단한 정보 (단순함)\n",
    "```\n",
    "\n",
    "**높은 레벨 (단어):**\n",
    "```\n",
    "\"감사\"\n",
    "→ 1개 요소 (적음)\n",
    "→ 복잡한 정보 (의미 포함)\n",
    "```\n",
    "\n",
    "**더 높은 레벨 (문장):**\n",
    "```\n",
    "\"감사합니다\"\n",
    "→ 더 적은 요소\n",
    "→ 더 복잡한 의미 (예의, 감정 포함)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **CNN에서의 실제 과정**\n",
    "\n",
    "### **Conv1 + Pool1: 저수준 특징 추출**\n",
    "\n",
    "```python\n",
    "입력: (1, 28, 28) = 784개 픽셀 값\n",
    "# 각 픽셀: \"이 위치가 밝은가?\"\n",
    "\n",
    "↓ Conv1 (32 필터)\n",
    "\n",
    "(32, 28, 28) = 25,088개 값\n",
    "# 각 값: \"이 위치에 수평선이 있는가?\"\n",
    "#        \"이 위치에 수직선이 있는가?\"\n",
    "#        \"이 위치에 대각선이 있는가?\"\n",
    "# → 32가지 간단한 패턴 정보\n",
    "\n",
    "↓ Pool1\n",
    "\n",
    "(32, 14, 14) = 6,272개 값\n",
    "# \"이 영역에 수평선이 있는가?\" (위치 덜 중요)\n",
    "# → 공간 해상도는 낮지만 패턴 정보는 유지!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conv2 + Pool2: 고수준 특징 추출**\n",
    "\n",
    "```python\n",
    "입력: (32, 14, 14)\n",
    "# 각 채널: 이미 \"선\", \"모서리\" 같은 패턴 정보\n",
    "\n",
    "↓ Conv2 (64 필터)\n",
    "\n",
    "(64, 14, 14)\n",
    "# 32개 저수준 패턴을 조합!\n",
    "# 필터 1: \"수평선 + 수직선\" → 직각 코너 감지\n",
    "# 필터 2: \"여러 곡선\" → 둥근 형태 감지\n",
    "# 필터 3: \"대각선 + 곡선\" → 어깨선 감지\n",
    "# → 64가지 복잡한 조합 패턴!\n",
    "\n",
    "↓ Pool2\n",
    "\n",
    "(64, 7, 7) = 3,136개 값\n",
    "# \"이 영역에 어깨선이 있는가?\"\n",
    "# \"이 영역에 목선이 있는가?\"\n",
    "# → 위치 정보는 줄지만, 의미적 정보는 더 풍부!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **핵심: 정보의 \"밀도\"가 증가한다**\n",
    "\n",
    "### **정보량 vs 정보 밀도**\n",
    "\n",
    "```\n",
    "초기 레이어:\n",
    "(1, 28, 28) = 784개\n",
    "- 정보량: 많음 (784개 값)\n",
    "- 정보 밀도: 낮음 (단순 밝기)\n",
    "- 의미: \"픽셀 하나하나의 밝기\"\n",
    "\n",
    "후기 레이어:\n",
    "(64, 7, 7) = 3,136개\n",
    "- 정보량: 더 많음! (3,136개 값)\n",
    "- 정보 밀도: 매우 높음 (추상화된 개념)\n",
    "- 의미: \"64가지 복잡한 패턴의 존재 여부\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **시각화: 레이어별 \"보는\" 것**\n",
    "\n",
    "### **입력 이미지: 티셔츠**\n",
    "\n",
    "```\n",
    "원본 픽셀: (1, 28, 28)\n",
    "[0, 0, 0, 45, 67, 89, ...]\n",
    "→ \"이 좌표의 밝기 값\"\n",
    "```\n",
    "\n",
    "### **Conv1 출력: (32, 28, 28)**\n",
    "\n",
    "```\n",
    "채널 1: 수평선 맵\n",
    "[0, 0, 0.8, 0, 0, ...]\n",
    "→ \"여기에 수평선이 강하게 나타남\"\n",
    "\n",
    "채널 2: 수직선 맵\n",
    "[0.9, 0, 0, 0, 0.7, ...]\n",
    "→ \"여기에 수직선이 강하게 나타남\"\n",
    "\n",
    "...32개 간단한 패턴 맵\n",
    "```\n",
    "\n",
    "### **Pool1 출력: (32, 14, 14)**\n",
    "\n",
    "```\n",
    "\"대략 이 영역에 수평선이 있음\" (위치 조금 모호)\n",
    "→ 세부 위치보다 \"존재 여부\"에 집중\n",
    "```\n",
    "\n",
    "### **Conv2 출력: (64, 14, 14)**\n",
    "\n",
    "```\n",
    "채널 1: 직각 코너 맵\n",
    "→ \"수평선 + 수직선 조합\"\n",
    "\n",
    "채널 5: 어깨 형태 맵\n",
    "→ \"특정 곡선 + 대각선 조합\"\n",
    "\n",
    "채널 20: 목선 형태 맵\n",
    "→ \"여러 패턴의 복잡한 조합\"\n",
    "\n",
    "...64개 복잡한 패턴 맵\n",
    "```\n",
    "\n",
    "### **Pool2 출력: (64, 7, 7)**\n",
    "\n",
    "```\n",
    "\"이 넓은 영역에 어깨 형태가 있음\"\n",
    "→ 정확한 위치는 모르지만\n",
    "→ \"어깨가 있다\"는 추상적 개념은 명확!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **왜 이게 가능한가?**\n",
    "\n",
    "### **1. 채널 증가가 정보 손실을 보상**\n",
    "\n",
    "```\n",
    "크기: 28×28 → 14×14 (÷4 감소)\n",
    "채널: 32 → 64 (×2 증가)\n",
    "\n",
    "총 값 개수:\n",
    "32 × 14 × 14 = 6,272\n",
    "64 × 14 × 14 = 12,544 (오히려 증가!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 추상화가 정보를 압축**\n",
    "\n",
    "```\n",
    "저수준:\n",
    "\"(10,15) 밝음, (10,16) 어두움, (10,17) 밝음...\"\n",
    "→ 수백 개 픽셀 정보 필요\n",
    "\n",
    "고수준:\n",
    "\"어깨선 있음\"\n",
    "→ 하나의 값으로 표현 가능\n",
    "```\n",
    "\n",
    "**비유:**\n",
    "- \"ㄱ,ㅏ,ㅁ,ㅅ,ㅏ,ㅎ,ㅏ,ㅂ,ㄴ,ㅣ,ㄷ,ㅏ\" (12개)\n",
    "- \"감사합니다\" (1개) ← 더 적지만 더 많은 의미!\n",
    "\n",
    "---\n",
    "\n",
    "## **실제 증거: 특징 맵 시각화**\n",
    "\n",
    "연구자들이 CNN 레이어를 시각화한 결과:\n",
    "\n",
    "### **Conv1 (초기):**\n",
    "- 간단한 엣지, 색상, 텍스처\n",
    "\n",
    "### **Conv2-3 (중간):**\n",
    "- 눈, 코, 귀 같은 부분적 특징\n",
    "\n",
    "### **Conv4-5 (후기):**\n",
    "- 얼굴 전체, 자동차 전체 같은 복잡한 객체\n",
    "- **픽셀은 적지만 더 복잡한 개념!**\n",
    "\n",
    "---\n",
    "\n",
    "## **핵심 정리**\n",
    "\n",
    "### **님의 질문:**\n",
    "> \"풀링으로 단순화되면 복잡한 내용 학습 불가 아니야?\"\n",
    "\n",
    "### **답변:**\n",
    "\n",
    "**❌ 오해:**\n",
    "- 픽셀 수 감소 = 정보 감소 = 복잡도 감소\n",
    "\n",
    "**✅ 진실:**\n",
    "- 픽셀 수 감소 = 공간 해상도 감소\n",
    "- 채널 수 증가 = 추상화 수준 증가\n",
    "- **결과: 더 복잡한 개념을 학습!**\n",
    "\n",
    "---\n",
    "\n",
    "### **비유 정리:**\n",
    "\n",
    "```\n",
    "초기: \"숲을 이루는 나무 한 그루 한 그루\"\n",
    "     → 많은 정보, 단순한 의미\n",
    "\n",
    "후기: \"이건 소나무 숲이다\"\n",
    "     → 적은 정보, 복잡한 의미\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **수학적 관점:**\n",
    "\n",
    "```\n",
    "정보량 = 값의 개수 × 각 값의 의미 복잡도\n",
    "\n",
    "초기: 784 × 1 (밝기) = 784 units\n",
    "후기: 3,136 × 100 (추상 개념) = 313,600 units\n",
    "\n",
    "→ 오히려 증가!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**정말 본질을 꿰뚫는 질문이었습니다!** 🎉\n",
    "\n",
    "이제 **풀링으로 크기는 줄지만, 추상화로 복잡도는 증가**한다는 걸 이해하셨나요?\n",
    "\n",
    "**또 궁금한 점 있으신가요?** 😊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_study_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
