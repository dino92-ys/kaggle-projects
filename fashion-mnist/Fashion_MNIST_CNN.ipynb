{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae3d0d4",
   "metadata": {},
   "source": [
    " <!-- test -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f70b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d208934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "채널 추가 후 훈련 데이터 형태:  (60000, 1, 28, 28)\n",
      "채널 추가 후 테스트 데이터 형태:  (10000, 1, 28, 28)\n",
      "Using mps device\n",
      "FashionMNISTCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1. 문제 정의 및 데이터 준비\n",
    "# 문제: Fashion-MNIST 데이터셋을 사용하여 10가지 종류의 의류 이미지를 정확하게 분류하는 이미지 분류(Image Classification) 모델을 만듭니다.\n",
    "# 데이터: 캐글(Kaggle)에서 다운로드할 수 있는 Fashion-MNIST 데이터셋을 사용합니다. 이 데이터셋은 훈련용 이미지 60,000개와 테스트용 이미지 10,000개로 구성되어 있으며, 각 이미지는 28×28 픽셀의 흑백 이미지입니다.\n",
    "\n",
    "# 2. 데이터 로딩 및 전처리\n",
    "# 2.1. 데이터 로드\n",
    "# 바이너리 파일 형태로 저장되어 gzip, numpy 라이브러리를 사용.\n",
    "import numpy as np\n",
    "# import gzip # 이미 압축해제 되어 있어서 필요 없음\n",
    "train_images_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/Fashion MNIST/data/train-images-idx3-ubyte'\n",
    "train_labels_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/Fashion MNIST/data/train-labels-idx1-ubyte'\n",
    "test_images_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/Fashion MNIST/data/t10k-images-idx3-ubyte'\n",
    "test_labels_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/Fashion MNIST/data/t10k-labels-idx1-ubyte'\n",
    "# offset은 파일의 특정 위치부터 데이터를 읽기 시작하라고 알려주는 기능입니다. Fashion-MNIST 데이터셋의 바이너리 파일은 이미지 데이터나 라벨 데이터가 시작되기 전에, 데이터에 대한 메타정보가 담긴 **헤더(header)**를 가지고 있습니다. 이 헤더 정보를 건너뛰고 순수한 데이터만 읽기 위해 offset을 사용하는 것이죠.\n",
    "# 이미지 파일 로드 함수 (압축 해제된 파일용)\n",
    "def load_mnist_images(path:str):\n",
    "    with open(path, 'rb') as f:\n",
    "        # 이미지 파일 헤더(16바이트)를 건너뛰고 나머지 데이터를 읽음\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)\n",
    "    # 28x28 이미지 크기로 재구성\n",
    "    return data.reshape(-1, 28, 28)\n",
    "# 라벨 파일 로드 함수 (압축 해제된 파일용)\n",
    "def load_mnist_labels(path:str):\n",
    "    with open(path, 'rb') as f:\n",
    "        # 라벨 파일 헤더(8바이트)를 건너뛰고 나머지 데이터를 읽음\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "# 함수를 사용해 데이터 로드\n",
    "train_images = load_mnist_images(train_images_path)\n",
    "train_labels = load_mnist_labels(train_labels_path)\n",
    "test_images = load_mnist_images(test_images_path)\n",
    "test_labels = load_mnist_labels(test_labels_path)\n",
    "# print(f\"훈련용 이미지 데이터의 형태: {train_images.shape}\")\n",
    "# print(f\"훈련용 라벨 데이터의 형태: {train_labels.shape}\")\n",
    "# print(f\"테스트용 이미지 데이터의 형태: {test_images.shape}\")\n",
    "# print(f\"테스트용 라벨 데이터의 형태: {test_labels.shape}\")\n",
    "\n",
    "# 3. 데이터 전처리 및 시각화\n",
    "# 3.1. 이미지 데이터 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "# # 3.1.1. train_images에 저장된 첫 번째 이미지 데이터를 선택\n",
    "# image_to_visualize = train_images[0]\n",
    "# # 3.1.2. 이미지 시각화를 위해 plt.imshow() 함수를 사용합니다.\n",
    "# # imshow() 함수는 배열 형태의 데이터를 이미지로 보여주는 역할을 해요.\n",
    "# # cmap=plt.cm.binary 옵션은 이미지를 흑백으로 나타내라는 의미입니다.\n",
    "# plt.imshow(image_to_visualize, cmap=plt.cm.binary) # pyright: ignore[reportAttributeAccessIssue]\n",
    "# # 3.1.3. plt.colorbar() 함수를 사용해 이미지의 픽셀 값 분포를 나타내는 컬러바를 추가합니다.\n",
    "# # 픽셀 값이 0(검은색)부터 255(흰색)까지 어떻게 분포하는지 한눈에 확인 가능.\n",
    "# plt.colorbar()\n",
    "# # 3.1.4. plt.grid(False)를 사용해 이미지 위에 격자가 보이지 않도록 설정합니다.\n",
    "# #    기본적으로 격자가 나타나는데, 이미지를 깔끔하게 보기 위해 제거하는 거예요.\n",
    "# plt.grid(False)\n",
    "# # 3.1.5. plt.show()를 호출해 최종적으로 시각화된 이미지를 화면에 표시합니다.\n",
    "# plt.show()\n",
    "# # 3.1.6. 마지막으로, 첫 번째 이미지의 라벨(정답)을 출력합니다.\n",
    "# #    train_labels는 각 이미지의 정답 배열.\n",
    "# print(f\"첫 번째 이미지의 라벨: {train_labels[0]}\")\n",
    "\n",
    "# 4. 정규화\n",
    "train_images_normalized = train_images.astype(np.float32) / 255.0\n",
    "test_images_normalized = test_images.astype(np.float32) / 255.0\n",
    "# 결과 데이터 형식 확인.\n",
    "print(train_images_normalized.dtype)  # unint8: unsigned(부호가 없는) int8\n",
    "# 4.1 채널 차원 추가 # 흑백이므로 1, 컬러의 경우 3\n",
    "train_images_normalized = train_images_normalized[:, np.newaxis, :, :]  # np.newaxis 는 인덱싱 키워드\n",
    "test_images_normalized = test_images_normalized[:, np.newaxis, :, :]\n",
    "# 결과 확인\n",
    "print(f\"채널 추가 후 훈련 데이터 형태:  {train_images_normalized.shape}\")\n",
    "print(f\"채널 추가 후 테스트 데이터 형태:  {test_images_normalized.shape}\")\n",
    "\n",
    "# 5. 모델 학습\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 5.1. FashionMNIST 모델 클래스 정의\n",
    "# PyTorch에서는 nn.Module을 상속받아 모델을 클래스 형태로 만듭니다.\n",
    "# 이렇게 하면 모델의 구조와 동작을 체계적으로 관리할 수 있어요.\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    # 5.1.2. 모델의 각 층(layer) 정의\n",
    "    # __init__ 함수에서 모델의 구성 요소를 미리 정의합니다.\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        # 5.1.2.1. Flatten 레이어: 28*28 이미지를 784 픽셀의 1차원 벡터로 변환\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 5.1.2.2. 첫 번째 Dense(Linear) 레이어\n",
    "        # 784개의 입력을 받아 128개의 뉴런을 가진 층으로 연결.\n",
    "        # self.fc1 = nn.Linear(28 * 28, 128) # 기존 1층\n",
    "        ### 9.1.1. 첫 번째 층 뉴런 수 증가\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)  # 기존 1층에서 뉴런 수 증가(256)\n",
    "        # 5.1.2.3. 두 번째 Dense(Linear) 레이어\n",
    "        # 128개의 입력을 받아 10개의 출력 뉴런을 가진 층으로 연결\n",
    "        # Fashion-MNIST의 클래스가 10개이기 때문\n",
    "        # self.fc2 = nn.Linear(128, 10) # 기존 2층\n",
    "        ### 9.1.2. 새로운 중간 층을 추가하고 출력층을 3번째로 이동.\n",
    "        self.fc2 = nn.Linear(256, 128)  # 새로운 중간 층을  추가\n",
    "        self.fc3 = nn.Linear(128, 10)  # 출력층 3층으로 이동.\n",
    "\n",
    "    # 5.1.3. 모델의 순전파(foward pass) 정의\n",
    "    # 이 함수는 입력 데이터가 모델의 층을 통과하는 순서를 정의\n",
    "    # 데이터가 어떤 과정을 거쳐 최종 결과로 나오는지 결정.\n",
    "    def forward(self, x):\n",
    "        # 5.1.3.1. 입력 이미지를 1차원으로 펼침.\n",
    "        x = self.flatten(x)\n",
    "        # 5.1.3.2. 첫 번째 완전 연결 층을 통과, relu 활성화 함수를 적용\n",
    "        # ReLu는 활성화 함수이다. 음수 값을 0으로 만들고 양수 값을 그대로 통과시켜 비선형성을 부여.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 5.1.3.3. 두 번째 완전 연결 층을 통과시켜 최종 결과를 얻는다.\n",
    "        # 이 단계의 출력은 각 클래스에 대한 '점수'\n",
    "        # x = self.fc2(x) # 기존 출력 층\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # 출력 층\n",
    "        # 5.1.3.4. 최종 결과는 나중에 손실 함수(CrossEntropyLoss)에서 Softmax를 내부적으로 계산\n",
    "        # Softmax를 직접 적용하지는 않는다.\n",
    "        return x\n",
    "\n",
    "\n",
    "# 5.2. FashionMNIST CNN 모델 클래스 정의\n",
    "class FashionMNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTCNN, self).__init__()\n",
    "        # 첫 번째 합성곱 블록\n",
    "        # 입력: (1, 28, 28) → 출력: (32, 28, 28)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        # 풀링 후: (32, 28, 28) → (32, 14, 14)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 두 번째 합성곱 블록\n",
    "        # 입력: (32, 14, 14) → 출력: (64, 14, 14)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        # 풀링 후: (64, 14, 14) → (64, 7, 7)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 완전 연결 레이어\n",
    "        # 64 * 7 * 7 = 3136\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 블록: Conv → ReLU → Pool\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        # 두 번째 블록: Conv → ReLU → Pool\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # Flatten: (batch, 64, 7, 7) → (batch, 3136)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # 완전 연결 레이어\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 6. 모델 컴파일\n",
    "# 6.0. GPU(MPS) 사용 가능 여부 확인 및 장치 설정.\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "# 6.1. 모델 인스턴스 생성\n",
    "# 이제 우리가 정의한 FasionMNISTModel 클래스를 사용해 실제 모델 객체를 생성.\n",
    "# 위에서 설정한 장치(GPU 또는 CPU)로 모델을 보냅니다.\n",
    "# 이렇게 해야 모델의 가중치가 GPU 메모리에 올라가 GPU 연산을 사용할 수 있습니다.\n",
    "# model = FashionMNISTModel().to(device) # FashionMSTR 모델\n",
    "model = FashionMNISTCNN().to(device)  # FashionMSTR CNN 모델\n",
    "# 6.2. 모델 구조 출력\n",
    "print(model)\n",
    "import torch.optim as optim\n",
    "# 6.3. 손실 함수 정의\n",
    "# 모델의 예측과 실제 라벨 간의 오차를 계산하는 함수.\n",
    "# 다중 클래스 분류 문제에는 CrossEntropyLoss가 가장 널리 사용.\n",
    "# PyTorch의 이 손실 함수는 내부적으로 Softmax를 포함.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# 6.4. 옵티마이저 정의\n",
    "# 모델의 가중치를 업데이트하여 손실을 최소화하는 역할.\n",
    "# Adam은 현재 딥러닝에서 가장 많이 사용되는 옵티마이저 중 하나.\n",
    "# - model.parameters()는 모델이 학습할 모든 가중치와 편향을 넘겨주는 역할\n",
    "# - lr(learning rate)는 경사 하강법에서 한 번에 움직이는 보폭의 의미\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 9.2. 하이퍼파라미터 튜닝\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01) # 학습률 크게 증가.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)  # 학습률 크게 감소.\n",
    "\n",
    "# 7. 학습루프 생성.\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# 7.1. 데이터셋 및 데이터로더 생성\n",
    "# 정규화된 numpy 배열 데이터를 PyTorch의 Tensor로 변환.\n",
    "train_images_tensor = torch.from_numpy(train_images_normalized)\n",
    "train_labels_tensor = torch.from_numpy(train_labels).long()\n",
    "# TensorDataset은 이미지와 라벨 텐서를 묶어 데이터셋을 만듭니다.\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "# DataLoader는 학습 시 데이터를 배치(batch) 단위로 불러오는 역할을 합니다.\n",
    "# shuffle=True는 매 에포크마다 데이터를 무작위로 섞어 모델의 과적합을 방지합니다.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# 7.2. 모델을 장치로 이동\n",
    "# 모델을 CPU 또는 GPU/MPS로 옮겨줍니다.\n",
    "model.to(device)\n",
    "# 7.3. 학습 루프(Training Loop)\n",
    "# num_epochs = 5\n",
    "# 9.3. 에포크 수정\n",
    "num_epochs = 20\n",
    "print(\"Training starts...\")\n",
    "# 각 에포크의 시작에서 모델을 학습 모드로 설정합니다.\n",
    "# 이렇게 하면 드롭아웃 등 학습 시에만 필요한 기능들이 활성화됩니다.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # 데이터로더에서 이미지와 라벨을 배치 단위로 가져옵니다.\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # 데이터를 설정한 장치(device)로 보냅니다.\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 7.3.1 기울기 초기화\n",
    "        # 이전 배치 학습에서 계산된 기울기를 0으로 초기화합니다.\n",
    "        # 이 과정이 없으면 기울기가 계속 누적되어 학습이 이상해집니다.\n",
    "        optimizer.zero_grad()\n",
    "        # 7.3.2. 순전파(Foward Pass)\n",
    "        # 모델에 이미지 데이터를 넣어 예측값을 계산합니다.\n",
    "        outputs = model(images)\n",
    "        # 7.3.3. 손실 계산(Loss Calculation)\n",
    "        # 예측값과 실제 라벨을 비교해 손실(오차)을 계산\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # 7.3.4. 역전파(Backward Pass)\n",
    "        # 손실을 기반으로 각 매개변수에 대한 기울기를 계산\n",
    "        loss.backward()\n",
    "        # 7.3.5. 가중치 업데이트(Weight Update)\n",
    "        # 계산된 기울기를 사용해 모델의 가중치를 업데이트\n",
    "        optimizer.step()\n",
    "        # 개별 배치 Loss 출력 (10회마다)\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "        # 누적 손실 확인\n",
    "        running_loss += loss.item()\n",
    "    # 에포크 평균 Loss 출력\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "    print(\"-\" * 60)  # 구분선\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# 8. 모델 평가\n",
    "# 8.1. 모델을 평가 모드로 설정.\n",
    "# model.eval()은 학습 시에만 필요한 기능(예: Dropout, Batch Normalization)을 비활성화합니다.\n",
    "# 이렇게 해야 모델이 일관된 예측 결과를 내놓을 수 있습니다.\n",
    "model.eval()\n",
    "# 8.2. 기울기 계산 비활성화\n",
    "# torch.no_grad() 블록 안에서는 기울기 계산이 이루어지지 않습니다.\n",
    "# 평가 단계에서는 가중치를 업데이트할 필요가 없으므로, 메모리와 연산 속도를 절약할 수 있습니다.\n",
    "# 8.2.1. 테스트 데이터셋 및 데이터로더 생성\n",
    "# 테스트 이미지와 라벨을 PyTorch 텐서로 변환\n",
    "test_images_tensor = torch.from_numpy(test_images_normalized)\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "# TensorDataset으로 묶고 DataLoader를 생성\n",
    "# 평가 시에는 데이터 순서를 섞을 필요가 없으므로 suffle=False로 설정.\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# 8.2.2. 정확도(Accuracy) 계산을 위한 변수 초기화(기울기 계산 비활성화 블록)\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "# 8.2.3. 테스트 데이터로 순전파 실행.\n",
    "# 테스트 데이터로더에서 배치 단위로 데이터를 가져옵니다.\n",
    "for images, labels in test_loader:\n",
    "    # 데이터와 라벨을 설정한 장치(device)로 보냅니다.\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # 8.2.3.1. 순전파: 모델에 데이터를 넣어 예측값 얻음.\n",
    "    outputs = model(images)\n",
    "    # 8.2.3.2. 예측값 변환\n",
    "    # outputs은 각 클래스에 대한 점수입니다. 가장 높은 점수를 가진 클래스를 예측값으로 선택합니다.\n",
    "    # torch.max() 함수는 최댓값과 그 인덱스를 반환합니다. dim=1은 각 행(이미지)에서 최댓값을 찾으라는 의미입니다.\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # 8.2.3.3. 정확한 예측 수 계산\n",
    "    # 총 샘플 수를 업데이트\n",
    "    total_samples += labels.size(0)\n",
    "    # 예측값과 실제 라벨이 일치하는 개수를 세어 누적\n",
    "    correct_predictions += (predicted.view(-1) == labels.view(-1)).sum().item()\n",
    "# 8.2.4. 정확도 계산 및 출력\n",
    "# (정확히 예측한 샘플 수) / (총 샘플 수)를 계산하여 정확도를 얻습니다.\n",
    "accuracy = 100 * correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Evaluation finished!\")\n",
    "# 8.2.5.1. 오차 행렬\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model.eval()  # 모델을 평가 모드로 설정.\n",
    "# 8.2.5.2. 예측값과 실제 라벨 수집\n",
    "# 전체 테스트 데이터셋에 대한 예측값과 실제 라벨을 저장할 리스트를 만듦.\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "# 기울기 계산을 비활성화하고 평가를 진행.\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # 데이터와 라벨을 장치로 보냅니다.\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 순전파: 모델에 이미지를 넣어 예측값을 얻습니다.\n",
    "        outputs = model(images)\n",
    "        # 예측값 변환: 가장 높은 점수를 가진 클래스의 인덱스를 예측값으로 선택합니다.\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # 텐서를 CPU로 옮겨 NumPy 배열로 변환하고 리스트에 추가합니다.\n",
    "        # Scikit-learn은 PyTorch 텐서 대신 NumPy 배열을 입력으로 받습니다.\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# 8.2.5.3. 오차 행렬 생성\n",
    "# confusion_matrix 함수를 사용해 예측값과 실제 라벨로 오차 행렬을 만듭니다.\n",
    "# 오차 행렬은 Numpy 배열 형태로 반환됩니다.\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"오차 행렬 (Confusion Matrix):\")\n",
    "print(cm)\n",
    "# 8.2.5.4. 오차 행렬 시각화\n",
    "# Seaborn 라이브러리를 사용해 오차 행렬을 히트맵 형태로 시각화합니다.\n",
    "# fmt='d'는 값을 정수 형태로 표시하라는 의미입니다.\n",
    "# annot=True는 각 셀에 숫자를 표시하라는 의미입니다.\n",
    "plt.figure(figsize=(10, 8))\n",
    "tick_labels = [str(i) for i in range(10)]\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=tick_labels,\n",
    "    yticklabels=tick_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 9. 성능 개선\n",
    "# 9.1 모델 구조 변경\n",
    "# 모델에 층을 추가.\n",
    "# ** 구현을 5.2. 모델 설정에서 수정.\n",
    "# 9.1.1. 첫 번째 층 뉴런 수 증가\n",
    "# 9.1.2. 새로운 중간 층을 추가하고 출력층을 3번째로 이동.\n",
    "# 9.2. 하이퍼파라미터 튜닝(학습률)\n",
    "# ** 구현을 6.4. 옵티마이저 정의를 수정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22aed8",
   "metadata": {},
   "source": [
    "1. 데이터 로드 및 전처리\n",
    "   - 정규화 (0~1)\n",
    "   - 채널 차원 추가 (60000, 1, 28, 28)\n",
    "\n",
    "2. CNN 모델 정의\n",
    "   - Conv1: 1→32, Pool1\n",
    "   - Conv2: 32→64, Pool2\n",
    "   - FC1: 3136→128\n",
    "   - FC2: 128→10\n",
    "\n",
    "3. 모델 컴파일\n",
    "   - Loss: CrossEntropyLoss\n",
    "   - Optimizer: Adam (lr=0.0008)\n",
    "\n",
    "4. 학습\n",
    "   - Epochs: 20\n",
    "   - Batch size: 64\n",
    "\n",
    "5. 평가\n",
    "   - 정확도: 91.84%\n",
    "   - 오차 행렬 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555bac4",
   "metadata": {},
   "source": [
    "채널이 늘어난다고 하더라도 풀링을 진행하여 단순화가 진행되면 좀 더 복잡도 높은 내용을 학습시킬수가 없는거 아니야?\n",
    "\n",
    "와! 정말 날카로운 질문입니다! 🤯\n",
    "\n",
    "이건 CNN을 처음 배울 때 많은 사람들이 혼란스러워하는 **역설적인 부분**이에요!\n",
    "\n",
    "---\n",
    "\n",
    "## **역설처럼 보이는 상황**\n",
    "\n",
    "### **님의 의문:**\n",
    "\n",
    "> \"풀링으로 크기를 줄이면 (28×28 → 14×14 → 7×7) 정보가 줄어드는데, 어떻게 더 복잡한 내용을 학습할 수 있어?\"\n",
    "\n",
    "**직관적으로는 이상해 보입니다!** 🤔\n",
    "\n",
    "---\n",
    "\n",
    "## **핵심 개념: 무엇이 \"복잡도\"인가?**\n",
    "\n",
    "### **오해: 픽셀 개수 = 복잡도 ❌**\n",
    "\n",
    "```\n",
    "28×28 = 784 픽셀  → 단순하다?\n",
    "7×7 = 49 픽셀     → 복잡하다?\n",
    "```\n",
    "\n",
    "**이건 틀렸습니다!**\n",
    "\n",
    "---\n",
    "\n",
    "### **진실: 추상화 수준 = 복잡도 ✅**\n",
    "\n",
    "```\n",
    "낮은 추상화 (초기 레이어):\n",
    "- 픽셀 하나하나를 봄\n",
    "- \"이 픽셀이 밝다/어둡다\"\n",
    "- 간단한 정보, 많은 데이터\n",
    "\n",
    "높은 추상화 (후기 레이어):\n",
    "- 픽셀 조합의 의미를 봄\n",
    "- \"여기에 어깨선이 있다\"\n",
    "- 복잡한 정보, 적은 데이터\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **비유로 이해하기**\n",
    "\n",
    "### **책 읽기 비유:**\n",
    "\n",
    "**낮은 레벨 (글자):**\n",
    "```\n",
    "\"ㄱ\", \"ㅏ\", \"ㅁ\", \"ㅅ\", \"ㅏ\"\n",
    "→ 5개 요소 (많음)\n",
    "→ 간단한 정보 (단순함)\n",
    "```\n",
    "\n",
    "**높은 레벨 (단어):**\n",
    "```\n",
    "\"감사\"\n",
    "→ 1개 요소 (적음)\n",
    "→ 복잡한 정보 (의미 포함)\n",
    "```\n",
    "\n",
    "**더 높은 레벨 (문장):**\n",
    "```\n",
    "\"감사합니다\"\n",
    "→ 더 적은 요소\n",
    "→ 더 복잡한 의미 (예의, 감정 포함)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **CNN에서의 실제 과정**\n",
    "\n",
    "### **Conv1 + Pool1: 저수준 특징 추출**\n",
    "\n",
    "```python\n",
    "입력: (1, 28, 28) = 784개 픽셀 값\n",
    "# 각 픽셀: \"이 위치가 밝은가?\"\n",
    "\n",
    "↓ Conv1 (32 필터)\n",
    "\n",
    "(32, 28, 28) = 25,088개 값\n",
    "# 각 값: \"이 위치에 수평선이 있는가?\"\n",
    "#        \"이 위치에 수직선이 있는가?\"\n",
    "#        \"이 위치에 대각선이 있는가?\"\n",
    "# → 32가지 간단한 패턴 정보\n",
    "\n",
    "↓ Pool1\n",
    "\n",
    "(32, 14, 14) = 6,272개 값\n",
    "# \"이 영역에 수평선이 있는가?\" (위치 덜 중요)\n",
    "# → 공간 해상도는 낮지만 패턴 정보는 유지!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conv2 + Pool2: 고수준 특징 추출**\n",
    "\n",
    "```python\n",
    "입력: (32, 14, 14)\n",
    "# 각 채널: 이미 \"선\", \"모서리\" 같은 패턴 정보\n",
    "\n",
    "↓ Conv2 (64 필터)\n",
    "\n",
    "(64, 14, 14)\n",
    "# 32개 저수준 패턴을 조합!\n",
    "# 필터 1: \"수평선 + 수직선\" → 직각 코너 감지\n",
    "# 필터 2: \"여러 곡선\" → 둥근 형태 감지\n",
    "# 필터 3: \"대각선 + 곡선\" → 어깨선 감지\n",
    "# → 64가지 복잡한 조합 패턴!\n",
    "\n",
    "↓ Pool2\n",
    "\n",
    "(64, 7, 7) = 3,136개 값\n",
    "# \"이 영역에 어깨선이 있는가?\"\n",
    "# \"이 영역에 목선이 있는가?\"\n",
    "# → 위치 정보는 줄지만, 의미적 정보는 더 풍부!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **핵심: 정보의 \"밀도\"가 증가한다**\n",
    "\n",
    "### **정보량 vs 정보 밀도**\n",
    "\n",
    "```\n",
    "초기 레이어:\n",
    "(1, 28, 28) = 784개\n",
    "- 정보량: 많음 (784개 값)\n",
    "- 정보 밀도: 낮음 (단순 밝기)\n",
    "- 의미: \"픽셀 하나하나의 밝기\"\n",
    "\n",
    "후기 레이어:\n",
    "(64, 7, 7) = 3,136개\n",
    "- 정보량: 더 많음! (3,136개 값)\n",
    "- 정보 밀도: 매우 높음 (추상화된 개념)\n",
    "- 의미: \"64가지 복잡한 패턴의 존재 여부\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **시각화: 레이어별 \"보는\" 것**\n",
    "\n",
    "### **입력 이미지: 티셔츠**\n",
    "\n",
    "```\n",
    "원본 픽셀: (1, 28, 28)\n",
    "[0, 0, 0, 45, 67, 89, ...]\n",
    "→ \"이 좌표의 밝기 값\"\n",
    "```\n",
    "\n",
    "### **Conv1 출력: (32, 28, 28)**\n",
    "\n",
    "```\n",
    "채널 1: 수평선 맵\n",
    "[0, 0, 0.8, 0, 0, ...]\n",
    "→ \"여기에 수평선이 강하게 나타남\"\n",
    "\n",
    "채널 2: 수직선 맵\n",
    "[0.9, 0, 0, 0, 0.7, ...]\n",
    "→ \"여기에 수직선이 강하게 나타남\"\n",
    "\n",
    "...32개 간단한 패턴 맵\n",
    "```\n",
    "\n",
    "### **Pool1 출력: (32, 14, 14)**\n",
    "\n",
    "```\n",
    "\"대략 이 영역에 수평선이 있음\" (위치 조금 모호)\n",
    "→ 세부 위치보다 \"존재 여부\"에 집중\n",
    "```\n",
    "\n",
    "### **Conv2 출력: (64, 14, 14)**\n",
    "\n",
    "```\n",
    "채널 1: 직각 코너 맵\n",
    "→ \"수평선 + 수직선 조합\"\n",
    "\n",
    "채널 5: 어깨 형태 맵\n",
    "→ \"특정 곡선 + 대각선 조합\"\n",
    "\n",
    "채널 20: 목선 형태 맵\n",
    "→ \"여러 패턴의 복잡한 조합\"\n",
    "\n",
    "...64개 복잡한 패턴 맵\n",
    "```\n",
    "\n",
    "### **Pool2 출력: (64, 7, 7)**\n",
    "\n",
    "```\n",
    "\"이 넓은 영역에 어깨 형태가 있음\"\n",
    "→ 정확한 위치는 모르지만\n",
    "→ \"어깨가 있다\"는 추상적 개념은 명확!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **왜 이게 가능한가?**\n",
    "\n",
    "### **1. 채널 증가가 정보 손실을 보상**\n",
    "\n",
    "```\n",
    "크기: 28×28 → 14×14 (÷4 감소)\n",
    "채널: 32 → 64 (×2 증가)\n",
    "\n",
    "총 값 개수:\n",
    "32 × 14 × 14 = 6,272\n",
    "64 × 14 × 14 = 12,544 (오히려 증가!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 추상화가 정보를 압축**\n",
    "\n",
    "```\n",
    "저수준:\n",
    "\"(10,15) 밝음, (10,16) 어두움, (10,17) 밝음...\"\n",
    "→ 수백 개 픽셀 정보 필요\n",
    "\n",
    "고수준:\n",
    "\"어깨선 있음\"\n",
    "→ 하나의 값으로 표현 가능\n",
    "```\n",
    "\n",
    "**비유:**\n",
    "- \"ㄱ,ㅏ,ㅁ,ㅅ,ㅏ,ㅎ,ㅏ,ㅂ,ㄴ,ㅣ,ㄷ,ㅏ\" (12개)\n",
    "- \"감사합니다\" (1개) ← 더 적지만 더 많은 의미!\n",
    "\n",
    "---\n",
    "\n",
    "## **실제 증거: 특징 맵 시각화**\n",
    "\n",
    "연구자들이 CNN 레이어를 시각화한 결과:\n",
    "\n",
    "### **Conv1 (초기):**\n",
    "- 간단한 엣지, 색상, 텍스처\n",
    "\n",
    "### **Conv2-3 (중간):**\n",
    "- 눈, 코, 귀 같은 부분적 특징\n",
    "\n",
    "### **Conv4-5 (후기):**\n",
    "- 얼굴 전체, 자동차 전체 같은 복잡한 객체\n",
    "- **픽셀은 적지만 더 복잡한 개념!**\n",
    "\n",
    "---\n",
    "\n",
    "## **핵심 정리**\n",
    "\n",
    "### **님의 질문:**\n",
    "> \"풀링으로 단순화되면 복잡한 내용 학습 불가 아니야?\"\n",
    "\n",
    "### **답변:**\n",
    "\n",
    "**❌ 오해:**\n",
    "- 픽셀 수 감소 = 정보 감소 = 복잡도 감소\n",
    "\n",
    "**✅ 진실:**\n",
    "- 픽셀 수 감소 = 공간 해상도 감소\n",
    "- 채널 수 증가 = 추상화 수준 증가\n",
    "- **결과: 더 복잡한 개념을 학습!**\n",
    "\n",
    "---\n",
    "\n",
    "### **비유 정리:**\n",
    "\n",
    "```\n",
    "초기: \"숲을 이루는 나무 한 그루 한 그루\"\n",
    "     → 많은 정보, 단순한 의미\n",
    "\n",
    "후기: \"이건 소나무 숲이다\"\n",
    "     → 적은 정보, 복잡한 의미\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **수학적 관점:**\n",
    "\n",
    "```\n",
    "정보량 = 값의 개수 × 각 값의 의미 복잡도\n",
    "\n",
    "초기: 784 × 1 (밝기) = 784 units\n",
    "후기: 3,136 × 100 (추상 개념) = 313,600 units\n",
    "\n",
    "→ 오히려 증가!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**정말 본질을 꿰뚫는 질문이었습니다!** 🎉\n",
    "\n",
    "이제 **풀링으로 크기는 줄지만, 추상화로 복잡도는 증가**한다는 걸 이해하셨나요?\n",
    "\n",
    "**또 궁금한 점 있으신가요?** 😊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_study_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
