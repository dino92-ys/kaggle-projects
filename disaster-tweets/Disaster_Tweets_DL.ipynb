{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f919bd56",
   "metadata": {},
   "source": [
    "딥러닝 텍스트 분류 파이프라인\n",
    "우리가 앞으로 진행할 텍스트 분류 딥러닝 모델 개발 과정은 크게 4단계로 나눌 수 있습니다.\n",
    "\n",
    "1. 데이터 준비 및 전처리:\n",
    "\n",
    "데이터셋을 불러오고, 필요한 열(column)을 확인합니다.\n",
    "\n",
    "딥러닝 모델이 이해할 수 있도록 텍스트 데이터를 정제합니다. 불필요한 기호 제거, 소문자 변환, 토큰화(tokenization) 등이 이 단계에 해당합니다.\n",
    "\n",
    "2. 단어 임베딩(Word Embedding):\n",
    "\n",
    "정제된 텍스트 데이터의 각 단어를 의미를 담은 벡터로 변환합니다.\n",
    "\n",
    "Word2Vec이나 GloVe와 같은 모델을 직접 학습하거나, 이미 학습된 모델을 불러와 활용할 수 있습니다.\n",
    "\n",
    "3. 딥러닝 모델 구축:\n",
    "\n",
    "PyTorch나 TensorFlow와 같은 딥러닝 프레임워크를 사용해 모델을 만듭니다.\n",
    "\n",
    "임베딩 레이어(Embedding Layer), LSTM 레이어(LSTM Layer), 그리고 **완전 연결 레이어(Fully Connected Layer)**를 순서대로 쌓아 딥러닝 모델을 구성합니다.\n",
    "\n",
    "4. 모델 학습 및 평가:\n",
    "\n",
    "학습 데이터를 모델에 넣어 가중치(weight)를 최적화하고, 손실(loss)을 줄이는 과정을 반복합니다.\n",
    "\n",
    "학습이 완료된 모델의 성능을 검증 데이터(validation data)와 테스트 데이터(test data)로 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147e8f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06519ad",
   "metadata": {},
   "source": [
    "1. 데이터 불러오기\n",
    "\n",
    "설명: 훈련 및 테스트 데이터를 CSV 파일에서 pandas DataFrame으로 로드.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "pandas:\n",
    "\n",
    "pd.read_csv(): CSV 파일을 DataFrame으로 로드.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "파일 경로:\n",
    "\n",
    "훈련 데이터: train.csv\n",
    "테스트 데이터: test.csv\n",
    "\n",
    "\n",
    "\n",
    "2. 탐색적 데이터 분석 (EDA)\n",
    "\n",
    "설명: 데이터의 구조와 특성을 파악하기 위해 보고서를 생성하고, keyword와 location 컬럼의 통계 정보를 확인.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "(주석 처리됨) dataprep.eda.create_report(): 데이터 보고서 생성.\n",
    "(주석 처리됨) report.save(): 보고서를 HTML 파일로 저장.\n",
    "\n",
    "\n",
    "인사이트:\n",
    "\n",
    "keyword: 221개 고유 값, 결측치 0.8% (61개).\n",
    "location: 3341개 고유 값, 결측치 33.3% (2533개).\n",
    "\n",
    "\n",
    "\n",
    "3. 텍스트 정제\n",
    "\n",
    "3.1. keyword 컬럼 결측치 처리:\n",
    "\n",
    "설명: keyword 컬럼의 결측치를 최빈값(mode())으로 대체.\n",
    "사용된 함수:\n",
    "\n",
    "pandas.DataFrame.fillna(): 결측치를 지정된 값으로 채움.\n",
    "pandas.Series.mode(): 최빈값 계산.\n",
    "pandas.DataFrame.isnull().sum(): 결측치 수 확인.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.2. 대문자를 소문자로 변환:\n",
    "\n",
    "설명: text 컬럼의 문자열을 소문자로 변환.\n",
    "사용된 함수:\n",
    "\n",
    "pandas.Series.str.lower(): 문자열을 소문자로 변환.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.3. 특수문자, 숫자, 기호 제거:\n",
    "\n",
    "설명: text 컬럼에서 알파벳과 공백을 제외한 모든 문자 제거.\n",
    "사용된 함수:\n",
    "\n",
    "pandas.Series.str.replace(): 정규 표현식을 사용해 패턴 대체.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. 토큰화\n",
    "\n",
    "설명: text 컬럼의 문자열을 단어 단위로 토큰화.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "nltk:\n",
    "\n",
    "nltk.tokenize.word_tokenize(): 문자열을 단어 단위로 분리.\n",
    "nltk.download(): punkt, stopwords 데이터 다운로드.\n",
    "\n",
    "\n",
    "pandas.DataFrame.apply(): 각 행에 함수 적용.\n",
    "\n",
    "\n",
    "결과: tokenized_text 컬럼 생성.\n",
    "\n",
    "5. 불용어 제거\n",
    "\n",
    "설명: 토큰화된 텍스트에서 불용어(영어 stopwords) 제거.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "nltk.corpus.stopwords: 영어 불용어 리스트 제공.\n",
    "List comprehension: 불용어가 아닌 단어만 필터링.\n",
    "pandas.DataFrame.apply(): 불용어 제거 함수 적용.\n",
    "\n",
    "\n",
    "결과: non_stopwords_text 컬럼 생성.\n",
    "후처리: 토큰 리스트를 문자열로 변환해 processed_text 컬럼 생성.\n",
    "\n",
    "lambda 및 str.join(): 토큰 리스트를 공백으로 연결.\n",
    "\n",
    "\n",
    "\n",
    "6. 벡터화 (Word2Vec)\n",
    "\n",
    "설명: 불용어 제거된 텍스트를 Word2Vec으로 임베딩 벡터로 변환.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "gensim.models.Word2Vec:\n",
    "\n",
    "Word2Vec(): Word2Vec 모델 학습 (매개변수: vector_size=100, window=5, min_count=5, workers=4).\n",
    "model.wv.most_similar(): 유사 단어 확인.\n",
    "model.wv[]: 특정 단어의 임베딩 벡터 조회.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "결과: 단어 사전 생성 및 단어 간 유사성 확인 (예: disaster 단어의 유사 단어).\n",
    "\n",
    "7. 정수 인코딩 및 패딩\n",
    "\n",
    "7.1. 정수 인코딩:\n",
    "\n",
    "설명: 토큰화된 단어를 Word2Vec 단어 사전의 인덱스로 변환.\n",
    "사용된 함수:\n",
    "\n",
    "Dictionary comprehension: word_to_index 생성.\n",
    "List comprehension: 토큰을 인덱스로 변환.\n",
    "pandas.DataFrame.apply(): 정수 인코딩 적용.\n",
    "\n",
    "\n",
    "결과: encoded_text 컬럼 생성.\n",
    "\n",
    "\n",
    "7.2. 패딩:\n",
    "\n",
    "설명: 모든 시퀀스의 길이를 가장 긴 시퀀스 길이(max_len)에 맞춰 0으로 패딩.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "numpy.array(): 패딩된 시퀀스를 배열로 변환.\n",
    "torch.LongTensor(): NumPy 배열을 PyTorch 텐서로 변환.\n",
    "Custom function (pad_sequence): 시퀀스 패딩.\n",
    "\n",
    "\n",
    "결과: X (입력 텐서), y (타겟 텐서) 생성.\n",
    "\n",
    "\n",
    "\n",
    "8. 데이터 분할\n",
    "\n",
    "설명: 데이터를 학습용(80%)과 검증용(20%)으로 분할.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "sklearn.model_selection.train_test_split: 데이터 분할 (매개변수: test_size=0.2, random_state=42).\n",
    "\n",
    "\n",
    "결과: X_train, X_val, y_train, y_val 생성.\n",
    "\n",
    "9. 데이터 로더 구축\n",
    "\n",
    "설명: PyTorch의 Dataset과 DataLoader를 사용해 데이터를 배치 단위로 처리.\n",
    "사용된 라이브러리 및 함수:\n",
    "\n",
    "torch.utils.data.Dataset:\n",
    "\n",
    "Custom class TextDataset: 데이터와 레이블을 묶음.\n",
    "__len__: 데이터셋 크기 반환.\n",
    "__getitem__: 인덱스별 데이터/레이블 반환.\n",
    "\n",
    "\n",
    "torch.utils.data.DataLoader:\n",
    "\n",
    "DataLoader(): 배치 생성 (매개변수: batch_size=64, shuffle=True/False).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사 단어: [('im', 0.9990947246551514), ('still', 0.9990237355232239), ('like', 0.9990084171295166), ('us', 0.9990033507347107), ('fire', 0.9989922046661377)]\n",
      "가장 긴 트윗의 길이: 20\n",
      "Using device: mps\n",
      "학습 데이터로더 배치 수: 96\n",
      "검증 데이터로더 배치 수: 24\n",
      "첫 번째 배치 데이터 형태: torch.Size([64, 20])\n",
      "첫 번째 배치 레이블 형태: torch.Size([64])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'mv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 235\u001b[0m\n\u001b[1;32m    233\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# 최종 출력 차원, 이진 분류이므로 1로 설정\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Word2Vec 가중치를 PyTorch 텐서로 변환\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m pretrained_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmv\u001b[49m\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# 모델 객체 생성\u001b[39;00m\n\u001b[1;32m    237\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(vocab_size\n\u001b[1;32m    238\u001b[0m                      , embedding_dim\n\u001b[1;32m    239\u001b[0m                      , hidden_dim\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m                      , pretrained_weights\u001b[38;5;241m=\u001b[39mpretrained_weights \u001b[38;5;66;03m# 가중치 적용\u001b[39;00m\n\u001b[1;32m    243\u001b[0m                        )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'mv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "# 데이터 불러오기 (DataFrame 객체로 로드)\n",
    "train_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/train.csv'\n",
    "train_df = pd.read_csv(train_path) # train 데이터를 DataFrame으로 로드\n",
    "test_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# 2. EDA\n",
    "# 보고서 생성 (target 인자 없이)\n",
    "# from dataprep.eda import create_report # dataprep 데이터 분석\n",
    "# report = create_report(train_df) # 여기서 target 인자를 삭제했습니다.\n",
    "# 보고서를 HTML 파일로 저장\n",
    "# 파일명은 train_dataprep_report.html로 지정합니다.\n",
    "# report.save('train_dataprep_report.html')\n",
    "\n",
    "# 데이터 분석\n",
    "# 1.keyword 칼럼:\n",
    "# Approximate Distinct Count: 221개\n",
    "# Approximate Unique (%): 2.9%\n",
    "# Missing: 61개\n",
    "# Missing (%): 0.8%\n",
    "# Memory Size: 556863\n",
    "# 인사이트: sweetviz와 유사하게 221개의 고유 키워드가 있고, 약 0.8%의 적은 결측치가 존재합니다. 이 칼럼은 충분히 활용 가치가 있어 보입니다. 결측치는 처리해주는 것이 좋습니다.\n",
    "# 2. location 칼럼:\n",
    "# Approximate Distinct Count: 3341개\n",
    "# Approximate Unique (%): 65.8%\n",
    "# Missing: 2533개\n",
    "# Missing (%): 33.3%\n",
    "# Memory Size: 404598\n",
    "# 인사이트: sweetviz에서 확인했던 것과 동일하게, 3341개의 매우 많은 고유 지역이 있으며, 무려 33.3%라는 높은 비율의 결측치가 존재합니다.\n",
    "\n",
    "# 3. 텍스트 정제\n",
    "# 3.1. keyword 컬럼 처리\n",
    "train_df['keyword'] = train_df['keyword'].fillna(train_df['keyword'].mode()[0]) # fatalities\n",
    "train_df.isnull().sum()\n",
    "# 3.2. 대문자 > 소문자\n",
    "train_df['text'] = train_df['text'].str.lower()\n",
    "# 3.3. 특수문자, 숫자, 기호 제거\n",
    "# r'[^a-z ]' 패턴을 사용하고, 대체할 문자열은 '' (빈 문자열)\n",
    "train_df['text'] = train_df['text'].str.replace(r'[^a-z ]', '', regex=True)\n",
    "\n",
    "# 4. 토큰화\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# NLTK 데이터 다운로드 확인\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "    #nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "# 4.1 토큰화 (Tokenization)\n",
    "# text 칼럼의 각 문자열에 word_tokenize 함수 적용\n",
    "train_df['tokenized_text'] = train_df['text'].apply(word_tokenize)\n",
    "# 토큰화 결과 확인\n",
    "# print(\"토큰화 후 샘플:\")\n",
    "# for i in range(5):\n",
    "#     print(f\"Original Text (Cleaned): {train_df['text'][i]}\")\n",
    "#     print(f\"Tokenized Text: {train_df['tokenized_text'][i]}\\n\")\n",
    "\n",
    "# 5. 불용어 제거\n",
    "stop_words = set(stopwords.words('english')) # 영어 중 불용어\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words] # 불용어가 아닌 단어\n",
    "train_df['non_stopwords_text'] = train_df['tokenized_text'].apply(remove_stopwords)\n",
    "# 불용어 제거 후 샘플\n",
    "# for i in range(5):\n",
    "#     print(f\"Tokenized Text: {train_df['tokenized_text'][i]}\")\n",
    "#     print(f\"Non-stopwords Text: {train_df['non_stopwords_text'][i]}\\n\")\n",
    "# 토큰 리스트를 문자열로 변환\n",
    "train_df['processed_text'] = train_df['non_stopwords_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# 6. 벡터화 (Word2Vec 모델 사용)\n",
    "from gensim.models import Word2Vec\n",
    "# 토큰화 및 불용어 제거가 완료된 텍스트 데이터를 준비합니다.\n",
    "# 이 데이터는 'non_stopwords_text' 열에 이미 들어있습니다.\n",
    "sentences = train_df['non_stopwords_text'].tolist()\n",
    "# Word2Vec 모델을 학습합니다.\n",
    "# vector_size: 임베딩 벡터의 차원 (임베딩 공간의 크기)\n",
    "# window: 주변 단어를 고려하는 윈도우 크기\n",
    "# min_count: 최소 단어 빈도 (이 값보다 적게 등장하는 단어는 학습에서 제외)\n",
    "# workers: 학습에 사용할 CPU 코어 수\n",
    "embedding_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    workers=4\n",
    ")\n",
    "# Word2Vec 모델의 가중치를 가져옵니다.\n",
    "import torch\n",
    "weights = torch.FloatTensor(embedding_model.wv.vectors)\n",
    "# 'disaster' 단어의 임베딩 벡터 확인\n",
    "# print(\"'disaster' 단어의 임베딩 벡터:\", embedding_model.wv['disaster'])\n",
    "# 'disaster'와 가장 유사한 단어 5개 찾기\n",
    "similar_words = embedding_model.wv.most_similar('disaster', topn=5)\n",
    "print(\"유사 단어:\", similar_words)\n",
    "\n",
    "# 7. 정수 인코딩 & 패딩\n",
    "# 7.1. 정수 인코딩\n",
    "# 정수 인코딩은 텍스트 데이터의 각 단어를 고유한 숫자로 변환하는 과정입니다. 이는 모델이 텍스트를 처리할 수 있도록 만들어 줍니다. 우리가 학습한 Word2Vec 모델에 이미 단어 사전이 포함되어 있어, 이 작업을 쉽게 할 수 있습니다.\n",
    "# 7.1.1. Word2Vec 모델의 단어 사전을 가져옵니다.\n",
    "word_to_index = {word: idx for idx, word in enumerate(embedding_model.wv.index_to_key)}\n",
    "# 7.1.2. 토큰화된 텍스트 데이터를 정수 인코딩합니다.\n",
    "def encode_text(tokens, word_to_index):\n",
    "    return [word_to_index[token] for token in tokens if token in word_to_index]\n",
    "train_df['encoded_text'] = train_df['non_stopwords_text'].apply(lambda x: encode_text(x, word_to_index))\n",
    "# print(\"정수 인코딩 후 샘플:\")\n",
    "# print(train_df['encoded_text'].iloc[0])\n",
    "# 7.2. 패딩\n",
    "# 패딩을 통해 모든 트윗의 길이를 동일하게 맞춰줍니다. 딥러닝 모델이 데이터를 효율적으로 처리할 수 있도록 형태를 표준화하는 작업입니다. 가장 긴 문장의 길이를 기준으로 짧은 문장의 뒤에 0을 채워 넣습니다.\n",
    "# 7.2.1. 가장 긴 문장 길이 찾기\n",
    "max_len = max(len(s) for s in train_df['encoded_text'])\n",
    "print(f\"가장 긴 트윗의 길이: {max_len}\")\n",
    "# 7.2.2. 패딩을 위한 함수 정의\n",
    "def pad_sequence(sequence, max_len):\n",
    "    # 시퀀스 길이를 max_len으로 맞추고, 부족한 부분은 0으로 채웁니다.\n",
    "    # PyTorch의 F.pad를 사용하거나, Python 리스트 조작으로 구현할 수 있습니다.\n",
    "    # F.pad는 텐서 형태에서 사용. 아래는 Python 리스트로 구현\n",
    "    padded = sequence + [0] * (max_len - len(sequence))\n",
    "    return padded\n",
    "# 7.2.3. 모든 문장에 패딩 적용\n",
    "padded_sequences = [pad_sequence(seq, max_len) for seq in train_df['encoded_text']]\n",
    "# 7.2.4. NumPy 배열로 변환 후 PyTorch 텐서로 변환\n",
    "import numpy as np\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "X = torch.LongTensor(padded_sequences)\n",
    "# Y (타겟 변수)도 텐서로 변환\n",
    "y = torch.LongTensor(train_df['target'].values)\n",
    "# print(\"패딩 후 데이터 형태:\", X.shape)\n",
    "# print(\"패딩 후 샘플:\", X[0])\n",
    "\n",
    "# 8. 데이터 분할\n",
    "# Word2Vec 기반 정수 인코딩 및 패딩된 시퀀스와 목표 변수 'target'을 학습용, 검증용으로 나눕니다.\n",
    "# test_size는 검증용 데이터의 비율을 의미합니다. (0.2는 20%)\n",
    "# random_state는 재현성을 위해 고정합니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# 데이터 로더 구축 전에 텐서를 디바이스로 이동\n",
    "# GPU 사용 설정 (맥북은 'mps')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# 텐서를 디바이스로 이동시키는 코드 추가\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_val = y_val.to(device)\n",
    "\n",
    "# 9.데이터 로더 구축\n",
    "# Dataset: 데이터셋의 **데이터(X)**와 **정답 레이블(Y)**을 묶어주는 역할을 합니다. 각 데이터 샘플에 쉽게 접근할 수 있도록 만들어 주죠.\n",
    "# DataLoader: Dataset을 감싸서 미니 배치(mini-batch) 단위로 데이터를 자동으로 묶어주고, 학습 과정에서 데이터를 모델에 전달하는 역할을 합니다. 이렇게 하면 전체 데이터를 한꺼번에 메모리에 올리지 않고도 효율적으로 학습할 수 있어요.\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 전체 길이를 반환\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터와 레이블을 반환\n",
    "        return self.X[idx], self.y[idx]\n",
    "# 데이터셋 객체 생성\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "# DataLoader 객체 생성\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"학습 데이터로더 배치 수: {len(train_loader)}\")\n",
    "print(f\"검증 데이터로더 배치 수: {len(val_loader)}\")\n",
    "# 첫 번째 배치 확인\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(f\"첫 번째 배치 데이터 형태: {X_batch.shape}\")\n",
    "    print(f\"첫 번째 배치 레이블 형태: {y_batch.shape}\")\n",
    "    break\n",
    "\n",
    "# 10. 딥러닝 모델 구축\n",
    "import torch.nn as nn # PyTorch의 신경망 모듈(패키지)\n",
    "class LSTMClassifier(nn.Module): # nn.Module을 상속받아 LSTMClassifier 클래스 정의\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, pretrained_weights=None):\n",
    "        # vocab_size: 단어 사전의 크기. 모델이 알아야 할 단어의 총 개수\n",
    "        # embedding_dim: 각 단어를 표현하는 임베딩 벡터의 차원, 각 단어를 몇 차원의 벡터로 표현할지를 결정.\n",
    "        # hidden_dim: LSTM의 은닉 상태(hidden state) 차원. LSTM 층이 학습하는 정보의 '깊이'를 결정.\n",
    "        # output_dim: 모델의 출력 차원. 분류 문제에서는 클래스의 수와 동일. 지금은 이진 분류이므로 0(재난 트윗 아님) 또는 1(재난 트윗)을 나타낼 수 있도록 보통 1로 설정.\n",
    "        # num_layers: LSTM 층을 설정. 기본 1개.\n",
    "        super().__init__() # 부모 클래스 초기화\n",
    "        # 레이어(층) 정의\n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) # 패딩 인덱스 0으로 설정. 패딩으로 설정된 0을 학습에서 제외시키기 위해서 설정.\n",
    "        # Word2Vec 가중치가 있으면 로드\n",
    "        if pretrained_weights is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_weights)\n",
    "            # Fine-tuning 허용 (학습 중 임베딩도 업데이트 됨)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        # LSTM 레이어\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True) # num_layers 추가: 층수만큼 스택\n",
    "        # 완전연결 레이어\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        # 입력 데이터 text를 임베딩 층에 통과시켜 단어 벡터로 변환합니다.  이 결과로 [batch size, seq len, embedding dim] 형태의 3차원 텐서가 생성되는데, 이것은 \"여러 문장(batch size)에 있는 각 단어(seq len)를 벡터(embedding dim)로 표현했다\"는 뜻입니다.\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # 임베딩된 텐서가 LSTM 층을 통과합니다. 이때 LSTM은 두 가지를 출력합니다.\n",
    "        # output: 각 시점(time step)의 출력.\n",
    "        #hidden: 마지막 시점의 은닉 상태(Hidden State).\n",
    "        # cell: 마지막 시점의 셀 상태(Cell State).\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        # hidden = [1, batch size, hidden dim]\n",
    "        # hidden = hidden.squeeze(0)\n",
    "        # hidden 텐서의 형태가 [1, batch size, hidden dim]인데, LSTM은 기본적으로 하나의 층을 사용하므로 맨 앞의 차원(1)은 필요가 없습니다. squeeze(0)를 사용해 불필요한 차원을 제거하여 [batch size, hidden dim] 형태로 만들어줍니다.\n",
    "        # hidden = [batch size, hidden dim]\n",
    "        hidden = hidden[-1]  # [num_layers, batch_size, hidden_dim]의 마지막 층 추출: [batch_size, hidden_dim]\n",
    "        return self.fc(hidden)\n",
    "        # 마지막으로, 이렇게 정리된 hidden 텐서를 완전 연결 층(fc)에 넣어 최종 분류 결과를 얻습니다. 이때 hidden은 \"문장 전체의 의미를 압축한 벡터\"라고 생각할 수 있습니다.\n",
    "import torch.optim as optim\n",
    "# 하이퍼파라미터 정의\n",
    "vocab_size = len(word_to_index) # 단어 사전 크기\n",
    "embedding_dim = 100 # vector_size\n",
    "hidden_dim = 512 # LSTM의 은닉 상태 크기(모델이 정보를 기억하여 복잡한 패턴을 학습)\n",
    "num_layers = 2 # LSTM 층 개수를 설정. 기본 1, 증가 시 모델 깊이 증가로 복잡한 시퀀스 패턴 학습 강화)\n",
    "output_dim = 1 # 최종 출력 차원, 이진 분류이므로 1로 설정\n",
    "# Word2Vec 가중치를 PyTorch 텐서로 변환\n",
    "pretrained_weights = torch.FloatTensor(embedding_model.wv.vectors)\n",
    "# 모델 객체 생성\n",
    "model = LSTMClassifier(vocab_size\n",
    "                     , embedding_dim\n",
    "                     , hidden_dim\n",
    "                     , output_dim\n",
    "                     , num_layers\n",
    "                     , pretrained_weights=pretrained_weights # 가중치 적용\n",
    "                       )\n",
    "# 손실 함수 정의: BCEWithLogitsLoss(이진 교차 엔트로피 손실 함수)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# 최적화 정의: Adam(최적화 알고리즘)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 11. 모델 학습 및 평가 (훈련 루프)\n",
    "# 학습 루프(Training Loop)\n",
    "# 모델이 데이터로부터 지식을 습득하는 과정입니다. 데이터를 미니 배치 단위로 모델에 주입하고, 모델의 예측과 실제 정답 사이의 차이(손실)를 계산합니다. 이 손실을 줄이기 위해 역전파(Backpropagation)를 통해 모델의 가중치들을 조금씩 조정합니다.\n",
    "# 평가 루프(Validation Loop)\n",
    "# 모델이 학습 데이터에만 과적합되지 않았는지 확인하는 과정입니다. 학습이 끝날 때마다 별도로 분리해 둔 검증 데이터셋으로 모델의 성능을 평가합니다. 이때는 가중치를 업데이트하지 않고 오직 예측 정확도만 계산합니다.\n",
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    # 모델 훈련의 핵심 부분으로, 미니 배치 단위로 학습.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # 1. 그래디언트 초기화: 이전 배치의 그래디언트를 초기화\n",
    "        # 그래디언트는 기울기또는 변화율을 말한다.\n",
    "        optimizer.zero_grad()\n",
    "        # 2. 예측값 계산: 입력 데이터를 모델에 넣어 계산\n",
    "        outputs = model(inputs)\n",
    "        # 3. 손실 계산: 예측값과 정답을 비교하여 손실을 계산\n",
    "        loss = criterion(outputs.squeeze(1), labels.float()) # 텐서의 형태를 위해 형태 변환\n",
    "        # 4. 역전파: 계산된 손실을 사용하여 모델의 모든 학습 가능한 매개변수에 대한 그래디언트를 계산. 손실을 최소화 하는 방향 제시. 역전파가 그래디언트를 계산하는 알고리즘이고 계산된 그래디언트를 사용하여 가중치를 업데이트하는 최적화 알고리즘을 경사하강법이라고 한다.\n",
    "        loss.backward()\n",
    "        # 5. 가중치 업데이트: 옵티마이저는 역전파를 통해 계산된 그래디언트 값을 사용하여 모델의 모든 가중치를 업데이트.\n",
    "        optimizer.step()\n",
    "        # 예를 들어, 3번 채점하여 4번에서 어디를 어떻게 공부할지 분석 5번 실제로 공부하여 실력 향상\n",
    "        # 훈련 진행 상황 추적(1~5까지 한 에포크)\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.round(torch.sigmoid(outputs)) # FC 층의 출력은 제한이 없다. 음수, 양수 등 아무거나 나올 수 있는데 우리는 0 or 1을 원하기 때문에 sigmoid 함수를 적용(sigmoid는 확률로 해석이 가능)시켜 0~1 사이 값으로 표현되게 변경하여 반올림해서 0 또는 1로 만들어 준다.\n",
    "        total_correct += (preds.squeeze(1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    # 평가 함수\n",
    "    model.eval() # 모델을 평가모드로 전환. Dropout이나 BatchNorm 같은 일부 층들이 비활성화\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 학습이 아닌 결과 값을 확인해야하니 그래디언트 계산을 비활성화하여 가중치가 변경되지 않도록 함.\n",
    "        for inputs, labels in val_loader:\n",
    "            # 1. 예측값 계산\n",
    "            outputs = model(inputs)\n",
    "            # 2. 손실 계산\n",
    "            loss = criterion(outputs.squeeze(1), labels.float())\n",
    "            # 훈련 진행 상황 추적\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            total_correct += (preds.squeeze(1) == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 에포크 수 설정: 모델을 5회 반복(10회 반복 시, 과적합 진행)\n",
    "N_EPOCHS = 5\n",
    "# 학습 및 평가 결과 저장을 위한 리스트\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "# GPU 사용 설정 (맥이기에 'mps')\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# 전체 학습 루프: 핵심 반복문\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # 훈련\n",
    "    train_loss, train_acc = train_model(model, train_loader, optimizer, criterion)\n",
    "    # 검증\n",
    "    val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "    # 결과 저장\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    # 결과 출력\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "# 1. 테스트 데이터 불러오기\n",
    "test_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "print(\"테스트 데이터 로드 완료.\")\n",
    "\n",
    "# 2. 테스트 데이터 전처리 (훈련 데이터와 동일한 방식 적용)\n",
    "# 2.1. keyword 결측치 처리: **훈련 데이터(train_df)의 최빈값**으로 채웁니다.\n",
    "keyword_mode = train_df['keyword'].mode()[0]\n",
    "test_df['keyword'] = test_df['keyword'].fillna(keyword_mode)\n",
    "\n",
    "# 2.2. text 컬럼 소문자 변환 및 특수문자 제거\n",
    "test_df['text'] = test_df['text'].str.lower()\n",
    "test_df['text'] = test_df['text'].str.replace(r'[^a-z ]', '', regex=True)\n",
    "\n",
    "# 2.3. 토큰화 및 불용어 제거\n",
    "test_df['tokenized_text'] = test_df['text'].apply(word_tokenize)\n",
    "test_df['non_stopwords_text'] = test_df['tokenized_text'].apply(remove_stopwords)\n",
    "print(\"텍스트 정제, 토큰화, 불용어 제거 완료.\")\n",
    "\n",
    "# 3. 정수 인코딩 및 패딩\n",
    "# 3.1. 정수 인코딩: **훈련 시 생성된 단어 사전(word_to_index)**을 사용합니다.\n",
    "test_df['encoded_text'] = test_df['non_stopwords_text'].apply(lambda x: encode_text(x, word_to_index))\n",
    "\n",
    "# 3.2. 패딩: **훈련 데이터 기준 최대 길이(max_len)**로 길이를 맞춥니다.\n",
    "test_padded_sequences = [pad_sequence(seq, max_len) for seq in test_df['encoded_text']]\n",
    "\n",
    "# 3.3. 최종 텐서 변환\n",
    "X_test = torch.LongTensor(np.array(test_padded_sequences))\n",
    "X_test = X_test.to(device)\n",
    "print(\"정수 인코딩, 패딩, 텐서 변환 완료.\")\n",
    "print(f\"최종 테스트 데이터 형태: {X_test.shape}\")\n",
    "\n",
    "# 4. 테스트 데이터 로더 생성\n",
    "# 테스트용 Dataset은 레이블(y)이 필요 없습니다.\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "test_dataset = TestDataset(X_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"테스트 데이터 로더 생성 완료.\")\n",
    "\n",
    "# 5. 모델 예측 수행\n",
    "# 5.1. 모델을 평가 모드로 전환 (매우 중요!)\n",
    "model.eval()\n",
    "\n",
    "# 5.2. 예측 결과를 담을 리스트 생성\n",
    "test_preds = []\n",
    "\n",
    "# 5.3. 그래디언트 계산 비활성화\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        # 모델 예측\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 예측값을 0 또는 1로 변환 (sigmoid -> 0.5 기준으로 반올림)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        # 결과를 리스트에 추가 (CPU로 이동 후 NumPy 배열로 변환)\n",
    "        test_preds.extend(preds.cpu().numpy().flatten().astype(int))\n",
    "\n",
    "print(\"모델 예측 완료.\")\n",
    "print(f\"총 예측 개수: {len(test_preds)}\")\n",
    "\n",
    "\n",
    "# 6. 제출 파일(submission.csv) 생성\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ '{submission_path}' 파일이 성공적으로 생성되었습니다.\")\n",
    "print(\"제출 파일 샘플:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_study_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
