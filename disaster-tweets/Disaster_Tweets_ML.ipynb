{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c987582e",
   "metadata": {},
   "source": [
    "NLP ë¶„ë¥˜ ë¬¸ì œ ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ (Blueprint)\n",
    "ì´ íŒŒì´í”„ë¼ì¸ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë“  ë¬¸ì œì— ê¸°ë³¸ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì²­ì‚¬ì§„ì…ë‹ˆë‹¤.\n",
    "\n",
    "Phase 1: ë°ì´í„° ë¶„ì„ ë° ì „ì²˜ë¦¬ (Analysis & Preprocessing)\n",
    "ì´ ë‹¨ê³„ì˜ ëª©í‘œëŠ” ì›ë³¸ í…ìŠ¤íŠ¸(Raw Text)ë¥¼ ê¸°ê³„ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ê¹¨ë—í•œ ë‹¨ì–´ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ê¸°ë³¸ í†µê³„ë¥¼ íŒŒì•…í•˜ê³ , í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ì²˜ëŸ¼ ê°„ë‹¨í•œ íŠ¹ì§•ê³¼ ëª©í‘œ ë³€ìˆ˜(target)ì˜ ê´€ê³„ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.\n",
    "í•´ì•¼í•  ì¼: \n",
    "\n",
    "í…ìŠ¤íŠ¸ ì •ì œ (Text Cleaning) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: ëª¨ë¸ í•™ìŠµì— ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¼ê´€ëœ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "í•´ì•¼í•  ì¼: \n",
    "\n",
    "í† í°í™” (Tokenization) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: ì •ì œëœ ë¬¸ì¥ì„ \"ë‹¨ì–´\"ì™€ ê°™ì€ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„(í† í°)ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "í—¤ì•¼í•  ì¼: \n",
    "ì‚¬ìš© ë„êµ¬: \n",
    "ë¶ˆìš©ì–´ ì œê±° (Stopword Removal) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: a, the, is ì™€ ê°™ì´ ì˜ë¯¸ ë¶„ì„ì— í° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "ì‚¬ìš© ë„êµ¬: \n",
    "\n",
    "Phase 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (Feature Engineering)\n",
    "ì´ ë‹¨ê³„ì˜ ëª©í‘œëŠ” ì „ì²˜ë¦¬ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” **ìˆ«ì ë²¡í„°(Numerical Vector)**ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. NLP íŒŒì´í”„ë¼ì¸ì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë¶€ë¶„ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë²¡í„°í™” (Vectorization) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ì í–‰ë ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "ì£¼ìš” ê¸°ë²•:\n",
    "Bag-of-Words (BoW): ê° ë‹¨ì–´ì˜ ë“±ì¥ íšŸìˆ˜ë¥¼ ì„¸ì–´ ë²¡í„°ë¡œ ë§Œë“­ë‹ˆë‹¤. (CountVectorizer)\n",
    "TF-IDF: ë‹¨ì–´ì˜ ë¹ˆë„ì™€ í•¨ê»˜, íŠ¹ì • ë¬¸ì„œì—ë§Œ ë‚˜íƒ€ë‚˜ëŠ” ì¤‘ìš”í•œ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. (TfidfVectorizer)\n",
    "ê³ ê¸‰ ê¸°ë²•ìœ¼ë¡œëŠ” Word2Vec, BERT ì„ë² ë”© ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Phase 3: ëª¨ë¸ë§ ë° í‰ê°€ (Modeling & Evaluation)\n",
    "ì´ ë‹¨ê³„ëŠ” ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜ëœ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì¼ë°˜ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•˜ê²Œ ì§„í–‰ë©ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ë¶„í•  (Data Splitting) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµ(Train) ë° ê²€ì¦(Validation) ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìš©ì–´ ì •ë¦¬: train ë°ì´í„° vs. X_train\n",
    "train ë°ì´í„°(ì›ë˜ íŒŒì¼): ìš°ë¦¬ê°€ ìºê¸€ì—ì„œ ë°›ì€ train.csv íŒŒì¼ ì „ì²´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ íŒŒì¼ì€ ì •ë‹µ(target ì¹¼ëŸ¼)ì´ í¬í•¨ë˜ì–´ ìˆì–´ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²€ì¦í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "test ë°ì´í„°(ì›ë˜ íŒŒì¼): ìºê¸€ì—ì„œ ë°›ì€ test.csv íŒŒì¼ ì „ì²´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ íŒŒì¼ì€ ì •ë‹µì´ ì—†ì–´ì„œ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œí•˜ëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "X_train (ìš°ë¦¬ê°€ ë§Œë“  ë³€ìˆ˜): train.csvì—ì„œ ë‚˜ëˆˆ í•™ìŠµìš© ë°ì´í„°ì…ë‹ˆë‹¤. ì¦‰, ì „ì²´ train ë°ì´í„°ì…‹ì˜ 80%ë¥¼ ì°¨ì§€í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. XëŠ” í”¼ì²˜(feature)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "ì—­í• : ëª¨ë¸ì´ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "X_val (ìš°ë¦¬ê°€ ë§Œë“  ë³€ìˆ˜): train.csvì—ì„œ ë‚˜ëˆˆ ê²€ì¦ìš©(Validation) ë°ì´í„°ì…ë‹ˆë‹¤. ì „ì²´ train ë°ì´í„°ì…‹ì˜ 20%ë¥¼ ì°¨ì§€í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. valì€ validationì˜ ì•½ìì…ë‹ˆë‹¤.\n",
    "ì—­í• : ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ í‰ê°€í•˜ê³ , ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ í•™ìŠµ (Model Training) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: ë²¡í„°í™”ëœ í•™ìŠµ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
    "ì‚¬ìš© ëª¨ë¸: ë¡œì§€ìŠ¤í‹± íšŒê·€, ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ, LightGBM ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì„±ëŠ¥ í‰ê°€ (Evaluation) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: ê²€ì¦ ì„¸íŠ¸ë¥¼ ì´ìš©í•´ ì •í™•ë„(Accuracy), F1-Score ë“± ì ì ˆí•œ ì§€í‘œë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆì¸¡ (Prediction) (ğŸ”œ To-Do)\n",
    "ëª©í‘œ: ìµœì¢… ì„ íƒëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ ê³ ë„í™” (íŠœë‹): GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "\n",
    "ìµœì¢… ì˜ˆì¸¡ ë° íŒŒì¼ ì œì¶œ: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… ì˜ˆì¸¡ ìˆ˜í–‰ ë° ìºê¸€ ì œì¶œ íŒŒì¼ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919bd56",
   "metadata": {},
   "source": [
    "1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬ ğŸ§¹\n",
    "\n",
    "ì´ ë‹¨ê³„ëŠ” ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ë¡œë“œ: train.csvì™€ test.csv íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ DataFrame í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²°ì¸¡ì¹˜ ì²˜ë¦¬: keyword ì¹¼ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´(mode())ë¡œ ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì •ì œ: ëª¨ë¸ í•™ìŠµì— ë°©í•´ê°€ ë˜ëŠ” ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³ , íŠ¹ìˆ˜ë¬¸ìë‚˜ ìˆ«ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "\n",
    "í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±°: ë¬¸ì¥ì„ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´(token)ë¡œ ë‚˜ëˆ„ê³ , 'a', 'the'ì™€ ê°™ì´ ì˜ë¯¸ê°€ ì—†ëŠ” ë¶ˆìš©ì–´(stopword)ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "\n",
    "2ë‹¨ê³„: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ğŸ“Š\n",
    "\n",
    "ì •ì œëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìˆ«ì ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "TF-IDF ë²¡í„°í™”: TfidfVectorizerë¥¼ ì‚¬ìš©í•´ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ì™€ ì¤‘ìš”ë„ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ TF-IDF í–‰ë ¬ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” ê° ë‹¨ì–´ì— ê³ ìœ í•œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ í…ìŠ¤íŠ¸ì˜ íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. train ë°ì´í„°ë¡œ í•™ìŠµ(fit_transform)í•œ vectorizer ê°ì²´ëŠ” ë‚˜ì¤‘ì— test ë°ì´í„°ë¥¼ ë³€í™˜í•  ë•Œ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "3ë‹¨ê³„: ëª¨ë¸ë§ ë° ì´ˆê¸° ì„±ëŠ¥ í‰ê°€ ğŸš€\n",
    "\n",
    "ì´ì œ ì‹¤ì œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ë¶„í• : train ë°ì´í„°ë¥¼ ëª¨ë¸ í•™ìŠµìš©(X_train, y_train)ê³¼ ì´ˆê¸° ì„±ëŠ¥ ê²€ì¦ìš©(X_val, y_val)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ í•™ìŠµ: LogisticRegression ëª¨ë¸ì„ X_trainê³¼ y_trainìœ¼ë¡œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "ì„±ëŠ¥ í‰ê°€: í•™ìŠµëœ ëª¨ë¸ì„ X_val ë°ì´í„°ë¡œ ì˜ˆì¸¡í•˜ê³ , ì‹¤ì œ ì •ë‹µ(y_val)ê³¼ ë¹„êµí•˜ì—¬ ì •í™•ë„(accuracy), ì •ë°€ë„(precision), ì¬í˜„ìœ¨(recall) ë“±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "4ë‹¨ê³„: ëª¨ë¸ ê³ ë„í™” - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ âœ¨\n",
    "\n",
    "ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœëŒ€í•œ ëŒì–´ì˜¬ë¦¬ê¸° ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "GridSearchCV ì‚¬ìš©: GridSearchCVë¥¼ ì‚¬ìš©í•´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì˜ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ C ê°’ì˜ ì—¬ëŸ¬ í›„ë³´êµ°([0.1, 1, 10, 100])ì„ íƒìƒ‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "êµì°¨ ê²€ì¦: GridSearchCVëŠ” êµì°¨ ê²€ì¦ì„ í†µí•´ ëª¨ë“  í›„ë³´ ì¡°í•©ì˜ í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´ í•œ ë²ˆì˜ ê²€ì¦ ì ìˆ˜ë³´ë‹¤ ë” ì‹ ë¢°ì„± ìˆëŠ” ëª¨ë¸ ì„±ëŠ¥ì„ ì¶”ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìµœì  ëª¨ë¸ ì„ ì •: ê°€ì¥ ë†’ì€ í‰ê·  ì ìˆ˜ë¥¼ ì–»ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ê°€ì§„ ëª¨ë¸ì„ **ìµœì¢… ëª¨ë¸(best_model)**ë¡œ ì„ ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "5ë‹¨ê³„: ìµœì¢… ì˜ˆì¸¡ ë° íŒŒì¼ ìƒì„± ğŸ†\n",
    "\n",
    "ëª¨ë“  ì¤€ë¹„ê°€ ëë‚œ í›„, ìµœì¢… ëª¨ë¸ì„ ì‚¬ìš©í•´ ì œì¶œ íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬: test ë°ì´í„°ì— train ë°ì´í„°ì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë²¡í„°í™”: train ë°ì´í„°ì—ì„œ í•™ìŠµëœ vectorizer ê°ì²´ë¥¼ ì‚¬ìš©í•´ test ë°ì´í„°ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤. ì´ë•Œ fit_transformì´ ì•„ë‹Œ transform ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìµœì¢… ì˜ˆì¸¡: ì„ ì •ëœ best_modelì„ ì‚¬ìš©í•´ test ë°ì´í„°ì˜ target ê°’ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "íŒŒì¼ ì œì¶œ: ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìºê¸€ì˜ ì œì¶œ ì–‘ì‹ì— ë§ê²Œ submission.csv íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "ëª¨ë¸ì˜ ì •í™•ë„(Accuracy): 0.7965\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84       874\n",
      "           1       0.85      0.63      0.73       649\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.78      0.78      1523\n",
      "weighted avg       0.80      0.80      0.79      1523\n",
      "\n",
      "\n",
      "--- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ---\n",
      "ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'C': 1, 'solver': 'liblinear'}\n",
      "ìµœê³  ì •í™•ë„ ì ìˆ˜: 0.7060344106098085\n",
      "ìµœì í™”ëœ ëª¨ë¸ì˜ ì •í™•ë„(X_val ê¸°ì¤€): 0.8884\n",
      "\n",
      "--- ìµœì í™”ëœ ëª¨ë¸ Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91       874\n",
      "           1       0.95      0.78      0.86       649\n",
      "\n",
      "    accuracy                           0.89      1523\n",
      "   macro avg       0.90      0.87      0.88      1523\n",
      "weighted avg       0.89      0.89      0.89      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (DataFrame ê°ì²´ë¡œ ë¡œë“œ)\n",
    "train_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/train.csv'\n",
    "train_df = pd.read_csv(train_path) # train ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë¡œë“œ\n",
    "test_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# 2. EDA\n",
    "# ë³´ê³ ì„œ ìƒì„± (target ì¸ì ì—†ì´)\n",
    "#from dataprep.eda import create_report # dataprep ë°ì´í„° ë¶„ì„\n",
    "#report = create_report(train_df) # ì—¬ê¸°ì„œ target ì¸ìë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\n",
    "# ë³´ê³ ì„œë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥\n",
    "# íŒŒì¼ëª…ì€ train_dataprep_report.htmlë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "# report.save('train_dataprep_report.html')\n",
    "\n",
    "# ë°ì´í„° ë¶„ì„\n",
    "# 1.keyword ì¹¼ëŸ¼:\n",
    "# Approximate Distinct Count: 221ê°œ\n",
    "# Approximate Unique (%): 2.9%\n",
    "# Missing: 61ê°œ\n",
    "# Missing (%): 0.8%\n",
    "# Memory Size: 556863\n",
    "# ì¸ì‚¬ì´íŠ¸: sweetvizì™€ ìœ ì‚¬í•˜ê²Œ 221ê°œì˜ ê³ ìœ  í‚¤ì›Œë“œê°€ ìˆê³ , ì•½ 0.8%ì˜ ì ì€ ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ ì¹¼ëŸ¼ì€ ì¶©ë¶„íˆ í™œìš© ê°€ì¹˜ê°€ ìˆì–´ ë³´ì…ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ëŠ” ì²˜ë¦¬í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "# 2. location ì¹¼ëŸ¼:\n",
    "# Approximate Distinct Count: 3341ê°œ\n",
    "# Approximate Unique (%): 65.8%\n",
    "# Missing: 2533ê°œ\n",
    "# Missing (%): 33.3%\n",
    "# Memory Size: 404598\n",
    "# ì¸ì‚¬ì´íŠ¸: sweetvizì—ì„œ í™•ì¸í–ˆë˜ ê²ƒê³¼ ë™ì¼í•˜ê²Œ, 3341ê°œì˜ ë§¤ìš° ë§ì€ ê³ ìœ  ì§€ì—­ì´ ìˆìœ¼ë©°, ë¬´ë ¤ 33.3%ë¼ëŠ” ë†’ì€ ë¹„ìœ¨ì˜ ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ ì •ì œ\n",
    "# 3.1. keyword ì»¬ëŸ¼ ì²˜ë¦¬\n",
    "train_df['keyword'] = train_df['keyword'].fillna(train_df['keyword'].mode()[0]) # fatalities\n",
    "train_df.isnull().sum()\n",
    "# 3.2. ëŒ€ë¬¸ì > ì†Œë¬¸ì\n",
    "train_df['text'] = train_df['text'].str.lower()\n",
    "# 3.3. íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì, ê¸°í˜¸ ì œê±°\n",
    "# r'[^a-z ]' íŒ¨í„´ì„ ì‚¬ìš©í•˜ê³ , ëŒ€ì²´í•  ë¬¸ìì—´ì€ '' (ë¹ˆ ë¬¸ìì—´)\n",
    "train_df['text'] = train_df['text'].str.replace(r'[^a-z ]', '', regex=True)\n",
    "\n",
    "# 4. í† í°í™”\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ í™•ì¸\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "    #nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "# 4.1 í† í°í™” (Tokenization)\n",
    "# text ì¹¼ëŸ¼ì˜ ê° ë¬¸ìì—´ì— word_tokenize í•¨ìˆ˜ ì ìš©\n",
    "train_df['tokenized_text'] = train_df['text'].apply(word_tokenize)\n",
    "# í† í°í™” ê²°ê³¼ í™•ì¸\n",
    "# print(\"í† í°í™” í›„ ìƒ˜í”Œ:\")\n",
    "# for i in range(5):\n",
    "#     print(f\"Original Text (Cleaned): {train_df['text'][i]}\")\n",
    "#     print(f\"Tokenized Text: {train_df['tokenized_text'][i]}\\n\")\n",
    "\n",
    "# 5. ë¶ˆìš©ì–´ ì œê±°\n",
    "stop_words = set(stopwords.words('english')) # ì˜ì–´ ì¤‘ ë¶ˆìš©ì–´\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words] # ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ë‹¨ì–´\n",
    "train_df['non_stopwords_text'] = train_df['tokenized_text'].apply(remove_stopwords)\n",
    "# ë¶ˆìš©ì–´ ì œê±° í›„ ìƒ˜í”Œ\n",
    "# for i in range(5):\n",
    "#     print(f\"Tokenized Text: {train_df['tokenized_text'][i]}\")\n",
    "#     print(f\"Non-stopwords Text: {train_df['non_stopwords_text'][i]}\\n\")\n",
    "# í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "train_df['processed_text'] = train_df['non_stopwords_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "# 6. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - ë²¡í„°í™”\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['processed_text']) # í•™ìŠµì‹œí‚¤ê³  TF-IDF í–‰ë ¬ë¡œ ë³€í™˜\n",
    "print(X_train_tfidf[:2].toarray()) # ì²« 2ê°œ ë¬¸ì„œì˜ ë²¡í„°í™” ê²°ê³¼\n",
    "\n",
    "# 6. ë°ì´í„° ë¶„í• \n",
    "# TF-IDF í–‰ë ¬ê³¼ ëª©í‘œ ë³€ìˆ˜ 'target'ì„ í•™ìŠµìš©, ê²€ì¦ìš©ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "# test_sizeëŠ” ê²€ì¦ìš© ë°ì´í„°ì˜ ë¹„ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. (0.2ëŠ” 20%)\n",
    "# random_stateëŠ” ì¬í˜„ì„±ì„ ìœ„í•´ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_tfidf,\n",
    "    train_df['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 7. ëª¨ë¸ í•™ìŠµ\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# 7.1. ëª¨ë¸ í•™ìŠµ\n",
    "# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "model = LogisticRegression(random_state=42)\n",
    "# í•™ìŠµ ë°ì´í„°(X_train)ì™€ ì •ë‹µ(y_train)ì„ ì‚¬ìš©í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
    "model.fit(X_train, y_train)\n",
    "# 7.2. ëª¨ë¸ ì˜ˆì¸¡\n",
    "# ê²€ì¦ ë°ì´í„°(X_val)ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "y_pred = model.predict(X_val)\n",
    "# 7.3. ëª¨ë¸ í‰ê°€\n",
    "# ì •í™•ë„(Accuracy)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"ëª¨ë¸ì˜ ì •í™•ë„(Accuracy): {accuracy:.4f}\\n\")\n",
    "# ìƒì„¸í•œ í‰ê°€ ì§€í‘œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# 8. ëª¨ë¸ ê³ ë„í™” - í•˜ì´í¼íŒŒë¦¬ë¯¸í„° íŠœë‹(Fine-tuning)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ í›„ë³´êµ°ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # ì •ê·œí™” ê°•ë„ C ê°’ í›„ë³´êµ°\n",
    "    'solver': ['liblinear']   # ìµœì í™”ë¥¼ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜\n",
    "}\n",
    "# GridSearchCV ê°ì²´ ìƒì„±\n",
    "# cv=5ëŠ” 5-ê²¹ êµì°¨ ê²€ì¦ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1  # ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬\n",
    ")\n",
    "# GridSearchCVë¥¼ í•™ìŠµ ë°ì´í„°ì— ì ìš©í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "grid_search.fit(X_train_tfidf, train_df['target'])\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ê·¸ë•Œì˜ ì ìˆ˜ í™•ì¸\n",
    "print(\"\\n--- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ---\")\n",
    "print(\"ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\", grid_search.best_params_)\n",
    "print(\"ìµœê³  ì •í™•ë„ ì ìˆ˜:\", grid_search.best_score_)\n",
    "# ìµœì ì˜ ëª¨ë¸ì„ ìµœì¢… ëª¨ë¸ë¡œ ì‚¬ìš©\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model_y_pred = best_model.predict(X_val)\n",
    "# ì •í™•ë„(Accuracy)ë¥¼ ê³„ì‚°í•˜ì—¬ ê¸°ë³¸ ëª¨ë¸ê³¼ ë¹„êµ\n",
    "best_model_accuracy = accuracy_score(y_val, best_model_y_pred)\n",
    "print(f\"ìµœì í™”ëœ ëª¨ë¸ì˜ ì •í™•ë„(X_val ê¸°ì¤€): {best_model_accuracy:.4f}\\n\")\n",
    "# ìƒì„¸í•œ í‰ê°€ ì§€í‘œë„ í™•ì¸ ê°€ëŠ¥\n",
    "print(\"--- ìµœì í™”ëœ ëª¨ë¸ Classification Report ---\")\n",
    "print(classification_report(y_val, best_model_y_pred))\n",
    "# 9.1. í…ìŠ¤íŠ¸ ì •ì œ\n",
    "# 9.1.1. keyword ì»¬ëŸ¼ ì²˜ë¦¬\n",
    "test_df['keyword'] = test_df['keyword'].fillna(test_df['keyword'].mode()[0]) # fatalities\n",
    "test_df.isnull().sum()\n",
    "# 9.1.2. ëŒ€ë¬¸ì > ì†Œë¬¸ì\n",
    "test_df['text'] = test_df['text'].str.lower()\n",
    "# 9.1.3. íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì, ê¸°í˜¸ ì œê±°\n",
    "# r'[^a-z ]' íŒ¨í„´ì„ ì‚¬ìš©í•˜ê³ , ëŒ€ì²´í•  ë¬¸ìì—´ì€ '' (ë¹ˆ ë¬¸ìì—´)\n",
    "test_df['text'] = test_df['text'].str.replace(r'[^a-z ]', '', regex=True)\n",
    "# 9.2. í† í°í™”\n",
    "# 9.2.1 í† í°í™” (Tokenization)\n",
    "# text ì¹¼ëŸ¼ì˜ ê° ë¬¸ìì—´ì— word_tokenize í•¨ìˆ˜ ì ìš©\n",
    "test_df['tokenized_text'] = test_df['text'].apply(word_tokenize)\n",
    "# í† í°í™” ê²°ê³¼ í™•ì¸\n",
    "# print(\"í† í°í™” í›„ ìƒ˜í”Œ:\")\n",
    "# for i in range(5):\n",
    "#     print(f\"Original Text (Cleaned): {test_df['text'][i]}\")\n",
    "#     print(f\"Tokenized Text: {test_df['tokenized_text'][i]}\\n\")\n",
    "# 9.3. ë¶ˆìš©ì–´ ì œê±°\n",
    "stop_words = set(stopwords.words('english')) # ì˜ì–´ ì¤‘ ë¶ˆìš©ì–´\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words] # ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ë‹¨ì–´\n",
    "test_df['non_stopwords_text'] = test_df['tokenized_text'].apply(remove_stopwords)\n",
    "# ë¶ˆìš©ì–´ ì œê±° í›„ ìƒ˜í”Œ\n",
    "# for i in range(5):\n",
    "#     print(f\"Tokenized Text: {test_df['tokenized_text'][i]}\")\n",
    "#     print(f\"Non-stopwords Text: {test_df['non_stopwords_text'][i]}\\n\")\n",
    "# í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "test_df['processed_text'] = test_df['non_stopwords_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "# 9.4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - ë²¡í„°í™”\n",
    "# train ë°ì´í„°ì—ì„œ í•™ìŠµëœ ê¸°ì¡´ vectorizer ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "X_test_tfidf = vectorizer.transform(test_df['processed_text']) # í•™ìŠµì‹œí‚¤ê³  TF-IDF í–‰ë ¬ë¡œ ë³€í™˜\n",
    "print(X_test_tfidf[:2].toarray()) # ì²« 2ê°œ ë¬¸ì„œì˜ ë²¡í„°í™” ê²°ê³¼\n",
    "\n",
    "# 10. í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "test_predictions = best_model.predict(X_test_tfidf)\n",
    "print(test_predictions[:10])\n",
    "\n",
    "# 11. ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ì œì¶œ ì–‘ì‹ì— ë§ê²Œ idì™€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹´ì€ DataFrameì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_predictions\n",
    "})\n",
    "\n",
    "# submission.csv íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_study_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
