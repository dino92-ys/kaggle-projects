{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2147e8f4",
   "metadata": {},
   "source": [
    "# Disaster Tweets - LSTM with Embedding Comparison\n",
    "## Word2Vec vs GloVe vs Random Embedding ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "**ëª©í‘œ:** 80% ì´ìƒ ì •í™•ë„ ë‹¬ì„± (TF-IDFì™€ ë¹„êµ ê°€ëŠ¥í•œ ìˆ˜ì¤€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eca7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Train ë°ì´í„°: 7613ê°œ, Test ë°ì´í„°: 3263ê°œ\n",
      "ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
      "ë‹¨ì–´ ì‚¬ì „ í¬ê¸°: 6385ê°œ\n",
      "ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´: 20\n",
      "Train: 6090, Val: 1523\n",
      "\n",
      "==================================================\n",
      "Training with Word2Vec Embedding\n",
      "==================================================\n",
      "Epoch 01/25 | Train Loss: 0.5975, Acc: 66.78% | Val Loss: 0.4558, Acc: 79.84%\n",
      "Epoch 02/25 | Train Loss: 0.3836, Acc: 83.56% | Val Loss: 0.4457, Acc: 80.89%\n",
      "Epoch 03/25 | Train Loss: 0.2935, Acc: 88.31% | Val Loss: 0.4835, Acc: 77.81%\n",
      "Epoch 04/25 | Train Loss: 0.2348, Acc: 90.85% | Val Loss: 0.5612, Acc: 79.25%\n",
      "Epoch 05/25 | Train Loss: 0.1905, Acc: 92.81% | Val Loss: 0.6232, Acc: 78.59%\n",
      "Epoch 06/25 | Train Loss: 0.1517, Acc: 94.19% | Val Loss: 0.6714, Acc: 77.61%\n",
      "Epoch 07/25 | Train Loss: 0.1094, Acc: 96.01% | Val Loss: 0.8403, Acc: 76.95%\n",
      "Early stopping at epoch 7\n",
      "GloVe: 5799/6384 ë‹¨ì–´ ë§¤ì¹­ (90.8%)\n",
      "\n",
      "==================================================\n",
      "Training with GloVe Embedding\n",
      "==================================================\n",
      "Epoch 01/25 | Train Loss: 0.5102, Acc: 75.40% | Val Loss: 0.4385, Acc: 80.17%\n",
      "Epoch 02/25 | Train Loss: 0.4452, Acc: 80.07% | Val Loss: 0.4703, Acc: 80.96%\n",
      "Epoch 03/25 | Train Loss: 0.4108, Acc: 82.32% | Val Loss: 0.4068, Acc: 82.14%\n",
      "Epoch 04/25 | Train Loss: 0.3736, Acc: 84.01% | Val Loss: 0.4126, Acc: 82.27%\n",
      "Epoch 05/25 | Train Loss: 0.3366, Acc: 85.96% | Val Loss: 0.4266, Acc: 80.76%\n",
      "Epoch 06/25 | Train Loss: 0.3187, Acc: 86.88% | Val Loss: 0.4686, Acc: 80.76%\n",
      "Epoch 07/25 | Train Loss: 0.2863, Acc: 87.59% | Val Loss: 0.4710, Acc: 80.76%\n",
      "Epoch 08/25 | Train Loss: 0.2516, Acc: 89.47% | Val Loss: 0.5109, Acc: 80.96%\n",
      "Early stopping at epoch 8\n",
      "\n",
      "==================================================\n",
      "Training with Random (Baseline) Embedding\n",
      "==================================================\n",
      "Epoch 01/25 | Train Loss: 0.6062, Acc: 66.08% | Val Loss: 0.4517, Acc: 79.58%\n",
      "Epoch 02/25 | Train Loss: 0.4215, Acc: 81.97% | Val Loss: 0.4399, Acc: 79.91%\n",
      "Epoch 03/25 | Train Loss: 0.3428, Acc: 85.93% | Val Loss: 0.4630, Acc: 79.65%\n",
      "Epoch 04/25 | Train Loss: 0.2868, Acc: 88.60% | Val Loss: 0.5120, Acc: 79.91%\n",
      "Epoch 05/25 | Train Loss: 0.2408, Acc: 90.56% | Val Loss: 0.5124, Acc: 78.99%\n",
      "Epoch 06/25 | Train Loss: 0.2143, Acc: 91.94% | Val Loss: 0.5667, Acc: 77.81%\n",
      "Epoch 07/25 | Train Loss: 0.1719, Acc: 93.50% | Val Loss: 0.6350, Acc: 77.94%\n",
      "Early stopping at epoch 7\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ì„ë² ë”© ë¹„êµ ì‹¤í—˜ ê²°ê³¼\n",
      "============================================================\n",
      "Embedding       | Val Accuracy    | ë¹„ê³ \n",
      "------------------------------------------------------------\n",
      "Word2Vec        | 80.89%          | \n",
      "GloVe           | 82.27%          | âœ… Best\n",
      "Random          | 79.91%          | \n",
      "============================================================\n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥: GloVe (82.27%)\n",
      "\n",
      "âœ… 'submission_lstm_enh.csv' íŒŒì¼ ìƒì„± ì™„ë£Œ!\n",
      "   id  target\n",
      "0   0       1\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       1\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ í™•ì¸\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================================\n",
    "# 1. ë°ì´í„° ë¡œë”©\n",
    "# ========================================\n",
    "train_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/kaggle-projects/disaster-tweets/data/train.csv'\n",
    "test_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/kaggle-projects/disaster-tweets/data/test.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "print(f\"Train ë°ì´í„°: {len(train_df)}ê°œ, Test ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "\n",
    "# ========================================\n",
    "# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ========================================\n",
    "def preprocess_text(df, keyword_mode=None):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°, í† í°í™”, ë¶ˆìš©ì–´ ì œê±°\"\"\"\n",
    "    df = df.copy()\n",
    "    if keyword_mode is None:\n",
    "        keyword_mode = df['keyword'].mode()[0]\n",
    "    df['keyword'] = df['keyword'].fillna(keyword_mode)\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    df['text'] = df['text'].str.replace(r'[^a-z ]', '', regex=True)\n",
    "    df['tokenized_text'] = df['text'].apply(word_tokenize)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['tokens'] = df['tokenized_text'].apply(\n",
    "        lambda tokens: [word for word in tokens if word not in stop_words]\n",
    "    )\n",
    "    return df, keyword_mode\n",
    "\n",
    "train_df, keyword_mode = preprocess_text(train_df)\n",
    "test_df, _ = preprocess_text(test_df, keyword_mode=keyword_mode)\n",
    "print(\"ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "\n",
    "# ========================================\n",
    "# 3. Word2Vec í•™ìŠµ ë° ë‹¨ì–´ ì‚¬ì „ ìƒì„±\n",
    "# ========================================\n",
    "sentences = train_df[\"tokens\"].tolist()\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "word_to_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}  # 0ì€ íŒ¨ë”©ìš©\n",
    "vocab_size = len(word_to_index) + 1\n",
    "print(f\"ë‹¨ì–´ ì‚¬ì „ í¬ê¸°: {vocab_size}ê°œ\")\n",
    "\n",
    "# ========================================\n",
    "# 4. ì •ìˆ˜ ì¸ì½”ë”© ë° íŒ¨ë”©\n",
    "# ========================================\n",
    "def encode_sequences(df, word_to_index, max_len=None):\n",
    "    \"\"\"í† í°ì„ ì •ìˆ˜ë¡œ ì¸ì½”ë”©í•˜ê³  íŒ¨ë”©\"\"\"\n",
    "    encoded = df[\"tokens\"].apply(\n",
    "        lambda tokens: [word_to_index[t] for t in tokens if t in word_to_index]\n",
    "    )\n",
    "    if max_len is None:\n",
    "        max_len = max(len(s) for s in encoded)\n",
    "    padded = [seq[:max_len] + [0] * max(0, max_len - len(seq)) for seq in encoded]\n",
    "    return torch.LongTensor(padded), max_len\n",
    "\n",
    "X, max_len = encode_sequences(train_df, word_to_index)\n",
    "y = torch.LongTensor(train_df[\"target\"].values)\n",
    "X_test, _ = encode_sequences(test_df, word_to_index, max_len)\n",
    "print(f\"ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´: {max_len}\")\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "\n",
    "# ========================================\n",
    "# 5. ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± í•¨ìˆ˜ë“¤\n",
    "# ========================================\n",
    "def create_word2vec_embedding(w2v_model, word_to_index, embedding_dim=100):\n",
    "    \"\"\"Word2Vec ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±\"\"\"\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "def load_glove_embeddings(glove_path, word_to_index, embedding_dim=100):\n",
    "    \"\"\"GloVe ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ë¡œë“œ\"\"\"\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    found = 0\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[idx] = embeddings_index[word]\n",
    "            found += 1\n",
    "    \n",
    "    print(f\"GloVe: {found}/{len(word_to_index)} ë‹¨ì–´ ë§¤ì¹­ ({found/len(word_to_index)*100:.1f}%)\")\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "def create_random_embedding(vocab_size, embedding_dim=100):\n",
    "    \"\"\"ëœë¤ ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±\"\"\"\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))\n",
    "    embedding_matrix[0] = 0  # íŒ¨ë”©ì€ 0\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# ========================================\n",
    "# 6. ê°œì„ ëœ LSTM ëª¨ë¸ (Dropout ì¶”ê°€)\n",
    "# ========================================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 num_layers=2, dropout=0.5, pretrained_weights=None, freeze_embedding=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        if pretrained_weights is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_weights)\n",
    "            self.embedding.weight.requires_grad = not freeze_embedding\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # Bidirectional: ë§ˆì§€ë§‰ forward + backward hidden state ê²°í•©\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# ========================================\n",
    "# 7. Dataset ë° DataLoader\n",
    "# ========================================\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ========================================\n",
    "# 8. Early Stopping í´ë˜ìŠ¤\n",
    "# ========================================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_model = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "# ========================================\n",
    "# 9. í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜\n",
    "# ========================================\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(1), labels.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        total_correct += (preds.squeeze(1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(1), labels.float())\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            total_correct += (preds.squeeze(1) == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "# ========================================\n",
    "# 10. ì „ì²´ í•™ìŠµ í•¨ìˆ˜\n",
    "# ========================================\n",
    "def train_model(embedding_matrix, embedding_name, epochs=25, batch_size=32, lr=0.001):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with {embedding_name} Embedding\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # DataLoader ìƒì„±\n",
    "    train_dataset = TextDataset(X_train, y_train)\n",
    "    val_dataset = TextDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=100,\n",
    "        hidden_dim=128,\n",
    "        output_dim=1,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "        pretrained_weights=embedding_matrix\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        print(f'Epoch {epoch+1:02}/{epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f}, Acc: {train_acc*100:.2f}% | '\n",
    "              f'Val Loss: {val_loss:.4f}, Acc: {val_acc*100:.2f}%')\n",
    "        \n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # ìµœì  ëª¨ë¸ ë³µì›\n",
    "    model.load_state_dict(early_stopping.best_model)\n",
    "    \n",
    "    return model, best_val_acc, history\n",
    "\n",
    "# ========================================\n",
    "# 11. ì„ë² ë”© ë¹„êµ ì‹¤í—˜\n",
    "# ========================================\n",
    "results = {}\n",
    "\n",
    "# 1) Word2Vec ì„ë² ë”©\n",
    "w2v_embedding = create_word2vec_embedding(w2v_model, word_to_index)\n",
    "w2v_model_trained, w2v_acc, w2v_history = train_model(w2v_embedding, \"Word2Vec\")\n",
    "results['Word2Vec'] = w2v_acc\n",
    "\n",
    "# 2) GloVe ì„ë² ë”© (íŒŒì¼ ê²½ë¡œ ì„¤ì • í•„ìš”)\n",
    "glove_path = '/Users/ys/Downloads/study/models/glove/glove.6B.100d.txt'\n",
    "if os.path.exists(glove_path):\n",
    "    glove_embedding = load_glove_embeddings(glove_path, word_to_index)\n",
    "    glove_model_trained, glove_acc, glove_history = train_model(glove_embedding, \"GloVe\")\n",
    "    results['GloVe'] = glove_acc\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ GloVe íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {glove_path}\")\n",
    "    print(\"GloVe ë‹¤ìš´ë¡œë“œ: https://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "    results['GloVe'] = None\n",
    "\n",
    "# 3) Random ì„ë² ë”© (Baseline)\n",
    "random_embedding = create_random_embedding(vocab_size)\n",
    "random_model_trained, random_acc, random_history = train_model(random_embedding, \"Random (Baseline)\")\n",
    "results['Random'] = random_acc\n",
    "\n",
    "# ========================================\n",
    "# 12. ê²°ê³¼ ë¹„êµ í‘œ\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ì„ë² ë”© ë¹„êµ ì‹¤í—˜ ê²°ê³¼\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Embedding':<15} | {'Val Accuracy':<15} | {'ë¹„ê³ '}\")\n",
    "print(\"-\"*60)\n",
    "for name, acc in results.items():\n",
    "    if acc is not None:\n",
    "        note = \"âœ… Best\" if acc == max(v for v in results.values() if v) else \"\"\n",
    "        print(f\"{name:<15} | {acc*100:.2f}%{'':<9} | {note}\")\n",
    "    else:\n",
    "        print(f\"{name:<15} | {'N/A':<15} | íŒŒì¼ ì—†ìŒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "best_embedding = max((k for k, v in results.items() if v), key=lambda k: results[k])\n",
    "print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥: {best_embedding} ({results[best_embedding]*100:.2f}%)\")\n",
    "\n",
    "# ========================================\n",
    "# 13. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©)\n",
    "# ========================================\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "if best_embedding == 'Word2Vec':\n",
    "    best_model = w2v_model_trained\n",
    "elif best_embedding == 'GloVe' and results['GloVe'] is not None:\n",
    "    best_model = glove_model_trained\n",
    "else:\n",
    "    best_model = random_model_trained\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "test_dataset = TestDataset(X_test.to(device))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "best_model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = best_model(inputs)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        test_preds.extend(preds.cpu().numpy().flatten().astype(int))\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "submission_path = 'submission_lstm_enh.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\nâœ… '{submission_path}' íŒŒì¼ ìƒì„± ì™„ë£Œ!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_study_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
