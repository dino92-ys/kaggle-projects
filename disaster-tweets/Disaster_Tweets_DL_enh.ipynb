{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2147e8f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 완료!\n",
      "컬럼 확인: True\n",
      "전처리 후 결측치 확인:\n",
      "id                       0\n",
      "keyword                  0\n",
      "location              2533\n",
      "text                     0\n",
      "target                   0\n",
      "tokenized_text           0\n",
      "non_stopwords_text       0\n",
      "dtype: int64\n",
      "유사 단어: [('im', 0.9991193413734436), ('still', 0.9990773797035217), ('fire', 0.9990521669387817), ('us', 0.9990509152412415), ('nuclear', 0.9990345239639282)]\n",
      "가장 긴 트윗의 길이: 20\n",
      "Using device: mps\n",
      "학습 데이터로더 배치 수: 96\n",
      "검증 데이터로더 배치 수: 24\n",
      "첫 번째 배치 데이터 형태: torch.Size([64, 20])\n",
      "첫 번째 배치 레이블 형태: torch.Size([64])\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.686 | Train Acc: 56.85%\n",
      "\tVal. Loss: 0.683 | Val. Acc: 57.39%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.684 | Train Acc: 56.95%\n",
      "\tVal. Loss: 0.682 | Val. Acc: 57.39%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.685 | Train Acc: 57.08%\n",
      "\tVal. Loss: 0.683 | Val. Acc: 57.39%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.684 | Train Acc: 56.95%\n",
      "\tVal. Loss: 0.682 | Val. Acc: 57.39%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.684 | Train Acc: 56.95%\n",
      "\tVal. Loss: 0.682 | Val. Acc: 57.39%\n",
      "테스트 데이터 로드 완료.\n",
      "테스트 데이터 전처리 완료 (train과 동일한 방식 적용)\n",
      "정수 인코딩, 패딩, 텐서 변환 완료.\n",
      "최종 테스트 데이터 형태: torch.Size([3263, 20])\n",
      "테스트 데이터 로더 생성 완료.\n",
      "모델 예측 완료.\n",
      "총 예측 개수: 3263\n",
      "✅ 'submission.csv' 파일이 성공적으로 생성되었습니다.\n",
      "제출 파일 샘플:\n",
      "   id  target\n",
      "0   0       0\n",
      "1   2       0\n",
      "2   3       0\n",
      "3   9       0\n",
      "4  11       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "# 데이터 불러오기 (DataFrame 객체로 로드)\n",
    "train_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/train.csv'\n",
    "train_df = pd.read_csv(train_path) # train 데이터를 DataFrame으로 로드\n",
    "test_path = '/Users/ys/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# 2. EDA\n",
    "# 보고서 생성 (target 인자 없이)\n",
    "# from dataprep.eda import create_report # dataprep 데이터 분석\n",
    "# report = create_report(train_df) # 여기서 target 인자를 삭제했습니다.\n",
    "# 보고서를 HTML 파일로 저장\n",
    "# 파일명은 train_dataprep_report.html로 지정합니다.\n",
    "# report.save('train_dataprep_report.html')\n",
    "\n",
    "# 데이터 분석\n",
    "# 1.keyword 칼럼:\n",
    "# Approximate Distinct Count: 221개\n",
    "# Approximate Unique (%): 2.9%\n",
    "# Missing: 61개\n",
    "# Missing (%): 0.8%\n",
    "# Memory Size: 556863\n",
    "# 인사이트: sweetviz와 유사하게 221개의 고유 키워드가 있고, 약 0.8%의 적은 결측치가 존재합니다. 이 칼럼은 충분히 활용 가치가 있어 보입니다. 결측치는 처리해주는 것이 좋습니다.\n",
    "# 2. location 칼럼:\n",
    "# Approximate Distinct Count: 3341개\n",
    "# Approximate Unique (%): 65.8%\n",
    "# Missing: 2533개\n",
    "# Missing (%): 33.3%\n",
    "# Memory Size: 404598\n",
    "# 인사이트: sweetviz에서 확인했던 것과 동일하게, 3341개의 매우 많은 고유 지역이 있으며, 무려 33.3%라는 높은 비율의 결측치가 존재합니다.\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 텍스트 전처리(정제) 함수 정의 섹션\n",
    "# ========================================\n",
    "# 토큰화 필요 함수\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# NLTK 데이터 다운로드 확인\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download(\"punkt\")\n",
    "    # nltk.download('punkt_tab')\n",
    "    nltk.download(\"stopwords\")\n",
    "def preprocess_text(df, keyword_mode=None):\n",
    "    \"\"\"\n",
    "    텍스트 전처리를 수행하는 함수\n",
    "    - keyword 결측치 처리\n",
    "    - 소문자 변환\n",
    "    - 특수문자/숫자 제거\n",
    "    - 토큰화\n",
    "    - 불용어 제거\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        전처리할 데이터프레임\n",
    "    keyword_mode : str, optional\n",
    "        keyword 컬럼의 최빈값. None이면 df에서 자동 계산\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "        전처리가 완료된 데이터프레임\n",
    "    keyword_mode : str\n",
    "        사용된 keyword 최빈값 (test 데이터에 재사용하기 위해 반환)\n",
    "    \"\"\"\n",
    "    df = df.copy()  # 원본 데이터 보존\n",
    "    # 3. 텍스트 정제\n",
    "    # 3.1. keyword 결측치를 최빈값으로 채우기\n",
    "    if keyword_mode is None:\n",
    "        keyword_mode = df['keyword'].mode()[0]\n",
    "    df['keyword'] = df['keyword'].fillna(keyword_mode)\n",
    "    # 3.2. 대문자를 소문자로 변환\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # 3.3. 특수문자, 숫자, 기호 제거 (a-z와 공백만 남김)\n",
    "    df['text'] = df['text'].str.replace(r'[^a-z ]', '', regex=True)\n",
    "    # 4. 토큰화: 문장을 단어 단위로 분리\n",
    "    df['tokenized_text'] = df['text'].apply(word_tokenize)\n",
    "    # 5. 불용어 제거: 의미 없는 단어(is, a, the 등) 제거\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['non_stopwords_text'] = df['tokenized_text'].apply(\n",
    "        lambda tokens: [word for word in tokens if word not in stop_words]\n",
    "    )\n",
    "    return df, keyword_mode\n",
    "\n",
    "# 3. 텍스트 정제 (전처리 함수 사용)\n",
    "train_df, keyword_mode = preprocess_text(train_df)\n",
    "print(\"전처리 완료!\")\n",
    "print(\"컬럼 확인:\", \"non_stopwords_text\" in train_df.columns)\n",
    "print(\"전처리 후 결측치 확인:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# 6. 벡터화 (Word2Vec 모델 학습)\n",
    "# Word2Vec은 단어를 벡터(숫자 배열)로 변환하여, 비슷한 의미를 가진 단어들이\n",
    "# 벡터 공간에서 가까이 위치하도록 학습하는 알고리즘입니다.\n",
    "from gensim.models import Word2Vec\n",
    "# 6.1. 학습 데이터 준비\n",
    "# non_stopwords_text 컬럼에는 불용어가 제거된 토큰 리스트가 들어있습니다.\n",
    "# 예: [['fire', 'building'], ['disaster', 'earthquake'], ...]\n",
    "sentences = train_df[\"non_stopwords_text\"].tolist()\n",
    "print(f\"Word2Vec 학습 문장 개수: {len(sentences)}개\")\n",
    "# 6.2. Word2Vec 모델 학습\n",
    "# 하이퍼파라미터 설정:\n",
    "# - vector_size=100: 각 단어를 100차원의 벡터로 표현\n",
    "# - window=5: 현재 단어를 기준으로 앞뒤 5개 단어까지 문맥으로 고려\n",
    "# - min_count=3: 3번 미만 등장한 단어는 학습에서 제외 (노이즈 제거)\n",
    "# - workers=4: 병렬 처리를 위한 CPU 코어 수\n",
    "embedding_model = Word2Vec(sentences, vector_size=100, window=5, min_count=3, workers=4)\n",
    "print(f\"학습된 단어 개수: {len(embedding_model.wv.index_to_key)}개\")\n",
    "# 6.3. 단어 사전 생성\n",
    "# 단어를 고유한 정수 인덱스로 매핑하는 사전을 만듭니다.\n",
    "# 예: {'fire': 0, 'disaster': 1, 'building': 2, ...}\n",
    "# 이 사전은 이후 정수 인코딩 단계에서 사용됩니다.\n",
    "word_to_index = {word: idx for idx, word in enumerate(embedding_model.wv.index_to_key)}\n",
    "print(f\"단어 사전 크기: {len(word_to_index)}개\")\n",
    "# 6.4. Word2Vec 가중치 추출\n",
    "# Word2Vec이 학습한 단어 벡터들을 PyTorch 텐서로 변환합니다.\n",
    "# 이 가중치는 나중에 LSTM 모델의 Embedding 층 초기값으로 사용됩니다.\n",
    "# pretrained_weights 형태: [vocab_size, vector_size] = [단어개수, 100]\n",
    "import torch\n",
    "pretrained_weights = torch.FloatTensor(embedding_model.wv.vectors)\n",
    "print(f\"임베딩 가중치 형태: {pretrained_weights.shape}\")\n",
    "# 6.5. 학습 결과 확인 (선택사항)\n",
    "# 'disaster'와 의미가 유사한 단어 5개를 찾아봅니다.\n",
    "# 벡터 공간에서 가까운 단어들을 찾아 Word2Vec이 잘 학습되었는지 확인합니다.\n",
    "try:\n",
    "    similar_words = embedding_model.wv.most_similar(\"disaster\", topn=5)\n",
    "    print(\"\\n'disaster'와 유사한 단어:\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"  - {word}: {similarity:.4f}\")\n",
    "except KeyError:\n",
    "    print(\"'disaster' 단어가 단어 사전에 없습니다.\")\n",
    "\n",
    "\n",
    "# 7. 정수 인코딩 & 패딩\n",
    "def encode_and_pad(df, word_to_index, max_len=None, has_target=True):\n",
    "    \"\"\"\n",
    "    정수 인코딩 및 패딩 수행\n",
    "\n",
    "    Why?\n",
    "    - 정수 인코딩과 패딩은 항상 함께 수행되므로 하나의 함수로 통합\n",
    "    - train/test 데이터에 동일한 전처리를 보장\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        처리할 데이터프레임 (non_stopwords_text 컬럼 필요)\n",
    "    word_to_index : dict\n",
    "        단어 사전 (Word2Vec 학습 시 생성됨)\n",
    "    max_len : int, optional\n",
    "        패딩 길이. None이면 df에서 최대 길이 계산\n",
    "    has_target : bool\n",
    "        target 컬럼 포함 여부 (train: True, test: False)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : torch.Tensor\n",
    "        패딩된 입력 텐서 [샘플 수, max_len]\n",
    "    y : torch.Tensor or None\n",
    "        타겟 텐서 (has_target=False면 None)\n",
    "    max_len : int\n",
    "        사용된 최대 길이 (test 데이터 처리 시 재사용)\n",
    "    \"\"\"\n",
    "    # 7.1. 정수 인코딩: 단어를 숫자로 변환\n",
    "    df[\"encoded_text\"] = df[\"non_stopwords_text\"].apply(\n",
    "        lambda tokens: [\n",
    "            word_to_index[token] for token in tokens if token in word_to_index\n",
    "        ]\n",
    "    )\n",
    "    # 7.2. 최대 길이 계산 (train에서만)\n",
    "    if max_len is None:\n",
    "        max_len = max(len(s) for s in df[\"encoded_text\"])\n",
    "\n",
    "    # 7.3. 패딩: 모든 시퀀스를 max_len 길이로 맞춤\n",
    "    padded_sequences = [seq + [0] * (max_len - len(seq)) for seq in df[\"encoded_text\"]]\n",
    "\n",
    "    # 텐서 변환\n",
    "    X = torch.LongTensor(np.array(padded_sequences))\n",
    "\n",
    "    # target이 있으면 y도 반환\n",
    "    y = None\n",
    "    if has_target:\n",
    "        y = torch.LongTensor(df[\"target\"].values)\n",
    "\n",
    "    return X, y, max_len\n",
    "\n",
    "\n",
    "# 8. 데이터 분할\n",
    "# Word2Vec 기반 정수 인코딩 및 패딩된 시퀀스와 목표 변수 'target'을 학습용, 검증용으로 나눕니다.\n",
    "# test_size는 검증용 데이터의 비율을 의미합니다. (0.2는 20%)\n",
    "# random_state는 재현성을 위해 고정합니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# 데이터 로더 구축 전에 텐서를 디바이스로 이동\n",
    "# GPU 사용 설정 (맥북은 'mps')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# 텐서를 디바이스로 이동시키는 코드 추가\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_val = y_val.to(device)\n",
    "\n",
    "# 9.데이터 로더 구축\n",
    "# Dataset: 데이터셋의 **데이터(X)**와 **정답 레이블(Y)**을 묶어주는 역할을 합니다. 각 데이터 샘플에 쉽게 접근할 수 있도록 만들어 주죠.\n",
    "# DataLoader: Dataset을 감싸서 미니 배치(mini-batch) 단위로 데이터를 자동으로 묶어주고, 학습 과정에서 데이터를 모델에 전달하는 역할을 합니다. 이렇게 하면 전체 데이터를 한꺼번에 메모리에 올리지 않고도 효율적으로 학습할 수 있어요.\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 전체 길이를 반환\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터와 레이블을 반환\n",
    "        return self.X[idx], self.y[idx]\n",
    "# 데이터셋 객체 생성\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "# DataLoader 객체 생성\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"학습 데이터로더 배치 수: {len(train_loader)}\")\n",
    "print(f\"검증 데이터로더 배치 수: {len(val_loader)}\")\n",
    "# 첫 번째 배치 확인\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(f\"첫 번째 배치 데이터 형태: {X_batch.shape}\")\n",
    "    print(f\"첫 번째 배치 레이블 형태: {y_batch.shape}\")\n",
    "    break\n",
    "\n",
    "# 10. 딥러닝 모델 구축\n",
    "import torch.nn as nn # PyTorch의 신경망 모듈(패키지)\n",
    "class LSTMClassifier(nn.Module): # nn.Module을 상속받아 LSTMClassifier 클래스 정의\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, pretrained_weights=None):\n",
    "        # vocab_size: 단어 사전의 크기. 모델이 알아야 할 단어의 총 개수\n",
    "        # embedding_dim: 각 단어를 표현하는 임베딩 벡터의 차원, 각 단어를 몇 차원의 벡터로 표현할지를 결정.\n",
    "        # hidden_dim: LSTM의 은닉 상태(hidden state) 차원. LSTM 층이 학습하는 정보의 '깊이'를 결정.\n",
    "        # output_dim: 모델의 출력 차원. 분류 문제에서는 클래스의 수와 동일. 지금은 이진 분류이므로 0(재난 트윗 아님) 또는 1(재난 트윗)을 나타낼 수 있도록 보통 1로 설정.\n",
    "        # num_layers: LSTM 층을 설정. 기본 1개.\n",
    "        super().__init__() # 부모 클래스 초기화\n",
    "        # 레이어(층) 정의\n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) # 패딩 인덱스 0으로 설정. 패딩으로 설정된 0을 학습에서 제외시키기 위해서 설정.\n",
    "        # Word2Vec 가중치가 있으면 로드\n",
    "        if pretrained_weights is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_weights)\n",
    "            # Fine-tuning 허용 (학습 중 임베딩도 업데이트 됨)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        # LSTM 레이어\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True) # num_layers 추가: 층수만큼 스택\n",
    "        # 완전연결 레이어\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        # 입력 데이터 text를 임베딩 층에 통과시켜 단어 벡터로 변환합니다.  이 결과로 [batch size, seq len, embedding dim] 형태의 3차원 텐서가 생성되는데, 이것은 \"여러 문장(batch size)에 있는 각 단어(seq len)를 벡터(embedding dim)로 표현했다\"는 뜻입니다.\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # 임베딩된 텐서가 LSTM 층을 통과합니다. 이때 LSTM은 두 가지를 출력합니다.\n",
    "        # output: 각 시점(time step)의 출력.\n",
    "        #hidden: 마지막 시점의 은닉 상태(Hidden State).\n",
    "        # cell: 마지막 시점의 셀 상태(Cell State).\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        # hidden = [1, batch size, hidden dim]\n",
    "        # hidden = hidden.squeeze(0)\n",
    "        # hidden 텐서의 형태가 [1, batch size, hidden dim]인데, LSTM은 기본적으로 하나의 층을 사용하므로 맨 앞의 차원(1)은 필요가 없습니다. squeeze(0)를 사용해 불필요한 차원을 제거하여 [batch size, hidden dim] 형태로 만들어줍니다.\n",
    "        # hidden = [batch size, hidden dim]\n",
    "        hidden = hidden[-1]  # [num_layers, batch_size, hidden_dim]의 마지막 층 추출: [batch_size, hidden_dim]\n",
    "        return self.fc(hidden)\n",
    "        # 마지막으로, 이렇게 정리된 hidden 텐서를 완전 연결 층(fc)에 넣어 최종 분류 결과를 얻습니다. 이때 hidden은 \"문장 전체의 의미를 압축한 벡터\"라고 생각할 수 있습니다.\n",
    "import torch.optim as optim\n",
    "# 하이퍼파라미터 정의\n",
    "vocab_size = len(word_to_index) # 단어 사전 크기\n",
    "embedding_dim = 100 # vector_size\n",
    "hidden_dim = 512 # LSTM의 은닉 상태 크기(모델이 정보를 기억하여 복잡한 패턴을 학습)\n",
    "num_layers = 2 # LSTM 층 개수를 설정. 기본 1, 증가 시 모델 깊이 증가로 복잡한 시퀀스 패턴 학습 강화)\n",
    "output_dim = 1 # 최종 출력 차원, 이진 분류이므로 1로 설정\n",
    "# Word2Vec 가중치를 PyTorch 텐서로 변환\n",
    "pretrained_weights = torch.FloatTensor(embedding_model.wv.vectors)\n",
    "# 모델 객체 생성\n",
    "model = LSTMClassifier(vocab_size\n",
    "                     , embedding_dim\n",
    "                     , hidden_dim\n",
    "                     , output_dim\n",
    "                     , num_layers\n",
    "                     , pretrained_weights=pretrained_weights # 가중치 적용\n",
    "                       )\n",
    "# 손실 함수 정의: BCEWithLogitsLoss(이진 교차 엔트로피 손실 함수)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# 최적화 정의: Adam(최적화 알고리즘)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 11. 모델 학습 및 평가 (훈련 루프)\n",
    "# 학습 루프(Training Loop)\n",
    "# 모델이 데이터로부터 지식을 습득하는 과정입니다. 데이터를 미니 배치 단위로 모델에 주입하고, 모델의 예측과 실제 정답 사이의 차이(손실)를 계산합니다. 이 손실을 줄이기 위해 역전파(Backpropagation)를 통해 모델의 가중치들을 조금씩 조정합니다.\n",
    "# 평가 루프(Validation Loop)\n",
    "# 모델이 학습 데이터에만 과적합되지 않았는지 확인하는 과정입니다. 학습이 끝날 때마다 별도로 분리해 둔 검증 데이터셋으로 모델의 성능을 평가합니다. 이때는 가중치를 업데이트하지 않고 오직 예측 정확도만 계산합니다.\n",
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    # 모델 훈련의 핵심 부분으로, 미니 배치 단위로 학습.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # 1. 그래디언트 초기화: 이전 배치의 그래디언트를 초기화\n",
    "        # 그래디언트는 기울기또는 변화율을 말한다.\n",
    "        optimizer.zero_grad()\n",
    "        # 2. 예측값 계산: 입력 데이터를 모델에 넣어 계산\n",
    "        outputs = model(inputs)\n",
    "        # 3. 손실 계산: 예측값과 정답을 비교하여 손실을 계산\n",
    "        loss = criterion(outputs.squeeze(1), labels.float()) # 텐서의 형태를 위해 형태 변환\n",
    "        # 4. 역전파: 계산된 손실을 사용하여 모델의 모든 학습 가능한 매개변수에 대한 그래디언트를 계산. 손실을 최소화 하는 방향 제시. 역전파가 그래디언트를 계산하는 알고리즘이고 계산된 그래디언트를 사용하여 가중치를 업데이트하는 최적화 알고리즘을 경사하강법이라고 한다.\n",
    "        loss.backward()\n",
    "        # 5. 가중치 업데이트: 옵티마이저는 역전파를 통해 계산된 그래디언트 값을 사용하여 모델의 모든 가중치를 업데이트.\n",
    "        optimizer.step()\n",
    "        # 예를 들어, 3번 채점하여 4번에서 어디를 어떻게 공부할지 분석 5번 실제로 공부하여 실력 향상\n",
    "        # 훈련 진행 상황 추적(1~5까지 한 에포크)\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.round(torch.sigmoid(outputs)) # FC 층의 출력은 제한이 없다. 음수, 양수 등 아무거나 나올 수 있는데 우리는 0 or 1을 원하기 때문에 sigmoid 함수를 적용(sigmoid는 확률로 해석이 가능)시켜 0~1 사이 값으로 표현되게 변경하여 반올림해서 0 또는 1로 만들어 준다.\n",
    "        total_correct += (preds.squeeze(1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    # 평가 함수\n",
    "    model.eval() # 모델을 평가모드로 전환. Dropout이나 BatchNorm 같은 일부 층들이 비활성화\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 학습이 아닌 결과 값을 확인해야하니 그래디언트 계산을 비활성화하여 가중치가 변경되지 않도록 함.\n",
    "        for inputs, labels in val_loader:\n",
    "            # 1. 예측값 계산\n",
    "            outputs = model(inputs)\n",
    "            # 2. 손실 계산\n",
    "            loss = criterion(outputs.squeeze(1), labels.float())\n",
    "            # 훈련 진행 상황 추적\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            total_correct += (preds.squeeze(1) == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 에포크 수 설정: 모델을 5회 반복(10회 반복 시, 과적합 진행)\n",
    "N_EPOCHS = 5\n",
    "# 학습 및 평가 결과 저장을 위한 리스트\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "# GPU 사용 설정 (맥이기에 'mps')\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# 전체 학습 루프: 핵심 반복문\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # 훈련\n",
    "    train_loss, train_acc = train_model(model, train_loader, optimizer, criterion)\n",
    "    # 검증\n",
    "    val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "    # 결과 저장\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    # 결과 출력\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "# ------------------------\n",
    "# 테스트 데이터\n",
    "# ------------------------\n",
    "\n",
    "# 1. 테스트 데이터 불러오기\n",
    "test_path = '/Users/rick/Library/Mobile Documents/com~apple~CloudDocs/Study/AI/Kaggle/NLP with Disaster Tweets/data/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "print(\"테스트 데이터 로드 완료.\")\n",
    "\n",
    "# 2. 테스트 데이터 전처리 (훈련 데이터와 동일한 방식 적용)\n",
    "# train 데이터와 동일한 전처리를 수행하되, keyword_mode는 train에서 사용한 값을 그대로 사용\n",
    "# 이렇게 하면 train과 test의 전처리 방식이 완전히 동일해집니다.\n",
    "test_df, _ = preprocess_text(test_df, keyword_mode=keyword_mode)\n",
    "print(\"테스트 데이터 전처리 완료 (train과 동일한 방식 적용)\")\n",
    "\n",
    "# 3. 정수 인코딩 및 패딩 (고도화 함수 사용)\n",
    "# encode_and_pad 함수를 사용하여 Train과 동일한 방식으로 처리\n",
    "X_test, _, _ = encode_and_pad(\n",
    "    test_df, \n",
    "    word_to_index, \n",
    "    max_len=max_len,      # Train에서 계산한 최대 길이 재사용\n",
    "    has_target=False       # Test 데이터에는 target 없음\n",
    ")\n",
    "X_test = X_test.to(device)\n",
    "print(\"정수 인코딩, 패딩, 텐서 변환 완료.\")\n",
    "print(f\"최종 테스트 데이터 형태: {X_test.shape}\")\n",
    "\n",
    "# 4. 테스트 데이터 로더 생성\n",
    "# 테스트용 Dataset은 레이블(y)이 필요 없습니다.\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "test_dataset = TestDataset(X_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"테스트 데이터 로더 생성 완료.\")\n",
    "\n",
    "# 5. 모델 예측 수행\n",
    "# 5.1. 모델을 평가 모드로 전환 (매우 중요!)\n",
    "model.eval()\n",
    "\n",
    "# 5.2. 예측 결과를 담을 리스트 생성\n",
    "test_preds = []\n",
    "\n",
    "# 5.3. 그래디언트 계산 비활성화\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        # 모델 예측\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 예측값을 0 또는 1로 변환 (sigmoid -> 0.5 기준으로 반올림)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        # 결과를 리스트에 추가 (CPU로 이동 후 NumPy 배열로 변환)\n",
    "        test_preds.extend(preds.cpu().numpy().flatten().astype(int))\n",
    "\n",
    "print(\"모델 예측 완료.\")\n",
    "print(f\"총 예측 개수: {len(test_preds)}\")\n",
    "\n",
    "\n",
    "# 6. 제출 파일(submission.csv) 생성\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission_path = 'submission_enh.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ '{submission_path}' 파일이 성공적으로 생성되었습니다.\")\n",
    "print(\"제출 파일 샘플:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_study_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
